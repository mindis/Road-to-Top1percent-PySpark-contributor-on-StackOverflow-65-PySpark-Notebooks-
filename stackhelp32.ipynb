{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list = [['A','B','C','col2,col3','X,Y'],\n       ['P','Q','R','col1','Z'],\n       ['I','J','K','col1,col3','S,T']]\ndf=spark.createDataFrame(list,['col1','col2','col3','col4','col5'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+---------+----+\ncol1|col2|col3|     col4|col5|\n+----+----+----+---------+----+\n   A|   B|   C|col2,col3| X,Y|\n   P|   Q|   R|     col1|   Z|\n   I|   J|   K|col1,col3| S,T|\n+----+----+----+---------+----+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df=df.withColumn(\"col6\", F.array(F.split((\"col4\"),','),F.split((\"col5\"),','))).drop(\"col4\",\"col5\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n\n\n@udf(\"map<string,string>\")\ndef as_dict(x):\n    return dict(zip(*x)) if x else None\n\ndf.withColumn(\"col6\", F.array(F.split((\"col4\"),','),F.split((\"col5\"),','))).drop(\"col4\",\"col5\")\\\n.withColumn(\"col6\", as_dict(\"col6\")).select(\"col1\",\"col2\",\"col3\",F.col(\"col6.col1\").alias(\"col1_\"),F.col(\"col6.col2\").alias(\"col2_\"),F.col(\"col6.col3\").alias(\"col3_\"))\\\n.withColumn(\"col1\", F.when(F.col(\"col1_\").isNotNull(), F.col(\"col1_\")).otherwise(F.col(\"col1\")))\\\n.withColumn(\"col2\", F.when(F.col(\"col2_\").isNotNull(),F.col(\"col2_\")).otherwise(F.col(\"col2\")))\\\n.withColumn(\"col3\",F.when(F.col(\"col3_\").isNotNull(),F.col(\"col3_\")).otherwise(F.col(\"col3\")))\\\n.drop(\"col1_\",\"col2_\",\"col3_\")\\\n.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   A|   X|   Y|\n   Z|   Q|   R|\n   S|   J|   T|\n+----+----+----+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.withColumn(\"col6\", F.map_from_arrays(F.split(\"col4\",','),F.split(\"col5\",','))).drop(\"col4\",\"col5\")\\\n.select(\"col1\",\"col2\",\"col3\",F.col(\"col6.col1\").alias(\"col1_\"),F.col(\"col6.col2\").alias(\"col2_\"),F.col(\"col6.col3\").alias(\"col3_\"))\\\n.withColumn(\"col1\", F.when(F.col(\"col1_\").isNotNull(), F.col(\"col1_\")).otherwise(F.col(\"col1\")))\\\n.withColumn(\"col2\", F.when(F.col(\"col2_\").isNotNull(),F.col(\"col2_\")).otherwise(F.col(\"col2\")))\\\n.withColumn(\"col3\",F.when(F.col(\"col3_\").isNotNull(),F.col(\"col3_\")).otherwise(F.col(\"col3\")))\\\n.drop(\"col1_\",\"col2_\",\"col3_\")\\\n.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o2813.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;`col4`&#39; given input columns: [col1, col2, col3, col6];;\n&#39;Project [col1#445, col2#446, col3#447, map_from_arrays(split(&#39;col4, ,), split(&#39;col5, ,)) AS col6#590]\n+- Project [col1#445, col2#446, col3#447, col6#497]\n   +- Project [col1#445, col2#446, col3#447, col4#448, col5#449, array(split(col4#448, ,), split(col5#449, ,)) AS col6#497]\n      +- LogicalRDD [col1#445, col2#446, col3#447, col4#448, col5#449], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:120)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.GeneratedMethodAccessor424.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2063578543544086&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col6&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>map_from_arrays<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col4&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;,&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col5&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;,&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col4&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;col5&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;col3&#34;</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col6.col1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col6.col2&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col2_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col6.col3&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isNotNull<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>otherwise<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col2_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isNotNull<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col2_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>otherwise<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col2&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3&#34;</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isNotNull<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3_&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>otherwise<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;`col4`&#39; given input columns: [col1, col2, col3, col6];;\\n&#39;Project [col1#445, col2#446, col3#447, map_from_arrays(split(&#39;col4, ,), split(&#39;col5, ,)) AS col6#590]\\n+- Project [col1#445, col2#446, col3#447, col6#497]\\n   +- Project [col1#445, col2#446, col3#447, col4#448, col5#449, array(split(col4#448, ,), split(col5#449, ,)) AS col6#497]\\n      +- LogicalRDD [col1#445, col2#446, col3#447, col4#448, col5#449], false\\n&#34;</div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["\ndf.withColumn(\"col6\", F.map_from_arrays(F.split(\"col4\",','),F.split(\"col5\",','))).drop(\"col4\",\"col5\")\\\n.select(\"col1\",\"col2\",\"col3\",F.col(\"col6.col1\").alias(\"col1_\"),F.col(\"col6.col2\").alias(\"col2_\"),F.col(\"col6.col3\").alias(\"col3_\"))\\\n.withColumn(\"col1\", F.when(F.col(\"col1_\").isNotNull(), F.col(\"col1_\")).otherwise(F.col(\"col1\")))\\\n.withColumn(\"col2\", F.when(F.col(\"col2_\").isNotNull(),F.col(\"col2_\")).otherwise(F.col(\"col2\")))\\\n.withColumn(\"col3\",F.when(F.col(\"col3_\").isNotNull(),F.col(\"col3_\")).otherwise(F.col(\"col3\")))\\\n.drop(\"col1_\",\"col2_\",\"col3_\")\\\n.show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   A|   X|   Y|\n   Z|   Q|   R|\n   S|   J|   T|\n+----+----+----+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["%scala\nval df = Seq((\"A\",\"B\",\"C\",\"col2,col3\",\"X,Y\"),\n       (\"P\",\"Q\",\"R\",\"col1\",\"Z\"),\n       (\"I\",\"J\",\"K\",\"col1,col3\",\"S,T\")).toDF(\"col1\",\"col2\",\"col3\",\"col4\",\"col5\")\ndf.show()\n\n\nimport org.apache.spark.sql.functions.{col, map_from_arrays, split, when}\n\ndf.withColumn(\"col6\", map_from_arrays(split($\"col4\",\",\"),split($\"col5\",\",\"))).drop(\"col4\",\"col5\")\n.select($\"col1\",$\"col2\",$\"col3\",col(\"col6.col1\").alias(\"col1_\"),col(\"col6.col2\").alias(\"col2_\"),col(\"col6.col3\").alias(\"col3_\"))\n.withColumn(\"col1\", when(col(\"col1_\").isNotNull, col(\"col1_\")).otherwise($\"col1\"))\n.withColumn(\"col2\", when(col(\"col2_\").isNotNull,col(\"col2_\")).otherwise($\"col2\"))\n.withColumn(\"col3\",when(col(\"col3_\").isNotNull,col(\"col3_\")).otherwise($\"col3\"))\n.drop(\"col1_\",\"col2_\",\"col3_\")\n.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+---------+----+\ncol1|col2|col3|     col4|col5|\n+----+----+----+---------+----+\n   A|   B|   C|col2,col3| X,Y|\n   P|   Q|   R|     col1|   Z|\n   I|   J|   K|col1,col3| S,T|\n+----+----+----+---------+----+\n\n+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   A|   X|   Y|\n   Z|   Q|   R|\n   S|   J|   T|\n+----+----+----+\n\ndf: org.apache.spark.sql.DataFrame = [col1: string, col2: string ... 3 more fields]\nimport org.apache.spark.sql.functions.{col, map_from_arrays, split, when}\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["list=  [['A','X','Y'],         \n        ['Z','Q','R'],       \n        ['S','J','T']]\n\ndf=spark.createDataFrame(list,['col1','col2','col3'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   A|   X|   Y|\n   Z|   Q|   R|\n   S|   J|   T|\n+----+----+----+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["list=[[1,'r1',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"1.0\"}, {\"name\": \"app\", \"version\": \"1.0\"}]}}],\n      [1,'r2',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"2.0\"}, {\"name\": \"app\", \"version\": \"2.0\"}]}}],\n      [2,'r3',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"3.0\"}, {\"name\": \"app\", \"version\": \"3.0\"}]}}]]\ndf=spark.createDataFrame(list,['accountId','resourceId','items'])\ndf.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["list=[[1,'r1',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"1.0\"}, {\"name\": \"app\", \"version\": \"1.0\"}]}}],\n      [1,'r2',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"2.0\"}, {\"name\": \"app\", \"version\": \"2.0\"}]}}],\n      [2,'r3',{\"application\":{\"component\":[{\"name\": \"tool\", \"version\": \"3.0\"}, {\"name\": \"app\", \"version\": \"3.0\"}]}}]]\ndf=spark.createDataFrame(list,['accountId','resourceId','items'])\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----------+-----------------------------------------------------------------------------------------------+\naccountId|resourceId|items                                                                                          |\n+---------+----------+-----------------------------------------------------------------------------------------------+\n1        |r1        |[application -&gt; [component -&gt; [[name -&gt; tool, version -&gt; 1.0], [name -&gt; app, version -&gt; 1.0]]]]|\n1        |r2        |[application -&gt; [component -&gt; [[name -&gt; tool, version -&gt; 2.0], [name -&gt; app, version -&gt; 2.0]]]]|\n2        |r3        |[application -&gt; [component -&gt; [[name -&gt; tool, version -&gt; 3.0], [name -&gt; app, version -&gt; 3.0]]]]|\n+---------+----------+-----------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- accountId: long (nullable = true)\n-- resourceId: string (nullable = true)\n-- items: map (nullable = true)\n    |-- key: string\n    |-- value: map (valueContainsNull = true)\n    |    |-- key: string\n    |    |-- value: array (valueContainsNull = true)\n    |    |    |-- element: map (containsNull = true)\n    |    |    |    |-- key: string\n    |    |    |    |-- value: string (valueContainsNull = true)\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>accountId</th><th>resourceId</th><th>items</th></tr></thead><tbody><tr><td>1</td><td>r1</td><td>Map(application -> Map(component -> List(Map(name -> tool, version -> 1.0), Map(name -> app, version -> 1.0))))</td></tr><tr><td>1</td><td>r2</td><td>Map(application -> Map(component -> List(Map(name -> tool, version -> 2.0), Map(name -> app, version -> 2.0))))</td></tr><tr><td>2</td><td>r3</td><td>Map(application -> Map(component -> List(Map(name -> tool, version -> 3.0), Map(name -> app, version -> 3.0))))</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.select(\"accountId\",\"resourceId\", F.explode(F.col(\"items.application\"))).withColumn(\"value\", F.explode(\"value\")).select(\"accountId\",\"resourceId\",\"value.name\",\"value.version\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----------+----+-------+\naccountId|resourceId|name|version|\n+---------+----------+----+-------+\n        1|        r1|tool|    1.0|\n        1|        r1| app|    1.0|\n        1|        r2|tool|    2.0|\n        1|        r2| app|    2.0|\n        2|        r3|tool|    3.0|\n        2|        r3| app|    3.0|\n+---------+----------+----+-------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.withColumn(\"items\", F.explode(F.col(\"items.application.component\"))).select(\"accountId\",\"resourceId\",\"items.name\",\"items.version\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----------+----+-------+\naccountId|resourceId|name|version|\n+---------+----------+----+-------+\n        1|        r1|tool|    1.0|\n        1|        r1| app|    1.0|\n        1|        r2|tool|    2.0|\n        1|        r2| app|    2.0|\n        2|        r3|tool|    3.0|\n        2|        r3| app|    3.0|\n+---------+----------+----+-------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["df.select(\"items.*\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3642.select.\n: org.apache.spark.sql.AnalysisException: Can only star expand struct data types. Attribute: `ArrayBuffer(items)`;\n\tat org.apache.spark.sql.catalyst.analysis.UnresolvedStar.expand(unresolved.scala:321)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList$1.apply(Analyzer.scala:1045)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList$1.apply(Analyzer.scala:1043)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList(Analyzer.scala:1043)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:953)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:948)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:948)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:775)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat sun.reflect.GeneratedMethodAccessor390.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-590345435021710&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;items.*&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">select</span><span class=\"ansi-blue-fg\">(self, *cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1350</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">12</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">15</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1351</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1352</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jcols<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>cols<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1353</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1354</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;Can only star expand struct data types. Attribute: `ArrayBuffer(items)`;&#39;</div>"]}}],"execution_count":17},{"cell_type":"code","source":["root\n |-- accountId: \n |-- resourceId: \n |-- PeriodId: \n |-- items: \n |    |-- application: \n |    |    |-- component: array"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nfrom pyspark.storagelevel import StorageLevel\n\nlist=[[1,None,None],\n       [2,'2',None],\n        [3,None,None],\n        [5, None, None],\n          [4,'four', 'IV']]\n\ndf=spark.createDataFrame(list,['VendorID','col2','col3'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----+----+\nVendorID|col2|col3|\n+--------+----+----+\n       1|null|null|\n       2|   2|null|\n       3|null|null|\n       5|null|null|\n       4|four|  IV|\n+--------+----+----+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["def getboth(df):\n  return df.filter(' and '.join([''+x+' is not null' for x in df.columns])), df.exceptAll(df.filter(' and '.join([''+x+' is not null' for x in df.columns])))\n\nNotNull, complete= getboth(df)\nNotNull.show()\ncomplete.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   4|four|  IV|\n+----+----+----+\n\n+----+----+----+\ncol1|col2|col3|\n+----+----+----+\nnull|null|null|\n   3|null|null|\n   1| one|null|\nnull|   2|null|\n+----+----+----+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.filter((' or '.join([''+x+' is not null' for x in df.columns if x !='VendorID'])))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----+----+\nVendorID|col2|col3|\n+--------+----+----+\n       2|   2|null|\n       4|four|  IV|\n+--------+----+----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.dropna(how='all', subset=[x for x in df.columns if x!='VendorID'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----+----+\nVendorID|col2|col3|\n+--------+----+----+\n       2|   2|null|\n       4|four|  IV|\n+--------+----+----+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df.filter(' and '.join([''+x+' is not null' for x in df.columns])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\nnull|null|null|\n+----+----+----+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["\" and \".join([\"\"+x+\" is not null\" for x in df.columns])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[70]: &#39;col1 is not null and col2 is not null and col3 is not null&#39;</div>"]}}],"execution_count":24},{"cell_type":"code","source":["' & '.join(['F.col('+x+').isNotNull()' for x in df.columns])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[48]: &#39;F.col(col1).isNotNull() &amp; F.col(col2).isNotNull() &amp; F.col(col3).isNotNull()&#39;</div>"]}}],"execution_count":25},{"cell_type":"code","source":["df1=df.dropna(how=\"any\")\n\ndf1.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   4|four|  IV|\n+----+----+----+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df.exceptAll(df1).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\nnull|null|null|\n   3|null|null|\n   1| one|null|\nnull|   2|null|\n+----+----+----+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"stackhelp32","notebookId":2063578543544077},"nbformat":4,"nbformat_minor":0}
