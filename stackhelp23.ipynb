{"cells":[{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\n\nlist=([1,5,4],\n    [1,5,None],\n    [1,5,1],\n    [1,5,4],\n    [2,5,1],\n    [2,5,2],\n    [2,5,None],\n    [2,5,None],\n     [2,5,4])\ndf=spark.createDataFrame(list,['I_id','p_id','xyz'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\nI_id|p_id| xyz|\n+----+----+----+\n   1|   5|   4|\n   1|   5|null|\n   1|   5|   1|\n   1|   5|   4|\n   2|   5|   1|\n   2|   5|   2|\n   2|   5|null|\n   2|   5|null|\n   2|   5|   4|\n+----+----+----+\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["w= Window().partitionBy(\"I_id\",\"p_id\").orderBy(F.col(\"xyz\").asc_nulls_first())\nw2= Window().partitionBy(\"I_id\",\"p_id\").orderBy(F.col(\"xyz\").asc_nulls_first()).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\ndf.withColumn(\"xyz1\",F.count(F.col(\"xyz\").isNotNull()).over(w))\\\n.withColumn(\"xyz2\", F.max(F.row_number().over(w)).over(w2))\\\n.withColumn(\"xyz3\", F.first(\"xyz1\").over(w))\\\n.withColumn(\"xyz4\", F.when((F.col(\"xyz2\")-F.col(\"xyz3\"))%2!=0, (F.col(\"xyz2\")-F.col(\"xyz3\"))+1).otherwise(F.col(\"xyz2\")-F.col(\"xyz3\")))\\\n.withColumn(\"xyz4\", (F.col(\"xyz4\")/2).cast(\"int\"))\\\n.withColumn(\"xyz6\", F.col(\"xyz4\")+F.col(\"xyz3\"))\\\n.withColumn(\"xyz5\", F.row_number().over(w))\\\n.withColumn(\"medianr\", F.when(F.col(\"xyz6\")==F.col(\"xyz5\"), F.col(\"xyz\")).otherwise(F.lit(None)))\\\n.withColumn(\"medianr2\", (F.mean(\"medianr\").over(w2)).cast(\"int\"))\\\n.withColumn(\"xyz\", F.when(F.col(\"xyz\").isNull(), F.col(\"medianr2\")).otherwise(F.col(\"xyz\")))\\\n.drop(\"xyz1\",\"xyz2\",\"xyz3\",\"xyz4\",\"xyz5\",\"xyz6\",\"medianr\",\"medianr2\")\\\n.orderBy(\"I_id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+---+\nI_id|p_id|xyz|\n+----+----+---+\n   1|   5|  4|\n   1|   5|  1|\n   1|   5|  4|\n   1|   5|  4|\n   2|   5|  2|\n   2|   5|  2|\n   2|   5|  1|\n   2|   5|  2|\n   2|   5|  4|\n+----+----+---+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["list=[{(11,[1,2,3],[1.0,1.0])},\n      {11,[1,2],[1.0,1.0]},\n       {11,[1,2],[1.0,1.0]},\n       {11,[1,4],[1.0,1.0]},\n       {11,[1,3,4],[1.0,1.0]}]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2595636507959732&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> list=[{(11,[1,2,3],[1.0,1.0])},\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      2</span>       <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-cyan-fg\">11</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>        <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-cyan-fg\">11</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>        <span class=\"ansi-blue-fg\">{</span><span class=\"ansi-cyan-fg\">11</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">1.0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">}</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>        {11,[1,3,4],[1.0,1.0]}]\n\n<span class=\"ansi-red-fg\">TypeError</span>: unhashable type: &#39;list&#39;</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df=spark.createDataFrame(list, ['features_array'])\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------+\nfeatures_array             |\n+---------------------------+\n[11, [1, 2, 3], [1.0, 1.0]]|\n[11, [1, 2], [1.0, 1.0]]   |\n[11, [1, 2], [1.0, 1.0]]   |\n[11, [1, 4], [1.0, 1.0]]   |\n[11, [1, 3, 4], [1.0, 1.0]]|\n+---------------------------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df = spark.createDataFrame([\n    (0, \"a\"),\n    (1, \"a\"),\n    (1, \"b\"),\n    (1, \"c\"),\n    (2, \"a\"),\n    (2, \"b\"),\n    (2, \"b\"),\n    (2, \"b\"),\n    (2, \"c\"),\n    (0, \"a\"),\n    (1, \"b\"),\n    (1, \"b\"),\n    (2, \"cc\"),\n    (3, \"a\"),\n    (4, \"a\"),\n    (5, \"c\")\n], [\"id\", \"category\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["pivoted = df.groupBy(\"id\").pivot(\"category\").count().na.fill(0)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\ninput_cols = [x for x in pivoted.columns if x != id]\n\nresult = (VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n    .transform(pivoted)\n    .select(\"id\", \"features\"))\n\n\ndf=result.withColumnRenamed(\"features\", \"features_array\")\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---------------------+\nid |features_array       |\n+---+---------------------+\n0  |(5,[1],[2.0])        |\n5  |(5,[0,3],[5.0,1.0])  |\n1  |[1.0,1.0,3.0,1.0,0.0]|\n3  |(5,[0,1],[3.0,1.0])  |\n2  |[2.0,1.0,3.0,1.0,1.0]|\n4  |(5,[0,1],[4.0,1.0])  |\n+---+---------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.withColumn(\"features1\", F.split(F.regexp_replace((F.regexp_replace(F.regexp_replace(F.col(\"features_array\").cast(\"string\"),'\\(','['),'\\)',']')),'\\[|]',''),',').cast(ArrayType(IntegerType()))).withColumn(\"sum_of_features\", F.expr(\"\"\"aggregate(features1, 0, (acc, x) -> acc + x)\"\"\")).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------------------+---------------+---------------+\n id|      features_array|      features1|sum_of_features|\n+---+--------------------+---------------+---------------+\n  0|       (5,[1],[2.0])|      [5, 1, 2]|              8|\n  5| (5,[0,3],[5.0,1.0])|[5, 0, 3, 5, 1]|             14|\n  1|[1.0,1.0,3.0,1.0,...|[1, 1, 3, 1, 0]|              6|\n  3| (5,[0,1],[3.0,1.0])|[5, 0, 1, 3, 1]|             10|\n  2|[2.0,1.0,3.0,1.0,...|[2, 1, 3, 1, 1]|              8|\n  4| (5,[0,1],[4.0,1.0])|[5, 0, 1, 4, 1]|             11|\n+---+--------------------+---------------+---------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df1.select(F.sum(\"sum_of_features\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\nsum(sum_of_features)|\n+--------------------+\n                  57|\n+--------------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["result1.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: long (nullable = true)\n-- features: vector (nullable = true)\n-- features1: array (nullable = true)\n    |-- element: long (containsNull = true)\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["%scala\nval df=Seq(\n       (1,-7.1732833,32.0414966),\n       (2,-7.1732844,32.0414406),\n       (3,-7.1732833,32.0414966),\n       (4,-7.1732833,32.0414966),\n       (5,-7.1732833,32.0414966),\n       (6,-7.1732833,32.0414966)\n        ).toDF(\"seq\",\"longitude\",\"latitude\")\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+----------+\nseq| longitude|  latitude|\n+---+----------+----------+\n  1|-7.1732833|32.0414966|\n  2|-7.1732844|32.0414406|\n  3|-7.1732833|32.0414966|\n  4|-7.1732833|32.0414966|\n  5|-7.1732833|32.0414966|\n  6|-7.1732833|32.0414966|\n+---+----------+----------+\n\ndf: org.apache.spark.sql.DataFrame = [seq: int, longitude: double ... 1 more field]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["%scala\n\ndf= spark.createDataFrame(list,(\"seq\",\"longitude\", \"latitude\"))\ndf.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\nval df=Seq(\n       (1,-7.1732833,32.0414966),\n       (2,-7.1732844,32.0414406),\n       (3,-7.1732833,32.0414966),\n       (4,-7.1732833,32.0414966),\n       (5,-7.1732833,32.0414966),\n       (6,-7.1732833,32.0414966)\n        ).toDF(\"seq\",\"longitude\",\"latitude\")\n\n\nimport org.apache.spark.sql.functions.lead \nimport org.apache.spark.sql.functions.col \n\nval w = org.apache.spark.sql.expressions.Window.orderBy(\"date\").orderBy(\"seq\")\n\ndf.withColumn(\"destination_longitude\", lead(\"longitude\",1,0).over(w)).withColumn(\"destination_latitude\", lead(\"latitude\",1,0).over(w)).select(col(\"longitude\").alias(\"origin_longitude\"),col(\"destination_longitude\"),col(\"latitude\").alias(\"origin_latitude\"),col(\"destination_latitude\")).filter(col(\"destination_longitude\")!==0.0).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+---------------------+---------------+--------------------+\norigin_longitude|destination_longitude|origin_latitude|destination_latitude|\n+----------------+---------------------+---------------+--------------------+\n      -7.1732833|           -7.1732844|     32.0414966|          32.0414406|\n      -7.1732844|           -7.1732833|     32.0414406|          32.0414966|\n      -7.1732833|           -7.1732833|     32.0414966|          32.0414966|\n      -7.1732833|           -7.1732833|     32.0414966|          32.0414966|\n      -7.1732833|           -7.1732833|     32.0414966|          32.0414966|\n+----------------+---------------------+---------------+--------------------+\n\ncommand-482160419578958:16: warning: method !== in class Column is deprecated: !== does not have the same precedence as ===, use =!= instead\ndf.withColumn(&quot;destination_longitude&quot;, lead(&quot;longitude&quot;,1,0).over(w)).withColumn(&quot;destination_latitude&quot;, lead(&quot;latitude&quot;,1,0).over(w)).select(col(&quot;longitude&quot;).alias(&quot;origin_longitude&quot;),col(&quot;destination_longitude&quot;),col(&quot;latitude&quot;).alias(&quot;origin_latitude&quot;),col(&quot;destination_latitude&quot;)).filter(col(&quot;destination_longitude&quot;)!==0.0).show()\n                                                                                                                                                                                                                                                                                                                               ^\ndf: org.apache.spark.sql.DataFrame = [seq: int, longitude: double ... 1 more field]\nimport org.apache.spark.sql.functions.lead\nimport org.apache.spark.sql.functions.col\nw: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6f68f14e\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["%python\nimport numpy as np\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\ncSchema = StructType([StructField(\"col\", LongType())])\nvals = [[0] for i in range(20)]\ntest_df = spark.createDataFrame(vals,schema=cSchema)\n\ntest_df.show(20)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\ncol|\n+---+\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n  0|\n+---+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["test_df = test_df.replace(float('nan'), None)\ntest_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\n col|\n+----+\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\nnull|\n+----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["df = sqlContext.createDataFrame([\n    ('www.example.com/researc', 'Research Reports'),\n    ('www.example.com/careers', 'Careers'),\n    ('www.example.com/blogs', 'blogs'),\n    ('www.example.com', None),\n    ('www.example.com/navigation', None),\n    ('www.example.com', None),\n    ('www.example.jp', None),\n    ('', 'blogs')], ['A', 'B'])\n\ndf.show()\n\nfilter_mask = df.where(df['B'].isNotNull())\nprint(\"\\n\\nFilter Mask\")\nfilter_mask.show()\n\nprint('\\n\\nfilter_mask[A]')\n#filter_mask.select('A').show()\n\n# Why is \"response\" returning everything?!\nresponse = df.filter(filter_mask.A.isin(df.A))\nprint(\"\\n\\nResulting DF\")\nresponse.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------------+\n                   A|               B|\n+--------------------+----------------+\nwww.example.com/r...|Research Reports|\nwww.example.com/c...|         Careers|\nwww.example.com/b...|           blogs|\n     www.example.com|            null|\nwww.example.com/n...|            null|\n     www.example.com|            null|\n      www.example.jp|            null|\n                    |           blogs|\n+--------------------+----------------+\n\n\n\nFilter Mask\n+--------------------+----------------+\n                   A|               B|\n+--------------------+----------------+\nwww.example.com/r...|Research Reports|\nwww.example.com/c...|         Careers|\nwww.example.com/b...|           blogs|\n                    |           blogs|\n+--------------------+----------------+\n\n\n\nfilter_mask[A]\n\n\nResulting DF\n+--------------------+----------------+\n                   A|               B|\n+--------------------+----------------+\nwww.example.com/r...|Research Reports|\nwww.example.com/c...|         Careers|\nwww.example.com/b...|           blogs|\n     www.example.com|            null|\nwww.example.com/n...|            null|\n     www.example.com|            null|\n      www.example.jp|            null|\n                    |           blogs|\n+--------------------+----------------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["list=[['1'],['-2'],['-1'],['0']]\ndf=spark.createDataFrame(list, [\"number\"])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nnumber|\n+------+\n     1|\n    -2|\n    -1|\n     0|\n+------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.withColumn(\"number\", F.abs(F.col(\"number\")).cast('int')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nnumber|\n+------+\n     1|\n     2|\n     1|\n     0|\n+------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nlist=[['USDINRFUTCUR23Feb201700000000FF00000000000001990067895000000000NNN*12'],\n      ['USDINRFUTCUR24Feb201700000000FF00000000000001990067895000000000NNN*12'],\n      ['USDINRFUTCUR25Feb201700000000FF00000000000001990067895000000000NNN*12']]\n\ndf=spark.createDataFrame(list,['col1'])\n\ndf.show(truncate=False)\n\ndf.withColumn(\"Currency1\", F.col(\"col1\").substr(0,3))\\\n  .withColumn(\"Currency2\", F.col(\"col1\").substr(4,3))\\\n  .withColumn(\"Type\", F.col(\"col1\").substr(7,6))\\\n  .withColumn(\"Time\", F.expr(\"\"\"substr(col1,13,length(col1))\"\"\"))\\\n  .drop(\"col1\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------+\ncol1                                                                 |\n+---------------------------------------------------------------------+\nUSDINRFUTCUR23Feb201700000000FF00000000000001990067895000000000NNN*12|\nUSDINRFUTCUR24Feb201700000000FF00000000000001990067895000000000NNN*12|\nUSDINRFUTCUR25Feb201700000000FF00000000000001990067895000000000NNN*12|\n+---------------------------------------------------------------------+\n\n+---------+---------+------+---------------------------------------------------------+\nCurrency1|Currency2|Type  |Time                                                     |\n+---------+---------+------+---------------------------------------------------------+\nUSD      |INR      |FUTCUR|23Feb201700000000FF00000000000001990067895000000000NNN*12|\nUSD      |INR      |FUTCUR|24Feb201700000000FF00000000000001990067895000000000NNN*12|\nUSD      |INR      |FUTCUR|25Feb201700000000FF00000000000001990067895000000000NNN*12|\n+---------+---------+------+---------------------------------------------------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"stackhelp23","notebookId":2034301554462221},"nbformat":4,"nbformat_minor":0}
