{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nlist=[[15224686, 'Green Dragon', 'The film follows',['Indie','Drama']],\n      [15585766, 'The Rats of Tobruk', 'Three friends are',['Drama']]]\n\n\ncustomSchema= StructType([StructField(\"movie_id\", IntegerType()),\n                        StructField(\"movie_name\", StringType()),\n                        StructField(\"plot\", StringType()),\n                          StructField(\"genre\", ArrayType(StringType()))])\n\ndf=spark.createDataFrame(list,customSchema)\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+------------------+-----------------+--------------+\nmovie_id|        movie_name|             plot|         genre|\n+--------+------------------+-----------------+--------------+\n15224686|      Green Dragon| The film follows|[Indie, Drama]|\n15585766|The Rats of Tobruk|Three friends are|       [Drama]|\n+--------+------------------+-----------------+--------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["list=[[\"['Indie', 'Drama', 'Action']\"]]\n\ndf=spark.createDataFrame(list,['genre'])\n\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>genre</th></tr></thead><tbody><tr><td>['Indie', 'Drama', 'Action']</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.withColumn(\"genre\", F.split(F.regexp_replace(\"genre\", \"\\[|]| |'\", \"\"),\",\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------+\ngenre                 |\n+----------------------+\n[Indie, Drama, Action]|\n+----------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["list=[[0,[1,2]],\n      [1,[0,1]],\n      [2,[0,1]],\n      [3,[0,1]]]\n\ndf=spark.createDataFrame(list,['id','label'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+\n id| label|\n+---+------+\n  0|[1, 2]|\n  1|[0, 1]|\n  2|[0, 1]|\n  3|[0, 1]|\n+---+------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["labels=['0','1','2']\n\nfrom pyspark.sql import functions as F\ndf.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(label,x->x={}))\"\\\n                                                    .format(\"'\"+y+\"'\"))).alias(y)) for y in labels]))\\\n            .select(\"id\",*[F.col(\"struct.{}.col1\".format(x)).alias('label'+x) for x in labels]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+------+------+\n id|label0|label1|label2|\n+---+------+------+------+\n  0|     0|     1|     1|\n  1|     1|     1|     0|\n  2|     1|     1|     0|\n  3|     1|     1|     0|\n+---+------+------+------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["list=[[['a', 'b', 'b', 'c']],\n     [['b', 'c', 'd']]]\n\ndf=spark.createDataFrame(list,['atr_list'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\n    atr_list|\n+------------+\n[a, b, b, c]|\n   [b, c, d]|\n+------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\na=df.withColumn(\"atr\", F.expr(\"\"\"transform(array_distinct(atr_list),x->aggregate(atr_list,0,(acc,y)->\\\n                               IF(y=x, acc+1,acc)))\"\"\"))\\\n  .withColumn(\"zip\", F.explode(F.arrays_zip(F.array_distinct(\"atr_list\"),(\"atr\"))))\\\n  .select(\"zip.*\").withColumnRenamed(\"0\",\"elements\")\\\n  .groupBy(\"elements\").agg(F.sum(\"atr\").alias(\"sum\"))\\\n  .collect()\n\n{a[i][0]: a[i][1] for i in range(len(a))} "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: {&#39;d&#39;: 1, &#39;c&#39;: 2, &#39;b&#39;: 3, &#39;a&#39;: 1}</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.withColumn(\"atr\", F.expr(\"\"\"transform(array_distinct(atr_list),x->aggregate(atr_list,0,(acc,y)->\\\n                               IF(y=x, acc+1,acc)))\"\"\"))\\\n  .withColumn(\"elements\", F.array_distinct(\"atr_list\"))\\\n  .withColumn(\"atr1\", F.expr(\"\"\"transform(arrays_zip(elements,atr),x-> array(x.atr,x.elements))\"\"\"))\\\n  .withColumn(\"atr1\", F.expr(\"\"\"transform(atr1, x-> struct(x[0] as aggregate(atr,0,(acc,y)->IF(y==x[1],acc=x[1],acc))))\"\"\"))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.sql.functions.expr.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input &#39;(&#39; expecting {&#39;)&#39;, &#39;,&#39;}(line 1, pos 44)\n\n== SQL ==\ntransform(atr1, x-&gt; struct(x[0] as aggregate(atr,0,(acc,y)-&gt;IF(y==x[1],acc=x[1],acc))))\n--------------------------------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:44)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parseExpression(DatabricksSqlParser.scala:46)\n\tat org.apache.spark.sql.functions$.expr(functions.scala:1366)\n\tat org.apache.spark.sql.functions.expr(functions.scala)\n\tat sun.reflect.GeneratedMethodAccessor347.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1896245210851985&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;elements&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>array_distinct<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;atr_list&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;atr1&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;transform(arrays_zip(elements,atr),x-&gt; array(x.atr,x.elements))&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;atr1&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;transform(atr1, x-&gt; struct(x[0] as aggregate(atr,0,(acc,y)-&gt;IF(y==x[1],acc=x[1],acc))))&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   <span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">expr</span><span class=\"ansi-blue-fg\">(str)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    673</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    674</span>     sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context\n<span class=\"ansi-green-fg\">--&gt; 675</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span>str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    676</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    677</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 73</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.streaming.StreamingQueryException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>                 <span class=\"ansi-green-fg\">raise</span> StreamingQueryException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: &#34;\\nmismatched input &#39;(&#39; expecting {&#39;)&#39;, &#39;,&#39;}(line 1, pos 44)\\n\\n== SQL ==\\ntransform(atr1, x-&gt; struct(x[0] as aggregate(atr,0,(acc,y)-&gt;IF(y==x[1],acc=x[1],acc))))\\n--------------------------------------------^^^\\n&#34;</div>"]}}],"execution_count":9},{"cell_type":"code","source":["for i in a:\n  print(i[0],i[1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">d 1\nc 2\nb 3\na 1\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["list=[[[['01', '100.0'], ['01', '400.0'], [None, '0.0'], ['06', '0.0'], ['01', '400.0'], [None, '0.0'], ['06', '200.0']]],\n      [[[None, '200.0'], ['06', '300.0'], ['01', '500'], ['06', '100.0'], ['01', '200'], ['07', '50.0']]]]\n\n\ndf=spark.createDataFrame(list,['arr'])\n\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------------------+\narr                                                                              |\n+---------------------------------------------------------------------------------+\n[[01, 100.0], [01, 400.0], [, 0.0], [06, 0.0], [01, 400.0], [, 0.0], [06, 200.0]]|\n[[, 200.0], [06, 300.0], [01, 500], [06, 100.0], [01, 200], [07, 50.0]]          |\n+---------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df.withColumn(\"Max_01\", F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='01')),z-> z[1]))\"\"\").cast('float'))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------+------+\narr                                                                               |Max_01|\n+----------------------------------------------------------------------------------+------+\n[[01, 100.0], [01, 400.0], [, 0.0], [06, 0.0], [01, 1000.0], [, 0.0], [06, 200.0]]|400.0 |\n[[, 200.0], [06, 300.0], [01, 500], [06, 100.0], [01, 200], [07, 50.0]]           |500.0 |\n+----------------------------------------------------------------------------------+------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf.withColumn(\"Max_01\",F.when(F.size(F.expr(\"\"\"filter(arr,x-> exists(x,y->y='01'))\"\"\"))!=0,\n       F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='01')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n  .withColumn(\"Max_06\",F.when(F.size(F.expr(\"\"\"filter(arr,x-> exists(x,y->y='06'))\"\"\"))!=0,\n       F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='06')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n  .withColumn(\"Max_07\",F.when(F.size(F.expr(\"\"\"filter(arr,x-> exists(x,y->y='07'))\"\"\"))!=0,\n       F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='07')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n   .show(truncate=False)\n\n#+---------------------------------------------------------------------------------+------+------+------+\n#|arr                                                                              |Max_01|Max_06|Max_07|\n#+---------------------------------------------------------------------------------+------+------+------+\n#|[[01, 100.0], [01, 400.0], [, 0.0], [06, 0.0], [01, 400.0], [, 0.0], [06, 200.0]]|400.0 |200.0 |0.0   |\n#|[[, 200.0], [06, 300.0], [01, 500], [06, 100.0], [01, 200], [07, 50.0]]          |500.0 |300.0 |50.0  |\n#+---------------------------------------------------------------------------------+------+------+------+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------------------+------+------+------+\narr                                                                              |Max_01|Max_06|Max_07|\n+---------------------------------------------------------------------------------+------+------+------+\n[[01, 100.0], [01, 400.0], [, 0.0], [06, 0.0], [01, 400.0], [, 0.0], [06, 200.0]]|400.0 |200.0 |0.0   |\n[[, 200.0], [06, 300.0], [01, 500], [06, 100.0], [01, 200], [07, 50.0]]          |500.0 |300.0 |50.0  |\n+---------------------------------------------------------------------------------+------+------+------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["  .withColumn(\"Max_06\",F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='06')),z-> float(z[1])))\"\"\"))\\\n  .withColumn(\"Max_07\",F.expr(\"\"\"array_max(transform(filter(arr, x-> exists(x,y-> y='07')),z-> float(z[1])))\"\"\"))\\\n  .show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["list=[[1 , False ,'2020-05-01'],\n      [1  , True ,'2020-05-06'],\n      [2   ,True ,'2020-05-04'],\n      [2  ,False ,'2020-05-07'],\n      [2   ,True ,'2020-05-09'],\n      [3  ,False ,'2020-05-11']]\n\ndf=spark.createDataFrame(list,['id','valid','eventdate'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+----------+\n id|valid| eventdate|\n+---+-----+----------+\n  1|false|2020-05-01|\n  1| true|2020-05-06|\n  2| true|2020-05-04|\n  2|false|2020-05-07|\n  2| true|2020-05-09|\n  3|false|2020-05-11|\n+---+-----+----------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\").orderBy(F.to_date(\"eventdate\",\"yyyy-MM-dd\"))\n\ndf.withColumn(\"lead\", F.lead(\"eventdate\").over(w))\\\n  .withColumn(\"sequence\", F.when(F.col(\"lead\").isNotNull(),\n                                 F.expr(\"\"\"sequence(to_date(eventdate),date_sub(to_date(lead),1), interval 1 day)\"\"\"))\\\n                                 .otherwise(F.array(\"eventdate\")))\\\n .select(\"id\",\"valid\",F.explode(\"sequence\").alias(\"eventdate\"))\\\n .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+----------+\nid |valid|eventdate |\n+---+-----+----------+\n1  |false|2020-05-01|\n1  |false|2020-05-02|\n1  |false|2020-05-03|\n1  |false|2020-05-04|\n1  |false|2020-05-05|\n1  |true |2020-05-06|\n3  |false|2020-05-11|\n2  |true |2020-05-04|\n2  |true |2020-05-05|\n2  |true |2020-05-06|\n2  |false|2020-05-07|\n2  |false|2020-05-08|\n2  |true |2020-05-09|\n+---+-----+----------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["list=[['Xxxxx998',    None  , '01'   ,   '04'    ,  2000    ,1000,    100],     \n      ['Xxxxx997' ,   '01'      ,'01'    ,  '07'     , 200    , 300  ,   400],    \n      ['Xxxxx996'  ,  '07'      ,None  ,  None   , 100  ,   None ,   None],   \n      ['Xxxx910'    , None    ,None  ,  None  , 300,     100  ,   200]] \n\ndf=spark.createDataFrame(list,['Primary_id','Code_1','Code_2','Code_3','Amt_1','Amt_2','Amt_3'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+------+------+-----+-----+-----+\nPrimary_id|Code_1|Code_2|Code_3|Amt_1|Amt_2|Amt_3|\n+----------+------+------+------+-----+-----+-----+\n  Xxxxx998|  null|    01|    04| 2000| 1000|  100|\n  Xxxxx997|    01|    01|    07|  200|  300|  400|\n  Xxxxx996|    07|  null|  null|  100| null| null|\n   Xxxx910|  null|  null|  null|  300|  100|  200|\n+----------+------+------+------+-----+-----+-----+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndictionary = dict(zip(['Code_1','Code_2','Code_3'], ['Amt_1','Amt_2','Amt_3']))\n\ndf.withColumn(\"trial\", F.array(*[F.array(F.col(x),F.col(y).cast(\"string\"))\\\n                                          for x,y in dictionary.items()]))\\\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+------+------+-----+-----+-----+------+------+------+\nPrimary_id|Code_1|Code_2|Code_3|Amt_1|Amt_2|Amt_3|Max_01|Max_07|Max_06|\n+----------+------+------+------+-----+-----+-----+------+------+------+\nXxxxx998  |null  |01    |04    |2000 |1000 |100  |1000  |0     |0     |\nXxxxx997  |01    |01    |07    |200  |300  |400  |300   |400   |0     |\nXxxxx996  |07    |null  |null  |100  |null |null |0     |100   |0     |\nXxxx910   |null  |null  |null  |300  |100  |200  |0     |0     |0     |\n+----------+------+------+------+-----+-----+-----+------+------+------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndictionary = dict(zip(['Code_1','Code_2','Code_3'], ['Amt_1','Amt_2','Amt_3']))\n\ndf.withColumn(\"trial\", F.array(*[F.array(F.col(x),F.col(y).cast(\"string\"))\\\n                                          for x,y in dictionary.items()]))\\\n  .withColumn(\"Max_01\",F.when(F.size(F.expr(\"\"\"filter(trial,x-> exists(x,y->y='01'))\"\"\"))!=0,\\\n       F.expr(\"\"\"array_max(transform(filter(trial, x-> exists(x,y-> y='01')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n  .withColumn(\"Max_06\",F.when(F.size(F.expr(\"\"\"filter(trial,x-> exists(x,y->y='06'))\"\"\"))!=0,\\\n       F.expr(\"\"\"array_max(transform(filter(trial, x-> exists(x,y-> y='06')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n  .withColumn(\"Max_07\",F.when(F.size(F.expr(\"\"\"filter(trial,x-> exists(x,y->y='07'))\"\"\"))!=0,\\\n       F.expr(\"\"\"array_max(transform(filter(trial, x-> exists(x,y-> y='07')),z-> float(z[1])))\"\"\"))\\\n             .otherwise(F.lit(0)))\\\n  .drop(\"trial\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+------+------+-----+-----+-----+------+------+------+\nPrimary_id|Code_1|Code_2|Code_3|Amt_1|Amt_2|Amt_3|Max_01|Max_06|Max_07|\n+----------+------+------+------+-----+-----+-----+------+------+------+\nXxxxx998  |null  |01    |04    |2000 |1000 |100  |1000.0|0.0   |0.0   |\nXxxxx997  |01    |01    |07    |200  |300  |400  |300.0 |0.0   |400.0 |\nXxxxx996  |07    |null  |null  |100  |null |null |0.0   |0.0   |100.0 |\nXxxx910   |null  |null  |null  |300  |100  |200  |0.0   |0.0   |0.0   |\n+----------+------+------+------+-----+-----+-----+------+------+------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["Primary_id  Code_1  Code_2  Code_3  Amt_1   Amt_2   Amt_3   Max_01  Max_07  Max_06\nXxxxx998    Null    01      04      2000    1000    100     1000    0       0\nXxxxx997    01      01      07      200     300     400     300     400     0\nXxxxx996    07      Null    Null    100     Null    Null    100     0       0\nXxxx910     Null    Null    Null    300     100     200     0       0       0"],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"name":"stackhelp56","notebookId":1781706148157676},"nbformat":4,"nbformat_minor":0}
