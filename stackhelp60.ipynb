{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[  0,   'a',  1.1],\n      [ 0,   'b', 0.6],\n      [1,   'b',  0.2],\n      [1,   'c',  0.6],\n      [2,   'c',  1.1],\n      [3,   'a',  0.2],\n      [3,   'b',  0.7]]\n\ndf=spark.createDataFrame(list,['day','user','score'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+-----+\nday|user|score|\n+---+----+-----+\n  0|   a|  1.1|\n  0|   b|  0.6|\n  1|   b|  0.2|\n  1|   c|  0.6|\n  2|   c|  1.1|\n  3|   a|  0.2|\n  3|   b|  0.7|\n+---+----+-----+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"user\").orderBy(\"day\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\ndf.withColumn(\"score\", F.mean(\"score\").over(w)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+------------------+\nday|user|             score|\n+---+----+------------------+\n  1|   c|               0.6|\n  2|   c|0.8500000000000001|\n  0|   b|               0.6|\n  1|   b|               0.4|\n  3|   b|               0.5|\n  0|   a|               1.1|\n  3|   a|              0.65|\n+---+----+------------------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Without any filtering:\n+---+----+-------------------+\n|day|user|              score|\n+---+----+-------------------+\n|  0|   a|                1.1|\n|  0|   b|                0.6|\n|  1|   c|                0.6|\n|  1|   b|                0.5|\n|  1|   a|               0.75|\n|  2|   c| 1.4000000000000001|\n|  2|   b|               0.25|\n|  2|   a|              0.375|\n|  3|   c| 0.7000000000000001|\n|  3|   b|              0.825|\n|  3|   a|             0.3875|\n|  4|   c|0.35000000000000003|\n|  4|   b|             0.4125|\n|  4|   a|            0.19375|\n|  5|   c|0.17500000000000002|\n|  5|   b|            0.20625|\n|  5|   a|           0.096875|\n|  6|   c|0.08750000000000001|\n|  6|   b|           0.103125|\n|  6|   a|          0.0484375|\n+---+----+-------------------+"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["list=[['clinic'],\n       ['office']]\n\ninput_df=spark.createDataFrame(list,['type_txt'])\n\ninput_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\ntype_txt|\n+--------+\n  clinic|\n  office|\n+--------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["condition = \"type_txt = 'clinic'\"\ninput_df1 = input_df.withColumn(\n        \"prm_data_category\",\n        F.when(F.expr(condition), F.lit(\"clinic\")) \n        .when(F.col(\"type_txt\") == 'office', F.lit(\"office\"))\n        .otherwise(F.lit(\"other\"))\n    )\n\ninput_df1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----------------+\ntype_txt|prm_data_category|\n+--------+-----------------+\n  clinic|           clinic|\n  office|           office|\n+--------+-----------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----------------+\ntype_txt|prm_data_category|\n+--------+-----------------+\n  clinic|           clinic|\n  office|           office|\n+--------+-----------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["list=[[['1', '9', '1']],\n      [['2', '2', '2','1', '2']],\n      [['3', '4', '4','1', '4']],\n      [['1', '4']],\n     [['99', '99', '100']],\n     [['92', '11', '92']],\n     [['0', '0', '1']]]\n\ndf=spark.createDataFrame(list,['array'])\n\n\ndf.show()\n       "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\n          array|\n+---------------+\n      [1, 9, 1]|\n[2, 2, 2, 1, 2]|\n[3, 4, 4, 1, 4]|\n         [1, 4]|\n  [99, 99, 100]|\n   [92, 11, 92]|\n      [0, 0, 1]|\n+---------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf\\\n  .withColumn(\"count\",\\\n          F.expr(\"\"\"map_from_arrays(array_distinct(array),transform(array_distinct(array),\\\n              x-> size(filter(array,y-> y=x))))\"\"\"))\\\n  .show(truncate=False)\n\n#+---------------+------------------------+\n#|array          |count                   |\n#+---------------+------------------------+\n#|[1, 9, 1]      |[1 -> 2, 9 -> 1]        |\n#|[2, 2, 2, 1, 2]|[2 -> 4, 1 -> 1]        |\n#|[3, 4, 4, 1, 4]|[3 -> 1, 4 -> 3, 1 -> 1]|\n#|[1, 4]         |[1 -> 1, 4 -> 1]        |\n#|[99, 99, 100]  |[99 -> 2, 100 -> 1]     |\n#|[92, 11, 92]   |[92 -> 2, 11 -> 1]      |\n#|[0, 0, 1]      |[0 -> 2, 1 -> 1]        |\n#+---------------+------------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------------+\narray          |count                   |\n+---------------+------------------------+\n[1, 9, 1]      |[1 -&gt; 2, 9 -&gt; 1]        |\n[2, 2, 2, 1, 2]|[2 -&gt; 4, 1 -&gt; 1]        |\n[3, 4, 4, 1, 4]|[3 -&gt; 1, 4 -&gt; 3, 1 -&gt; 1]|\n[1, 4]         |[1 -&gt; 1, 4 -&gt; 1]        |\n[99, 99, 100]  |[99 -&gt; 2, 100 -&gt; 1]     |\n[92, 11, 92]   |[92 -&gt; 2, 11 -&gt; 1]      |\n[0, 0, 1]      |[0 -&gt; 2, 1 -&gt; 1]        |\n+---------------+------------------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["df.show() #sample dataframe\n#+---------------+\n#|          array|\n#+---------------+\n#|      [1, 1, 1]|\n#|[2, 1, 3, 3, 2]|\n#|         [8, 99]|\n#|      [9, 7, G]|\n#|      [S, T, U]|\n#|      [G, C, G]|\n#+---------------+\n\nfrom pyspark.sql import functions as F\ndf\\\n  .withColumn(\"count\",\\\n          F.expr(\"\"\"array_sort(transform(array_distinct(array),\\\n              x-> aggregate(sort_array(array), 0,(acc,t)->acc+IF(t=x,1,0))))\"\"\"))\\\n  .withColumn(\"zip\", F.map_from_arrays(F.array_distinct(F.sort_array(F.col(\"array\"))),F.col(\"count\")))\\\n  .show(truncate=False)\n\n#+---------------+---------+------------------------+\n#|array          |count    |zip                     |\n#+---------------+---------+------------------------+\n#|[A, A, B]      |[1, 2]   |[A -> 1, B -> 2]        |\n#|[D, P, E, P, P]|[1, 1, 3]|[D -> 1, E -> 1, P -> 3]|\n#|[H, X]         |[1, 1]   |[H -> 1, X -> 1]        |\n#|[P, Q, G]      |[1, 1, 1]|[G -> 1, P -> 1, Q -> 1]|\n#|[S, T, U]      |[1, 1, 1]|[S -> 1, T -> 1, U -> 1]|\n#|[G, C, G]      |[1, 2]   |[C -> 1, G -> 2]        |\n#+---------------+---------+------------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\n          array|\n+---------------+\n      [1, 9, 1]|\n[2, 2, 2, 1, 2]|\n[3, 4, 4, 1, 4]|\n         [1, 4]|\n  [99, 99, 100]|\n   [92, 11, 92]|\n      [0, 0, 1]|\n+---------------+\n\n+---------------+---------+------------------------+\narray          |count    |zip                     |\n+---------------+---------+------------------------+\n[1, 9, 1]      |[1, 2]   |[1 -&gt; 1, 9 -&gt; 2]        |\n[2, 2, 2, 1, 2]|[1, 4]   |[1 -&gt; 1, 2 -&gt; 4]        |\n[3, 4, 4, 1, 4]|[1, 1, 3]|[1 -&gt; 1, 3 -&gt; 1, 4 -&gt; 3]|\n[1, 4]         |[1, 1]   |[1 -&gt; 1, 4 -&gt; 1]        |\n[99, 99, 100]  |[1, 2]   |[100 -&gt; 1, 99 -&gt; 2]     |\n[92, 11, 92]   |[1, 2]   |[11 -&gt; 1, 92 -&gt; 2]      |\n[0, 0, 1]      |[1, 2]   |[0 -&gt; 1, 1 -&gt; 2]        |\n+---------------+---------+------------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#+---------------+\n#|          array|\n#+---------------+\n#|      [1, 9, 1]|\n#|[2, 2, 2, 1, 2]|\n#|[3, 4, 4, 1, 4]|\n#|         [1, 4]|\n#|  [99, 99, 100]|\n#|   [92, 11, 92]|\n#|      [0, 0, 1]|\n#+---------------+\n\nfrom pyspark.sql import functions as F\ndf\\\n  .withColumn(\"count\",\\\n          F.expr(\"\"\"map_from_arrays(array_distinct(array),transform(array_distinct(array),\\\n              x-> aggregate(array, 0,(acc,t)->acc+IF(t=x,1,0))))\"\"\"))\\\n  .show(truncate=False)\n\n#+---------------+------------------------+\n#|array          |count                   |\n#+---------------+------------------------+\n#|[1, 9, 1]      |[1 -> 2, 9 -> 1]        |\n#|[2, 2, 2, 1, 2]|[2 -> 4, 1 -> 1]        |\n#|[3, 4, 4, 1, 4]|[3 -> 1, 4 -> 3, 1 -> 1]|\n#|[1, 4]         |[1 -> 1, 4 -> 1]        |\n#|[99, 99, 100]  |[99 -> 2, 100 -> 1]     |\n#|[92, 11, 92]   |[92 -> 2, 11 -> 1]      |\n#|[0, 0, 1]      |[0 -> 2, 1 -> 1]        |\n#+---------------+------------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------------+\narray          |count                   |\n+---------------+------------------------+\n[1, 9, 1]      |[1 -&gt; 2, 9 -&gt; 1]        |\n[2, 2, 2, 1, 2]|[2 -&gt; 4, 1 -&gt; 1]        |\n[3, 4, 4, 1, 4]|[3 -&gt; 1, 4 -&gt; 3, 1 -&gt; 1]|\n[1, 4]         |[1 -&gt; 1, 4 -&gt; 1]        |\n[99, 99, 100]  |[99 -&gt; 2, 100 -&gt; 1]     |\n[92, 11, 92]   |[92 -&gt; 2, 11 -&gt; 1]      |\n[0, 0, 1]      |[0 -&gt; 2, 1 -&gt; 1]        |\n+---------------+------------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nelements=[1,9,2,3,4,99,100,92,11,0]\ncollected=df.withColumn(\"struct\", F.struct(*[F.struct(F.expr(\"size(filter(atr_list,x->x={}))\"\\\n                                                    .format(y)).alias(str(y)) for y in elements)]))\\\n            .select(*[F.sum(F.col(\"struct.{}.col1\".format(x))).alias(x) for x in elements]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3792028914277906&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> elements<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">9</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">99</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">100</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">92</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">11</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> collected=df.withColumn(&#34;struct&#34;, F.struct(*[F.struct(F.expr(&#34;size(filter(atr_list,x-&gt;x={}))&#34;\\\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">                                                     .format(y)).alias(str(y)) for y in elements)]))\\\n</span><span class=\"ansi-green-intense-fg ansi-bold\">      6</span>             <span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span>F<span class=\"ansi-blue-fg\">.</span>sum<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;struct.{}.col1&#34;</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> x <span class=\"ansi-green-fg\">in</span> elements<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">struct</span><span class=\"ansi-blue-fg\">(*cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    691</span>     <span class=\"ansi-green-fg\">if</span> len<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span> <span class=\"ansi-green-fg\">and</span> isinstance<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">(</span>list<span class=\"ansi-blue-fg\">,</span> set<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    692</span>         cols <span class=\"ansi-blue-fg\">=</span> cols<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-fg\">--&gt; 693</span><span class=\"ansi-red-fg\">     </span>jc <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>struct<span class=\"ansi-blue-fg\">(</span>_to_seq<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">,</span> cols<span class=\"ansi-blue-fg\">,</span> _to_java_column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    694</span>     <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    695</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">_to_seq</span><span class=\"ansi-blue-fg\">(sc, cols, converter)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 65</span><span class=\"ansi-red-fg\">         </span>cols <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>converter<span class=\"ansi-blue-fg\">(</span>c<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> c <span class=\"ansi-green-fg\">in</span> cols<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span>     <span class=\"ansi-green-fg\">return</span> sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">&lt;listcomp&gt;</span><span class=\"ansi-blue-fg\">(.0)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     63</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 65</span><span class=\"ansi-red-fg\">         </span>cols <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span>converter<span class=\"ansi-blue-fg\">(</span>c<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> c <span class=\"ansi-green-fg\">in</span> cols<span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     66</span>     <span class=\"ansi-green-fg\">return</span> sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonUtils<span class=\"ansi-blue-fg\">.</span>toSeq<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">_to_java_column</span><span class=\"ansi-blue-fg\">(col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     51</span>             <span class=\"ansi-blue-fg\">&#34;{0} of type {1}. &#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     52</span>             <span class=\"ansi-blue-fg\">&#34;For column literals, use &#39;lit&#39;, &#39;array&#39;, &#39;struct&#39; or &#39;create_map&#39; &#34;</span>\n<span class=\"ansi-green-fg\">---&gt; 53</span><span class=\"ansi-red-fg\">             &#34;function.&#34;.format(col, type(col)))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">     54</span>     <span class=\"ansi-green-fg\">return</span> jcol\n<span class=\"ansi-green-intense-fg ansi-bold\">     55</span> \n\n<span class=\"ansi-red-fg\">TypeError</span>: Invalid argument, not a string or column: &lt;generator object &lt;genexpr&gt; at 0x7f36ea86b930&gt; of type &lt;class &#39;generator&#39;&gt;. For column literals, use &#39;lit&#39;, &#39;array&#39;, &#39;struct&#39; or &#39;create_map&#39; function.</div>"]}}],"execution_count":13},{"cell_type":"code","source":["elements=[1,9,2,3,4,99,100,92,11,0]\nfrom pyspark.sql import functions as F\ncollected=df.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(array,x->x={}))\"\\\n                                                    .format(y))).alias(str(y))) for y in elements]))\\\n            .select(\"array\",F.split(F.concat_ws(\",\",*[(F.col(\"struct.{}.col1\".format(x)).alias(str(x)+'count'))\\\n                                          for x in elements]).alias(\"count\"),',').alias(\"count\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------------------+\narray          |count                         |\n+---------------+------------------------------+\n[1, 9, 1]      |[2, 1, 0, 0, 0, 0, 0, 0, 0, 0]|\n[2, 2, 2, 1, 2]|[1, 0, 4, 0, 0, 0, 0, 0, 0, 0]|\n[3, 4, 4, 1, 4]|[1, 0, 0, 1, 3, 0, 0, 0, 0, 0]|\n[1, 4]         |[1, 0, 0, 0, 1, 0, 0, 0, 0, 0]|\n[99, 99, 100]  |[0, 0, 0, 0, 0, 2, 1, 0, 0, 0]|\n[92, 11, 92]   |[0, 0, 0, 0, 0, 0, 0, 2, 1, 0]|\n[0, 0, 1]      |[1, 0, 0, 0, 0, 0, 0, 0, 0, 2]|\n+---------------+------------------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["elements=[1,9,2,3,4,99,100,92,11,0]\nfrom pyspark.sql import functions as F\ncollected=df.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(array,x->x={}))\"\\\n                                                    .format(y))).alias(str(y))) for y in elements]))\\\n            .withColumn(\"vals\", F.array(*[(F.col(\"struct.{}.col1\".format(x))) for x in elements]))\\\n            .select(\"array\",F.arrays_zip(F.array(*[F.lit(x) for x in elements]),\\\n                                    F.col(\"vals\")).alias(\"count\"))\\\n            .withColumn(\"count\", F.expr(\"\"\"filter(count,x-> x.vals != 0)\"\"\"))\\\n            .withColumn(\"count\")\n            .show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["elements=[1,9,2,3,4,99,100,92,11,0]\nfrom pyspark.sql import functions as F\ncollected=df.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(array,x->x={}))\"\\\n                                                    .format(y))).alias(str(y))) for y in elements]))\\\n            .withColumn(\"vals\", F.array(*[(F.col(\"struct.{}.col1\".format(x))) for x in elements]))\\\n            .withColumn(\"elems\", F.array(*[F.lit(x) for x in elements]))\\\n            .withColumn(\"count\", F.map_from_entries(F.expr(\"\"\"filter(arrays_zip(elems,vals),x-> x.vals != 0)\"\"\")))\\\n            .select(\"array\",\"count\")\\\n            .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+------------------------+\narray          |count                   |\n+---------------+------------------------+\n[1, 9, 1]      |[1 -&gt; 2, 9 -&gt; 1]        |\n[2, 2, 2, 1, 2]|[1 -&gt; 1, 2 -&gt; 4]        |\n[3, 4, 4, 1, 4]|[1 -&gt; 1, 3 -&gt; 1, 4 -&gt; 3]|\n[1, 4]         |[1 -&gt; 1, 4 -&gt; 1]        |\n[99, 99, 100]  |[99 -&gt; 2, 100 -&gt; 1]     |\n[92, 11, 92]   |[92 -&gt; 2, 11 -&gt; 1]      |\n[0, 0, 1]      |[1 -&gt; 1, 0 -&gt; 2]        |\n+---------------+------------------------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["elements=[1,9,2,3,4,99,100,92,11,0]\nfrom pyspark.sql import functions as F\ncollected=df.withColumn(\"struct\", F.array(*[(F.struct(F.expr(\"size(filter(array,x->x={}))\"\\\n                                                    .format(y))).alias(str(y))) for y in elements]))\\\n             .withColumn(\"struct\", F.expr(\"\"\"filter(struct,x-> x.col1!=0)\"\"\"))\\\n             .printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- array: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- struct: array (nullable = false)\n    |-- element: struct (containsNull = false)\n    |    |-- col1: integer (nullable = false)\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["elements=[1,9,2,3,4,99,100,92,11,0]\nfrom pyspark.sql import functions as F\ncollected=df.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(array,x->x={}))\"\\\n                                                    .format(y))).alias(str(y))) for y in elements]))\\\n            .select(\"array\",F.map_from_arrays(F.array(*[F.lit(x) for x in elements]),\\\n                                                       F.array(*[(F.col(\"struct.{}.col1\".format(x)))\\\n                                          for x in elements])))\\\n                    .printSchema()\n            #.select(\"array\", F.expr(\"\"\"filter(count,x->x.)\"\"\".alias(\"count\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- array: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- map_from_arrays(array(1, 9, 2, 3, 4, 99, 100, 92, 11, 0), array(struct.1.col1, struct.9.col1, struct.2.col1, struct.3.col1, struct.4.col1, struct.99.col1, struct.100.col1, struct.92.col1, struct.11.col1, struct.0.col1)): map (nullable = false)\n    |-- key: integer\n    |-- value: integer (valueContainsNull = false)\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["list=[[13,       18,'Name',  'project/sd-03-bloc...',    'true'    ,  'standard',               1.0,           3],\n      [13,         7,'Name',  'project/sd-03-bloc...',    'true' ,      'standard',               1.0,            3],\n      [13,        27,'Name',  'project/sd-03-bloc...',    'true',       'standard',               1.0,            3]]\n\ntable1=spark.createDataFrame(list,['student_id','project_id','name','project_name','approved','evaluation_type'       ,'grade','cohort_number'])\n\n\ntable1.show()\n\n\nlist1=[[    3,       18],\n       [  3,        27],\n     [     4,        15],\n      [    3,         7],\n        [   3,        35]]\ntable2=spark.createDataFrame(list1,['cohort_number','project_id'])\n\ntable2.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+----+--------------------+--------+---------------+-----+-------------+\nstudent_id|project_id|name|        project_name|approved|evaluation_type|grade|cohort_number|\n+----------+----------+----+--------------------+--------+---------------+-----+-------------+\n        13|        18|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n        13|         7|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n        13|        27|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n+----------+----------+----+--------------------+--------+---------------+-----+-------------+\n\n+-------------+----------+\ncohort_number|project_id|\n+-------------+----------+\n            3|        18|\n            3|        27|\n            4|        15|\n            3|         7|\n            3|        35|\n+-------------+----------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["table1.join(table2.withColumnRenamed(\"project_id\",\"project_id2\"), ['cohort_number'],'right')\\\n       .groupBy(\"project_id2\").agg(*[F.first(x).alias(x) for x in table1.columns])\\\n       .dropna().show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+----------+----+--------------------+--------+---------------+-----+-------------+\nproject_id2|student_id|project_id|name|        project_name|approved|evaluation_type|grade|cohort_number|\n+-----------+----------+----------+----+--------------------+--------+---------------+-----+-------------+\n          7|        13|        18|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n         27|        13|        18|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n         35|        13|        18|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n         18|        13|        18|Name|project/sd-03-blo...|    true|       standard|  1.0|            3|\n+-----------+----------+----------+----+--------------------+--------+---------------+-----+-------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["list=[['s1',0,1.2],\n     ['s1',0,2.2],\n     ['s1',1,3.2],\n      ['s1',1,4.2],\n     ['s2',1,5.2],\n     ['s1',2,6.2],\n     ['s1',2,7.2]]\n\ndf=spark.createDataFrame(list,['status','year','close_price'])\n\ndf.show()\n\nlist1=[['s1',0,1.2,0.0],\n     ['s1',0,2.2,0.0],\n     ['s1',1,3.2,1.2],\n      ['s1',1,4.2,2.2],\n     ['s2',1,5.2,0.0],\n     ['s1',2,6.2,3.2],\n     ['s1',2,7.2,4.2]]\n\ndf1=spark.createDataFrame(list1,['status','year','close_price','open_price'])\n       \ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+-----------+\nstatus|year|close_price|\n+------+----+-----------+\n    s1|   0|        1.2|\n    s1|   0|        2.2|\n    s1|   1|        3.2|\n    s1|   1|        4.2|\n    s2|   1|        5.2|\n    s1|   2|        6.2|\n    s1|   2|        7.2|\n+------+----+-----------+\n\n+------+----+-----------+----------+\nstatus|year|close_price|open_price|\n+------+----+-----------+----------+\n    s1|   0|        1.2|       0.0|\n    s1|   0|        2.2|       0.0|\n    s1|   1|        3.2|       1.2|\n    s1|   1|        4.2|       2.2|\n    s2|   1|        5.2|       0.0|\n    s1|   2|        6.2|       3.2|\n    s1|   2|        7.2|       4.2|\n+------+----+-----------+----------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.show() #sample data\n\n#+------+----+-----------+\n#|status|year|close_price|\n#+------+----+-----------+\n#|    s1|   0|        1.2|\n#|    s1|   0|        2.2|\n#|    s1|   1|        3.2|\n#|    s1|   1|        4.2|\n#|    s2|   1|        5.2|\n#|    s1|   2|        6.2|\n#|    s1|   2|        7.2|\n#+------+----+-----------+\n\n\nw=Window().partitionBy(\"status\").orderBy(\"mono_id\")\nw1=Window().orderBy(\"mono_id\")\nw2=Window().partitionBy(\"sum\").orderBy(\"mono_id\")\ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n  .withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"sum\", F.sum(F.when(F.col(\"rowNum\")==1, F.lit(1)).otherwise(F.lit(0))).over(w1))\\\n  .withColumn(\"sum\", F.when((F.row_number().over(w2)==1) & (F.col(\"sum\")==2), F.lit(1)).otherwise(F.col(\"sum\")))\\\n    .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n     .withColumn(\"lag2\", F.lag(\"close_price\",3).over(w1))\\\n  .withColumn(\"open_price\", F.when((F.col(\"sum\")==1)&(F.col(\"lag1\").isNotNull()), F.col(\"lag1\"))\\\n                             .when((F.col(\"sum\")!=1),F.col(\"lag2\"))\\\n                              .otherwise(F.lit(0)))\\\n .withColumn(\"open_price\", F.when(F.col(\"rowNum\")==1, F.lit(0)).otherwise(F.col(\"open_price\")))\\\n  .orderBy(\"mono_id\").drop(\"mono_id\",\"lag1\",\"lag2\",\"rowNum\")\\\n  .show()\n\n#+------+----+-----------+---+----------+\n#|status|year|close_price|sum|open_price|\n#+------+----+-----------+---+----------+\n#|    s1|   0|        1.2|  1|       0.0|\n#|    s1|   0|        2.2|  1|       0.0|\n#|    s1|   1|        3.2|  1|       1.2|\n#|    s1|   1|        4.2|  1|       2.2|\n#|    s2|   1|        5.2|  1|       0.0|\n#|    s1|   2|        6.2|  2|       3.2|\n#|    s1|   2|        7.2|  2|       4.2|\n#+------+----+-----------+---+----------+\n "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+-----------+---+----------+\nstatus|year|close_price|sum|open_price|\n+------+----+-----------+---+----------+\n    s1|   0|        1.2|  1|       0.0|\n    s1|   0|        2.2|  1|       0.0|\n    s1|   1|        3.2|  1|       1.2|\n    s1|   1|        4.2|  1|       2.2|\n    s2|   1|        5.2|  1|       0.0|\n    s1|   2|        6.2|  2|       3.2|\n    s1|   2|        7.2|  2|       4.2|\n+------+----+-----------+---+----------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["lagfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n\n\nw1=Window().orderBy(F.col(\"mono_id\"))\nw2=Window().partitionBy(\"status\").orderBy(\"mono_id\")\n\n\ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n   .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n     .withColumn(\"lag2\", F.lag(\"close_price\",3).over(w1))\\\n              .withColumn(\"open_price\",F.when(F.row_number().over(w2)==1,\\\n                                   F.lit(0)).when((F.col(\"lag2\").isNull())&(F.col(\"lag1\").isNotNull()),F.col(\"lag1\"))\\\n                                            .when(F.col(\"lag2\").isNull()&(F.col(\"lag1\").isNull()),F.lit(0))\\\n                                                 .otherwise(F.col(\"lag2\"))).orderBy(\"mono_id\")\\\n   `                                             .drop(\"mono_id\",\"lag1\",\"lag2\").show()\n\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+-----------+----------+\nstatus|year|close_price|open_price|\n+------+----+-----------+----------+\n    s1|   0|        1.2|       0.0|\n    s1|   0|        2.2|       0.0|\n    s1|   1|        3.2|       1.2|\n    s2|   1|        4.2|       0.0|\n    s2|   1|        5.2|       2.2|\n    s1|   2|        6.2|       3.2|\n    s2|   2|        7.2|       4.2|\n+------+----+-----------+----------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: [&#39;status&#39;, &#39;close_price&#39;, &#39;open_price&#39;, &#39;close_price1&#39;, &#39;open_price1&#39;]</div>"]}}],"execution_count":24},{"cell_type":"code","source":["\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n\nw1=Window().orderBy(F.col(\"year\"),F.col(\"mono_id\"))\nw2=Window().partitionBy(\"status\").orderBy(F.col(\"year\"),F.col(\"mono_id\"))\n\n\ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n   .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n              .withColumn(\"open_price\",F.when(F.row_number().over(w2)==1,F.lit(0))\\\n                          .when(F.col(\"lag1\").isNull(),F.lit(0))\\\n                                  .otherwise(F.col(\"lag1\")))\\\n    .orderBy(\"year\",\"mono_id\")\\\n    .drop(\"mono_id\",\"lag1\",\"lag2\").show()\n\n\n#+------+-----------+----------+------------+-----------+\n#|status|close_price|open_price|close_price1|open_price1|\n#+------+-----------+----------+------------+-----------+\n#|    s1|        1.2|       0.0|         2.1|        0.0|\n#|    s1|        2.2|       1.2|         3.1|        0.0|\n#|    s1|        3.2|       0.0|         4.1|        3.1|\n#|    s2|        4.2|       2.2|         5.1|        3.1|\n#|    s2|        5.2|       3.2|         6.1|        4.1|\n#|    s1|        6.2|       4.2|         7.1|        5.1|\n#+------+-----------+----------+------------+-----------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+----+-----------+----------+\nstatus|year|close_price|open_price|\n+------+----+-----------+----------+\n    s1|   0|        1.2|       0.0|\n    s1|   0|        2.2|       0.0|\n    s1|   1|        3.2|       1.2|\n    s2|   1|        4.2|       0.0|\n    s2|   1|        5.2|       3.2|\n    s1|   2|        6.2|       4.2|\n    s2|   2|        7.2|       5.2|\n+------+----+-----------+----------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n\nw1=Window().orderBy(F.col(\"mono_id\"))\nw2=Window().partitionBy(\"status\").orderBy(\"mono_id\")\n\n\ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n   .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n     .withColumn(\"lag2\", F.lag(\"close_price\",3).over(w1))\\\n              .withColumn(\"open_price\",F.when(F.row_number().over(w2)==1,\\\n                                   F.lit(0)).when((F.col(\"lag2\").isNull())&(F.col(\"lag1\").isNotNull()),F.col(\"lag1\"))\\\n                                            .when(F.col(\"lag2\").isNull()&(F.col(\"lag1\").isNull()),F.lit(0))\\\n                                                 .otherwise(F.col(\"lag2\"))).orderBy(\"mono_id\")\\\n    .withColumn(\"lag3\", F.lag(\"close_price1\",2).over(w1))\\\n     .withColumn(\"lag4\", F.lag(\"close_price1\",2).over(w1))\\\n              .withColumn(\"open_price1\",F.when(F.row_number().over(w2)==1,\\\n                                   F.lit(0)).when((F.col(\"lag4\").isNull())&(F.col(\"lag3\").isNotNull()),F.col(\"lag3\"))\\\n                                            .when(F.col(\"lag4\").isNull()&(F.col(\"lag3\").isNull()),F.lit(0))\\\n                                                 .otherwise(F.col(\"lag4\"))).orderBy(\"mono_id\")\\\n                                                .drop(\"mono_id\",\"lag1\",\"lag2\",\"lag3\",\"lag4\")\\\n      .show()\n  \n\n#+------+-----------+----------+------------+-----------+\n#|status|close_price|open_price|close_price1|open_price1|\n#+------+-----------+----------+------------+-----------+\n#|    s1|        1.2|       0.0|         2.1|        0.0|\n#|    s1|        2.2|       0.0|         3.1|        2.1|\n#|    s1|        3.2|       1.2|         4.1|        0.0|\n#|    s2|        4.2|       0.0|         5.1|        3.1|\n#|    s2|        5.2|       2.2|         6.1|        4.1|\n#|    s1|        6.2|       3.2|         7.1|        5.1|\n#|    s2|        7.2|       4.2|         7.1|        5.1|\n#+------+-----------+----------+------------+-----------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------+----------+------------+-----------+\nstatus|close_price|open_price|close_price1|open_price1|\n+------+-----------+----------+------------+-----------+\n    s1|        1.2|       0.0|         1.2|        0.0|\n    s1|        2.2|       0.0|         2.1|        0.0|\n    s1|        3.2|       1.2|         3.1|        1.2|\n    s2|        4.2|       0.0|         4.1|        0.0|\n    s2|        5.2|       2.2|         5.1|        3.1|\n    s1|        6.2|       3.2|         6.1|        4.1|\n    s2|        7.2|       4.2|         7.1|        5.1|\n+------+-----------+----------+------------+-----------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n\nw1=Window().orderBy(F.col(\"mono_id\"))\nw2=Window().partitionBy(\"status\").orderBy(\"mono_id\")\n\n\ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n   .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n     .withColumn(\"lag2\", F.lag(\"close_price\",3).over(w1))\\\n              .withColumn(\"open_price\",F.when(F.row_number().over(w2)==1,\\\n                                   F.lit(0)).when((F.col(\"lag2\").isNull())|((F.col(\"lag1\").isNull()),F.lit(0)),\n                                                  F.lit(0))\\\n                                                 .otherwise(F.col(\"lag2\"))).orderBy(\"mono_id\")\\\n                                               .drop(\"mono_id\",\"lag1\",\"lag2\").select(\"status\",\"close_price\",\"open_price\").show()\n\n\n#+------+-----------+----------+------------+-----------+\n#|status|close_price|open_price|close_price1|open_price1|\n#+------+-----------+----------+------------+-----------+\n#|    s1|        1.2|       0.0|         2.1|        0.0|\n#|    s1|        2.2|       0.0|         3.1|        2.1|\n#|    s2|        3.2|       1.2|         4.1|        0.0|\n#|    s2|        4.2|       0.0|         5.1|        3.1|\n#|    s1|        5.2|       2.2|         6.1|        4.1|\n#|    s1|        6.2|       3.2|         7.1|        5.1|\n#|    s1|        7.2|       4.2|         7.1|        5.1|\n#+------+-----------+----------+------------+-----------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2152602049843695&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     11</span>      <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;lag2&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>lag<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;close_price&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">3</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>over<span class=\"ansi-blue-fg\">(</span>w1<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span>               .withColumn(&#34;open_price&#34;,F.when(F.row_number().over(w2)==1,\\\n<span class=\"ansi-green-fg\">---&gt; 13</span><span class=\"ansi-red-fg\">                                    </span>F<span class=\"ansi-blue-fg\">.</span>lit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;lag2&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isNull<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">|</span><span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;lag1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isNull<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>lit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     14</span>                                                   F.lit(0))\\\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span>                                                  .otherwise(F.col(&#34;lag2&#34;))).orderBy(&#34;mono_id&#34;)\\\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">_</span><span class=\"ansi-blue-fg\">(self, other)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>     <span class=\"ansi-green-fg\">def</span> _<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> other<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>         jc <span class=\"ansi-blue-fg\">=</span> other<span class=\"ansi-blue-fg\">.</span>_jc <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">else</span> other\n<span class=\"ansi-green-fg\">--&gt; 115</span><span class=\"ansi-red-fg\">         </span>njc <span class=\"ansi-blue-fg\">=</span> getattr<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>njc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>     _<span class=\"ansi-blue-fg\">.</span>__doc__ <span class=\"ansi-blue-fg\">=</span> doc\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1246</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1247</span>     <span class=\"ansi-green-fg\">def</span> __call__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1248</span><span class=\"ansi-red-fg\">         </span>args_command<span class=\"ansi-blue-fg\">,</span> temp_args <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_build_args<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1249</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1250</span>         command <span class=\"ansi-blue-fg\">=</span> proto<span class=\"ansi-blue-fg\">.</span>CALL_COMMAND_NAME <span class=\"ansi-blue-fg\">+</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_build_args</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1210</span>     <span class=\"ansi-green-fg\">def</span> _build_args<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1211</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>converters <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> len<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1212</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-blue-fg\">(</span>new_args<span class=\"ansi-blue-fg\">,</span> temp_args<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_get_args<span class=\"ansi-blue-fg\">(</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1213</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1214</span>             new_args <span class=\"ansi-blue-fg\">=</span> args\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_get_args</span><span class=\"ansi-blue-fg\">(self, args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1197</span>                 <span class=\"ansi-green-fg\">for</span> converter <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>                     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">.</span>can_convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1199</span><span class=\"ansi-red-fg\">                         </span>temp_arg <span class=\"ansi-blue-fg\">=</span> converter<span class=\"ansi-blue-fg\">.</span>convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1200</span>                         temp_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>                         new_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</span> in <span class=\"ansi-cyan-fg\">convert</span><span class=\"ansi-blue-fg\">(self, object, gateway_client)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    499</span>         java_list <span class=\"ansi-blue-fg\">=</span> ArrayList<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    500</span>         <span class=\"ansi-green-fg\">for</span> element <span class=\"ansi-green-fg\">in</span> object<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 501</span><span class=\"ansi-red-fg\">             </span>java_list<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">(</span>element<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    502</span>         <span class=\"ansi-green-fg\">return</span> java_list\n<span class=\"ansi-green-intense-fg ansi-bold\">    503</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1246</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1247</span>     <span class=\"ansi-green-fg\">def</span> __call__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1248</span><span class=\"ansi-red-fg\">         </span>args_command<span class=\"ansi-blue-fg\">,</span> temp_args <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_build_args<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1249</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1250</span>         command <span class=\"ansi-blue-fg\">=</span> proto<span class=\"ansi-blue-fg\">.</span>CALL_COMMAND_NAME <span class=\"ansi-blue-fg\">+</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_build_args</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1210</span>     <span class=\"ansi-green-fg\">def</span> _build_args<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1211</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>converters <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> len<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1212</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-blue-fg\">(</span>new_args<span class=\"ansi-blue-fg\">,</span> temp_args<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_get_args<span class=\"ansi-blue-fg\">(</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1213</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1214</span>             new_args <span class=\"ansi-blue-fg\">=</span> args\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_get_args</span><span class=\"ansi-blue-fg\">(self, args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1197</span>                 <span class=\"ansi-green-fg\">for</span> converter <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>                     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">.</span>can_convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1199</span><span class=\"ansi-red-fg\">                         </span>temp_arg <span class=\"ansi-blue-fg\">=</span> converter<span class=\"ansi-blue-fg\">.</span>convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1200</span>                         temp_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>                         new_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</span> in <span class=\"ansi-cyan-fg\">convert</span><span class=\"ansi-blue-fg\">(self, object, gateway_client)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    498</span>         ArrayList <span class=\"ansi-blue-fg\">=</span> JavaClass<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;java.util.ArrayList&#34;</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    499</span>         java_list <span class=\"ansi-blue-fg\">=</span> ArrayList<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 500</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">for</span> element <span class=\"ansi-green-fg\">in</span> object<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    501</span>             java_list<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">(</span>element<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    502</span>         <span class=\"ansi-green-fg\">return</span> java_list\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    342</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    343</span>     <span class=\"ansi-green-fg\">def</span> __iter__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 344</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Column is not iterable&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    345</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    346</span>     <span class=\"ansi-red-fg\"># string methods</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: Column is not iterable</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n    \n    \nw1=Window().orderBy(F.col(\"mono_id\"))\nw2=Window().partitionBy(\"status\").orderBy(\"mono_id\")\n    \n    \ndf.withColumn(\"mono_id\", F.monotonically_increasing_id())\\\n       .withColumn(\"lag1\", F.lag(\"close_price\",2).over(w1))\\\n         .withColumn(\"lag2\", F.lag(\"close_price\",3).over(w1))\\\n                  .withColumn(\"open_price\",F.when(F.row_number().over(w2)==1,\\\n                                       F.lit(0)).when((F.col(\"lag2\").isNull())&(F.col(\"lag1\").isNotNull()),F.col(\"lag1\"))\\\n                                                .when(F.col(\"lag2\").isNull()&(F.col(\"lag1\").isNull()),F.lit(0))\\\n                                                     .otherwise(F.col(\"lag2\"))).orderBy(\"mono_id\").drop(\"mono_id\",\"lag1\",\"lag2\").select(\"status\",\"close_price\",\"open_price\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------+----------+\nstatus|close_price|open_price|\n+------+-----------+----------+\n    s1|        1.2|       0.0|\n    s1|        2.2|       0.0|\n    s1|        3.2|       1.2|\n    s2|        4.2|       0.0|\n    s2|        5.2|       2.2|\n    s1|        6.2|       3.2|\n    s2|        7.2|       4.2|\n+------+-----------+----------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["+------+-----------+----------+------------+-----------+\n|status|close_price|open_price|close_price1|open_price1|\n+------+-----------+----------+------------+-----------+\n|    s1|        1.2|       0.0|         2.1|        0.0|\n|    s1|        2.2|       0.0|         3.1|        2.1|\n|    s2|        3.2|       1.2|         4.1|        0.0|\n|    s2|        4.2|       0.0|         5.1|        3.1|\n|    s1|        5.2|       2.2|         6.1|        4.1|\n|    s1|        6.2|       3.2|         7.1|        5.1|\n|    s1|        7.2|       4.2|         7.1|        5.1|\n+------+-----------+----------+------------+-----------+"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["list=[[  None     , 4.905615,'2019-08-01 00:00:00',  1],\n     [51.819645, None        ,'2019-08-01 00:00:00',   1],\n     [51.81964, 4.961713,'2019-08-01 00:00:00',   2],\n     [   None      ,   None,      '2019-08-01 00:00:00',   3],\n     [51.82918, 4.911187,        None           ,   3],\n     [51.82385, 4.901488,'2019-08-01 00:00:03',   5]]\n\n\ndf=spark.createDataFrame(list,['latitude','longitude','timestamplast','name'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+-------------------+----+\n latitude|longitude|      timestamplast|name|\n+---------+---------+-------------------+----+\n     null| 4.905615|2019-08-01 00:00:00|   1|\n51.819645|     null|2019-08-01 00:00:00|   1|\n 51.81964| 4.961713|2019-08-01 00:00:00|   2|\n     null|     null|2019-08-01 00:00:00|   3|\n 51.82918| 4.911187|               null|   3|\n 51.82385| 4.901488|2019-08-01 00:00:03|   5|\n+---------+---------+-------------------+----+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["w=Window().partitionBy(\"name\").orderBy(F.lit(1))\n\n\ndf.withColumn(\"latitude\", F.when(F.col(\"latitude\").isNull()))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["list=[['2019-08-01 00:00:00',   1, 0],\n      ['2019-08-01 00:01:00',   1, 60], \n      ['2019-08-01 00:01:15',   1, 15],\n      ['2019-08-01 03:00:00',   2, 0],\n      ['2019-08-01 04:00:00',   2, 3600],\n      ['2019-08-01 00:15:00',   3, 0]]\n\ndf=spark.createDataFrame(list,['timestamplast','name','time_d'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----+------+\n      timestamplast|name|time_d|\n+-------------------+----+------+\n2019-08-01 00:00:00|   1|     0|\n2019-08-01 00:01:00|   1|    60|\n2019-08-01 00:01:15|   1|    15|\n2019-08-01 03:00:00|   2|     0|\n2019-08-01 04:00:00|   2|  3600|\n2019-08-01 00:15:00|   3|     0|\n+-------------------+----+------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"name\").orderBy(F.col(\"timestamplast\"))\ndf.withColumn(\"time_d\", F.lag(F.unix_timestamp(\"timestamplast\")).over(w))\\\n  .withColumn(\"time_d\", F.when(F.col(\"time_d\").isNotNull(), F.unix_timestamp(\"timestamplast\")-F.col(\"time_d\"))\\\n                         .otherwise(F.lit(0))).orderBy(\"name\",\"timestamplast\").show()\n\n#+-------------------+----+------+\n#|      timestamplast|name|time_d|\n#+-------------------+----+------+\n#|2019-08-01 00:00:00|   1|     0|\n#|2019-08-01 00:01:00|   1|    60|\n#|2019-08-01 00:01:15|   1|    15|\n#|2019-08-01 03:00:00|   2|     0|\n#|2019-08-01 04:00:00|   2|  3600|\n#|2019-08-01 00:15:00|   3|     0|\n#+-------------------+----+------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----+------+\n      timestamplast|name|time_d|\n+-------------------+----+------+\n2019-08-01 00:00:00|   1|     0|\n2019-08-01 00:01:00|   1|    60|\n2019-08-01 00:01:15|   1|    15|\n2019-08-01 03:00:00|   2|     0|\n2019-08-01 04:00:00|   2|  3600|\n2019-08-01 00:15:00|   3|     0|\n+-------------------+----+------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["dftmp = spark.createDataFrame([('ab',)], ['data'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndftmp.withColumn('repeat', F.expr(\"\"\"array_repeat(data, len)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o341.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;`len`&#39; given input columns: [data]; line 1 pos 19;\n&#39;Project [data#2, &#39;array_repeat(data#2, &#39;len) AS repeat#4]\n+- LogicalRDD [data#2], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:120)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:322)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8$$anonfun$apply$13.apply(TreeNode.scala:381)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:82)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3543)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1377)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2309)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2030286165518933&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">from</span> pyspark<span class=\"ansi-blue-fg\">.</span>sql <span class=\"ansi-green-fg\">import</span> functions <span class=\"ansi-green-fg\">as</span> F\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>dftmp<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;repeat&#39;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;array_repeat(data, len)&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;`len`&#39; given input columns: [data]; line 1 pos 19;\\n&#39;Project [data#2, &#39;array_repeat(data#2, &#39;len) AS repeat#4]\\n+- LogicalRDD [data#2], false\\n&#34;</div>"]}}],"execution_count":35},{"cell_type":"code","source":["dftmp.withColumn('repeat', F.expr(\"\"\"array_repeat(data, length(data))\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+\ndata|  repeat|\n+----+--------+\n  ab|[ab, ab]|\n+----+--------+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf = spark.createDataFrame([(5000, 'US'),(2500, 'IN'),(4500, 'AU'),(4500, 'NZ')],[\"Sales\", \"Region\"])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+\nSales|Region|\n+-----+------+\n 5000|    US|\n 2500|    IN|\n 4500|    AU|\n 4500|    NZ|\n+-----+------+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["df.when(condition1==True, withColumn('This', lit(\"yes\")) & withColumn('That', lit(\"also yes\"))).otherwise(withColumn('This', lit(\"no\")) & withColumn('That', lit(\"also no\")))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#+-----+------+\n#|Sales|Region|\n#+-----+------+\n#| 5000|    US|\n#| 2500|    IN|\n#| 4500|    AU|\n#| 4500|    NZ|\n#+-----+------+\n\nfrom pyspark.sql import functions as F\n\ndf.withColumn(\"col\", F.when(F.col(\"Region\")=='US',\\\n                            F.struct(F.lit(\"yes\").alias(\"This\"),F.lit(\"also yes\").alias(\"That\")))\\\n                      .otherwise(F.struct(F.lit(\"no\").alias(\"This\"),F.lit(\"also no\").alias(\"That\"))))\\\n                      .select(*df.columns,\"col.*\")\\\n                      .show()\n\n#+-----+------+----+--------+\n#|Sales|Region|This|    That|\n#+-----+------+----+--------+\n#| 5000|    US| yes|also yes|\n#| 2500|    IN|  no| also no|\n#| 4500|    AU|  no| also no|\n#| 4500|    NZ|  no| also no|\n#+-----+------+----+--------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+----+--------+\nSales|Region|This|    That|\n+-----+------+----+--------+\n 5000|    US| yes|also yes|\n 2500|    IN|  no| also no|\n 4500|    AU|  no| also no|\n 4500|    NZ|  no| also no|\n+-----+------+----+--------+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["from pyspark.sql import functions as F\nimport datetime\ndf = spark.createDataFrame([('America/NewYork', '2020-02-01 10:00:00'),('Africa/Nairobi', '2020-02-01 10:00:00')],[\"OriginTz\", \"Time\"])\n\ndf=df.withColumn(\"y\", F.lit('2020-01-01'))\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+-------------------+----------+\n       OriginTz|               Time|         y|\n+---------------+-------------------+----------+\nAmerica/NewYork|2020-02-01 10:00:00|2020-01-01|\n Africa/Nairobi|2020-02-01 10:00:00|2020-01-01|\n+---------------+-------------------+----------+\n\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"new_date\", F.expr(\"\"\"IF(Time<y, Time + interval 14 hours, Time + interval 10 hours)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+-------------------+----------+-------------------+\n       OriginTz|               Time|         y|           new_date|\n+---------------+-------------------+----------+-------------------+\nAmerica/NewYork|2020-02-01 10:00:00|2020-01-01|2020-02-01 20:00:00|\n Africa/Nairobi|2020-02-01 10:00:00|2020-01-01|2020-02-01 20:00:00|\n+---------------+-------------------+----------+-------------------+\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["list=[['\"alex\"john\"', 30 ,  'burlington'  ,'nj',      'usa'],\n      ['\"mohammad\"hashmi\"', 30 ,  'burlington'  ,'nj',      'usa']]\n\ndf=spark.createDataFrame(list,['name','age','county','state','country'])\n\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+---+----------+-----+-------+\nname             |age|county    |state|country|\n+-----------------+---+----------+-----+-------+\n&#34;alex&#34;john&#34;      |30 |burlington|nj   |usa    |\n&#34;mohammad&#34;hashmi&#34;|30 |burlington|nj   |usa    |\n+-----------------+---+----------+-----+-------+\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["df.select(*[F.regexp_replace(x,'^\\\"|\\\"$','').alias(x) for x in df.columns]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+---+----------+-----+-------+\n           name|age|    county|state|country|\n+---------------+---+----------+-----+-------+\n      alex&#34;john| 30|burlington|   nj|    usa|\nmohammad&#34;hashmi| 30|burlington|   nj|    usa|\n+---------------+---+----------+-----+-------+\n\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["df.withColumn(\"name\", F.expr(\"\"\"substring(name,2,length(name)-2)\"\"\")).show()\n\n#+---------+---+----------+-----+-------+\n#|name     |age|county    |state|country|\n#+---------+---+----------+-----+-------+\n#|alex\"john|30 |burlington|nj   |usa    |\n#+---------+---+----------+-----+-------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+---+----------+-----+-------+\n           name|age|    county|state|country|\n+---------------+---+----------+-----+-------+\n      alex&#34;john| 30|burlington|   nj|    usa|\nmohammad&#34;hashmi| 30|burlington|   nj|    usa|\n+---------------+---+----------+-----+-------+\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nlist=[['2019-02-23','2019-02-20',          2],\n      ['2019-03-20','2019-02-20',          7],\n      ['2019-03-21', '2019-02-21',         12],\n      ['2019-03-22', '2019-02-22',         27],\n      ['2019-03-23', '2019-02-23',         91]]\n\ndf=spark.createDataFrame(list,['AsofDate','oneMonthAgo','value'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+\n  AsofDate|oneMonthAgo|value|\n+----------+-----------+-----+\n2019-02-23| 2019-02-20|    2|\n2019-03-20| 2019-02-20|    7|\n2019-03-21| 2019-02-21|   12|\n2019-03-22| 2019-02-22|   27|\n2019-03-23| 2019-02-23|   91|\n+----------+-----------+-----+\n\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(F.dayofmonth(\"AsofDate\"))\\\n          .orderBy(F.to_timestamp(\"AsofDate\").cast(\"long\"))\\\n          .rangeBetween(86400*-30,0)\n\nfirst=F.first(\"value\").over(w)\n\ndf.withColumn(\"1MonthAgoValue\", F.when(first!=F.col(\"value\"), first)\\\n                                 .otherwise(F.lit(None))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+--------------+\n  AsofDate|oneMonthAgo|value|1MonthAgoValue|\n+----------+-----------+-----+--------------+\n2019-03-22| 2019-02-22|   27|          null|\n2019-03-20| 2019-02-20|    7|          null|\n2019-02-23| 2019-02-20|    2|          null|\n2019-03-23| 2019-02-23|   91|             2|\n2019-03-21| 2019-02-21|   12|          null|\n+----------+-----------+-----+--------------+\n\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy(F.year('AsofDate'),F.dayofmonth('AsofDate')).orderBy(F.month('AsofDate'))\n\ndf.withColumn('1MonthAgoValue', F.lag('value').over(w)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------+-----+--------------+\n  AsofDate|oneMonthAgo|value|1MonthAgoValue|\n+----------+-----------+-----+--------------+\n2019-03-22| 2019-02-22|   27|          null|\n2019-02-23| 2019-02-20|    2|          null|\n2019-03-23| 2019-02-23|   91|             2|\n2019-03-21| 2019-02-21|   12|          null|\n2019-03-20| 2019-02-20|    7|          null|\n+----------+-----------+-----+--------------+\n\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"stackhelp60","notebookId":1884350808666411},"nbformat":4,"nbformat_minor":0}
