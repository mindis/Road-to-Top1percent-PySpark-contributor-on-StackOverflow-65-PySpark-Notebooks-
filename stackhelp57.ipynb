{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[324.456 , 'hi' ,    'test'],\n[453.987  ,'hello',  'python'],\n[768.66   ,'test',   'java']]\n\ndf=spark.createDataFrame(list,['col1','col2','col3'])\n\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- col1: double (nullable = true)\n-- col2: string (nullable = true)\n-- col3: string (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.withColumn(\"col4\",F.expr(\"\"\"substring(col1,0,instr(col1),\".\"+2\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.sql.functions.expr.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input &#39;&lt;EOF&gt;&#39; expecting {&#39;)&#39;, &#39;,&#39;}(line 1, pos 34)\n\n== SQL ==\nsubstring(col1,0,instr(col1),&#34;.&#34;+2\n----------------------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:44)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parseExpression(DatabricksSqlParser.scala:46)\n\tat org.apache.spark.sql.functions$.expr(functions.scala:1366)\n\tat org.apache.spark.sql.functions.expr(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2240815886526342&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col4&#34;</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;substring(col1,0,instr(col1),&#34;.&#34;+2&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">expr</span><span class=\"ansi-blue-fg\">(str)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    673</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    674</span>     sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context\n<span class=\"ansi-green-fg\">--&gt; 675</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span>str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    676</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    677</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 73</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.streaming.StreamingQueryException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>                 <span class=\"ansi-green-fg\">raise</span> StreamingQueryException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: &#39;\\nextraneous input \\&#39;&lt;EOF&gt;\\&#39; expecting {\\&#39;)\\&#39;, \\&#39;,\\&#39;}(line 1, pos 34)\\n\\n== SQL ==\\nsubstring(col1,0,instr(col1),&#34;.&#34;+2\\n----------------------------------^^^\\n&#39;</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.withColumn('test',F.regexp_extract(\"col1\",'\\d+[.]\\d{2}',0)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+------+\n   col1| col2|  col3|  test|\n+-------+-----+------+------+\n324.456|   hi|  test|324.45|\n453.987|hello|python|453.98|\n 768.66| test|  java|768.66|\n+-------+-----+------+------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.withColumn(\"col4\", F.expr(\"\"\"IF(length(col1)==7, substring(col1,0,length(col1)-1),col1)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+------+------+\n   col1| col2|  col3|  col4|\n+-------+-----+------+------+\n324.456|   hi|  test|324.45|\n453.987|hello|python|453.98|\n 768.66| test|  java|768.66|\n+-------+-----+------+------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["col1     col2   col3   col4\n324.456  hi     test   324.45\n453.987  hello  python 453.98\n768.66   test   java   768.66"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["list=[['t1', ['m1']],\n      ['t3' ,['m1', 'm2']],\n      ['t4' , ['m1', 'm2']],\n      ['t6' , ['m2']],\n      ['t7' , ['m3']],\n      ['t8' , ['m3']],\n      ['t9' , ['m1']]]\n\n\ndf=spark.createDataFrame(list,['time','message'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+\ntime| message|\n+----+--------+\n  t1|    [m1]|\n  t3|[m1, m2]|\n  t4|[m1, m2]|\n  t6|    [m2]|\n  t7|    [m3]|\n  t8|    [m3]|\n  t9|    [m1]|\n+----+--------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["w=Window().partitionBy(\"message\")\ndf.withColumn(\"mono\", F.monotonically_increasing_id())\\\n  .withColumn(\"message\", F.explode(\"message\"))\\\n  .show()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------+-----------+\ntime|message|       mono|\n+----+-------+-----------+\n  t1|     m1| 8589934592|\n  t3|     m1|17179869184|\n  t3|     m2|17179869184|\n  t4|     m1|25769803776|\n  t4|     m2|25769803776|\n  t6|     m2|34359738368|\n  t7|     m3|42949672960|\n  t8|     m3|51539607552|\n  t9|     m1|60129542144|\n+----+-------+-----------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["+--------+-------+-----+\n message | start | end |\n+--------+-------+-----+\n   m1    |  t1   |  t4 |\n   m2    |  t3   |  t6 |\n   m3    |  t7   |  t8 |\n   m1    |  t9   |  t9 |"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["list=[[1,'2015-01-05' , '2015-01-05',  0.0076],\n     [1,'2015-01-06' , '2015-01-05',  0.0026],\n     [1,'2015-01-07' , '2015-01-05',  0.0016],\n     [1,'2015-01-08' , '2015-01-05',  0.0006],\n     [2,'2015-01-09' , '2015-01-05',  0.0012],\n     [2,'2015-01-10' , '2015-01-05',  0.0014],\n     [1,'2015-01-12' , '2015-01-12',  0.0026],\n     [1,'2015-01-13' , '2015-01-12',  0.0086],\n     [1,'2015-01-14' , '2015-01-12',  0.0046],\n     [1,'2015-01-15' , '2015-01-12',  0.0021],\n     [2,'2015-01-16' , '2015-01-12',  0.0042],\n     [2,'2015-01-17' , '2015-01-12',  0.0099]]\n\ndf=spark.createDataFrame(list,['id','calendarday','last_monday','indexCP'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+-----------+-------+\n id|calendarday|last_monday|indexCP|\n+---+-----------+-----------+-------+\n  1| 2015-01-05| 2015-01-05| 0.0076|\n  1| 2015-01-06| 2015-01-05| 0.0026|\n  1| 2015-01-07| 2015-01-05| 0.0016|\n  1| 2015-01-08| 2015-01-05| 6.0E-4|\n  2| 2015-01-09| 2015-01-05| 0.0012|\n  2| 2015-01-10| 2015-01-05| 0.0014|\n  1| 2015-01-12| 2015-01-12| 0.0026|\n  1| 2015-01-13| 2015-01-12| 0.0086|\n  1| 2015-01-14| 2015-01-12| 0.0046|\n  1| 2015-01-15| 2015-01-12| 0.0021|\n  2| 2015-01-16| 2015-01-12| 0.0042|\n  2| 2015-01-17| 2015-01-12| 0.0099|\n+---+-----------+-----------+-------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["w=Window().partitionBy(\"last_monday\")\\\n          .orderBy(F.to_date(\"calendarday\",\"yyyy-MM-dd\"))\\\n          .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\ndf.withColumn(\"PreviousYearUnique\", F.first(\"indexCP\").over(w)).show()\n\n#+---+-----------+-----------+-------+------------------+\n#| id|calendarday|last_monday|indexCP|PreviousYearUnique|\n#+---+-----------+-----------+-------+------------------+\n#|  1| 2015-01-05| 2015-01-05| 0.0076|            0.0076|\n#|  1| 2015-01-06| 2015-01-05| 0.0026|            0.0076|\n#|  1| 2015-01-07| 2015-01-05| 0.0016|            0.0076|\n#|  1| 2015-01-08| 2015-01-05| 6.0E-4|            0.0076|\n#|  2| 2015-01-09| 2015-01-05| 0.0012|            0.0076|\n#|  2| 2015-01-10| 2015-01-05| 0.0014|            0.0076|\n#|  1| 2015-01-12| 2015-01-12| 0.0026|            0.0026|\n#|  1| 2015-01-13| 2015-01-12| 0.0086|            0.0026|\n#|  1| 2015-01-14| 2015-01-12| 0.0046|            0.0026|\n#|  1| 2015-01-15| 2015-01-12| 0.0021|            0.0026|\n#|  2| 2015-01-16| 2015-01-12| 0.0042|            0.0026|\n#|  2| 2015-01-17| 2015-01-12| 0.0099|            0.0026|\n#+---+-----------+-----------+-------+------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+-----------+-------+------------------+\n id|calendarday|last_monday|indexCP|PreviousYearUnique|\n+---+-----------+-----------+-------+------------------+\n  1| 2015-01-05| 2015-01-05| 0.0076|            0.0076|\n  1| 2015-01-06| 2015-01-05| 0.0026|            0.0076|\n  1| 2015-01-07| 2015-01-05| 0.0016|            0.0076|\n  1| 2015-01-08| 2015-01-05| 6.0E-4|            0.0076|\n  2| 2015-01-09| 2015-01-05| 0.0012|            0.0076|\n  2| 2015-01-10| 2015-01-05| 0.0014|            0.0076|\n  1| 2015-01-12| 2015-01-12| 0.0026|            0.0026|\n  1| 2015-01-13| 2015-01-12| 0.0086|            0.0026|\n  1| 2015-01-14| 2015-01-12| 0.0046|            0.0026|\n  1| 2015-01-15| 2015-01-12| 0.0021|            0.0026|\n  2| 2015-01-16| 2015-01-12| 0.0042|            0.0026|\n  2| 2015-01-17| 2015-01-12| 0.0099|            0.0026|\n+---+-----------+-----------+-------+------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [[1, 'foo'],\n     [1, 'bar'], \n     [1, 'foo'], \n     [1, 'foo'], \n     [2, 'bar'], \n     [2, 'foo'], \n     [2, 'bar'], \n     [2, 'foo'],\n     [3,'foo']],\n    ['session_id', 'event'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----+\nsession_id|event|\n+----------+-----+\n         1|  foo|\n         1|  bar|\n         1|  foo|\n         1|  foo|\n         2|  bar|\n         2|  foo|\n         2|  bar|\n         2|  foo|\n         3|  foo|\n+----------+-----+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df = spark.createDataFrame(\n        [[1, '2020-01-01 12:30:00.000', 'foo'], [1, '2020-01-01 12:31:00.000', 'bar'], [1, '2020-01-01 12:32:00.000', 'foo'],\n    [1, '2020-01-01 12:33:00.000', 'foo'], [2, '2020-01-01 13:00:00.000', 'bar'], [2, '2020-01-01 13:01:00.000', 'foo'],\n    [2, '2020-01-01 13:02:00.000', 'bar'], [2, '2020-01-01 13:03:00.000', 'foo']],\n        ['session_id', 'timestamp', 'event']\n    )\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----------------------+-----+\nsession_id|timestamp              |event|\n+----------+-----------------------+-----+\n1         |2020-01-01 12:30:00.000|foo  |\n1         |2020-01-01 12:31:00.000|bar  |\n1         |2020-01-01 12:32:00.000|foo  |\n1         |2020-01-01 12:33:00.000|foo  |\n2         |2020-01-01 13:00:00.000|bar  |\n2         |2020-01-01 13:01:00.000|foo  |\n2         |2020-01-01 13:02:00.000|bar  |\n2         |2020-01-01 13:03:00.000|foo  |\n+----------+-----------------------+-----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"timestamp\")\nw2=Window().partitionBy(\"session_id\").orderBy(\"timestamp\")\nw3=Window().partitionBy(\"session_id\")\ndf.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\", 'yyyy-MM-dd HH:mm:ss.SSS'))\\\n  .withColumn(\"session_id\", F.sum(F.when((F.col(\"event\")=='bar'),F.lit(1))\\\n                                         .otherwise(F.lit(0))).over(w))\\\n  .withColumn(\"rowNum\", F.row_number().over(w2))\\\n  .withColumn(\"max\", F.max(\"rowNum\").over(w3))\\\n  .withColumn(\"first\", F.when((F.col(\"rowNum\")==1)&(F.col(\"event\")=='foo'), F.lit(1))\\\n                       .otherwise(F.lit(0)))\\\n  .filter('max>=2 and first=0').drop(*['rowNum','sample_timestamp','max','first']).show()\n\n#+----------+-------------------+-----+\n#|session_id|          timestamp|event|\n#+----------+-------------------+-----+\n#|         1|2020-01-01 12:31:00|  bar|\n#|         1|2020-01-01 12:32:00|  foo|\n#|         1|2020-01-01 12:33:00|  foo|\n#|         2|2020-01-01 13:00:00|  bar|\n#|         2|2020-01-01 13:01:00|  foo|\n#|         3|2020-01-01 13:02:00|  bar|\n#|         3|2020-01-01 13:03:00|  foo|\n#+----------+-------------------+-----+\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------------+-----+\nsession_id|          timestamp|event|\n+----------+-------------------+-----+\n         1|2020-01-01 12:31:00|  bar|\n         1|2020-01-01 12:32:00|  foo|\n         1|2020-01-01 12:33:00|  foo|\n         2|2020-01-01 13:00:00|  bar|\n         2|2020-01-01 13:01:00|  foo|\n         3|2020-01-01 13:02:00|  bar|\n         3|2020-01-01 13:03:00|  foo|\n+----------+-------------------+-----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["+----------+-----+\n|session_id|event|\n+----------+-----+\n|         1|  bar|\n|         1|  foo|\n|         1|  foo|\n|         2|  bar|\n|         2|  foo|\n|         3|  bar|\n|         3|  foo|\n+----------+-----+"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"stackhelp57","notebookId":3914354172321346},"nbformat":4,"nbformat_minor":0}
