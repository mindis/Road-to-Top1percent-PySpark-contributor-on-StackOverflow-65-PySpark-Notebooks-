{"cells":[{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nlist=[[['3927'],'Hey'],\n      [['3410'],'Yo'],\n      [['3927'],'Why'],\n      [['2227','3410'],'Am'],\n      [['3410','3927'],'Here']]\ncSchema = StructType([StructField(\"Code\", ArrayType(StringType())),StructField(\"Document_title\", StringType())])\ndf= spark.createDataFrame(list,schema=cSchema)\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------------+\n        Code|Document_title|\n+------------+--------------+\n      [3927]|           Hey|\n      [3410]|            Yo|\n      [3927]|           Why|\n[2227, 3410]|            Am|\n[3410, 3927]|          Here|\n+------------+--------------+\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["df1=df.withColumn(\"Code\",F.explode(F.col(\"Code\")))\ndf1.groupBy(F.col(\"Code\"))\\\n   .agg(F.collect_list(\"Document_title\"))\\\n   .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------------------------+\nCode|collect_list(Document_title)|\n+----+----------------------------+\n3927|            [Hey, Why, Here]|\n3410|              [Yo, Am, Here]|\n2227|                        [Am]|\n+----+----------------------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.{avg,col,lit,when}\nval df =List((\"ABC\", 10),\n              (\"ABC\", 20),\n              (\"ABC\", 30),\n              (\"ABC\", 40),\n              (\"ABC\", 50),\n              (\"ABC\", 60),\n              (\"ABC\", 70),\n              (\"ABC\", 80),\n              (\"ABC\", 90),\n              (\"ABC\", 100)\n            ).\n          toDF(\"name\", \"value\")\n\nval window = Window.partitionBy($\"name\").orderBy($\"value\").rowsBetween(-2, -1)\nval df2 = df.withColumn(\"rolling_average\", avg($\"value\") over(window)).withColumn(\"rolling_average\", when(col(\"rolling_average\")===10, lit(null)).otherwise(col(\"rolling_average\")))\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+---------------+\nname|value|rolling_average|\n+----+-----+---------------+\n ABC|   10|           null|\n ABC|   20|           null|\n ABC|   30|           15.0|\n ABC|   40|           25.0|\n ABC|   50|           35.0|\n ABC|   60|           45.0|\n ABC|   70|           55.0|\n ABC|   80|           65.0|\n ABC|   90|           75.0|\n ABC|  100|           85.0|\n+----+-----+---------------+\n\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.{avg, col, lit, when}\ndf: org.apache.spark.sql.DataFrame = [name: string, value: int]\nwindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@66c7eea0\ndf2: org.apache.spark.sql.DataFrame = [name: string, value: int ... 1 more field]\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nlist=[['a',[[95.0], [25.0, 25.0], [40.0]]],\n      ['a',[[95.0], [20.0, 80.0]]],\n      ['a',[[95.0], [25.0, 75.0]]],\n      ['b',[[95.0], [25.0, 75.0]]],\n      ['b',[[95.0], [12.0, 88.0]]]]\n      \ncSchema = StructType([StructField(\"A\", StringType())\\\n                      ,StructField(\"B\", ArrayType(ArrayType(FloatType())))])\ndf= spark.createDataFrame(list,schema=cSchema)\n\ndf.printSchema()\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- A: string (nullable = true)\n-- B: array (nullable = true)\n    |-- element: array (containsNull = true)\n    |    |-- element: float (containsNull = true)\n\n+---+--------------------+\n  A|                   B|\n+---+--------------------+\n  a|[[95.0], [25.0, 2...|\n  a|[[95.0], [20.0, 8...|\n  a|[[95.0], [25.0, 7...|\n  b|[[95.0], [25.0, 7...|\n  b|[[95.0], [12.0, 8...|\n+---+--------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["def remove_highest(col):\n    return (np.sort( np.asarray([item for sublist in col for item in sublist])  )[:-1]).tolist()\n\nudf_remove_highest = F.udf( remove_highest , ArrayType(FloatType()) )\ndf.withColumn(\"B\",udf_remove_highest(\"B\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------------------+\n  A|                 B|\n+---+------------------+\n  a|[25.0, 25.0, 40.0]|\n  a|      [20.0, 80.0]|\n  a|      [25.0, 75.0]|\n  b|      [25.0, 75.0]|\n  b|      [12.0, 88.0]|\n+---+------------------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nexpression=\"\"\"filter(B, x -> x != C )\"\"\"\ndf1=df.withColumn(\"B\",(F.sort_array(F.flatten(\"B\")))).withColumn(\"C\",F.array_max(\"B\")).withColumn(\"B\", F.expr(expression) )\\\n.drop(\"C\")\ndf1.show()\n\n\n  \n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------------------+\n  A|                 B|\n+---+------------------+\n  a|[25.0, 25.0, 40.0]|\n  a|      [20.0, 80.0]|\n  a|      [25.0, 75.0]|\n  b|      [25.0, 75.0]|\n  b|      [12.0, 88.0]|\n+---+------------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["arr=[25.0,25.0,40.0,95.0]\narr[:-1]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[130]: [25.0, 25.0, 40.0]</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df1.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- A: string (nullable = true)\n-- B: array (nullable = true)\n    |-- element: float (containsNull = true)\n-- Max: struct (nullable = false)\n    |-- col1: float (nullable = true)\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df1=df.withColumn(\"B\",(F.sort_array(F.flatten(\"B\")))).withColumn(\"Max\", F.array(F.array_max(\"B\")))\ndf1.withColumn(\"B\",F.array_except(\"B\",\"Max\")).drop(\"Max\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------------+\n  A|           B|\n+---+------------+\n  a|[25.0, 40.0]|\n  a|[20.0, 80.0]|\n  a|[25.0, 75.0]|\n  b|[25.0, 75.0]|\n  b|[12.0, 88.0]|\n+---+------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df1.withColumn(\"B\", F.array_remove(\"B\", F.col(\"Max\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3543595061519617&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;B&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>array_remove<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;B&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Max&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">array_remove</span><span class=\"ansi-blue-fg\">(col, element)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2012</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2013</span>     sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context\n<span class=\"ansi-green-fg\">-&gt; 2014</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>array_remove<span class=\"ansi-blue-fg\">(</span>_to_java_column<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> element<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2015</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2016</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1246</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1247</span>     <span class=\"ansi-green-fg\">def</span> __call__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1248</span><span class=\"ansi-red-fg\">         </span>args_command<span class=\"ansi-blue-fg\">,</span> temp_args <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_build_args<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1249</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1250</span>         command <span class=\"ansi-blue-fg\">=</span> proto<span class=\"ansi-blue-fg\">.</span>CALL_COMMAND_NAME <span class=\"ansi-blue-fg\">+</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_build_args</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1210</span>     <span class=\"ansi-green-fg\">def</span> _build_args<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1211</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>converters <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> len<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1212</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-blue-fg\">(</span>new_args<span class=\"ansi-blue-fg\">,</span> temp_args<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_get_args<span class=\"ansi-blue-fg\">(</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1213</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1214</span>             new_args <span class=\"ansi-blue-fg\">=</span> args\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_get_args</span><span class=\"ansi-blue-fg\">(self, args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1197</span>                 <span class=\"ansi-green-fg\">for</span> converter <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>                     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">.</span>can_convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1199</span><span class=\"ansi-red-fg\">                         </span>temp_arg <span class=\"ansi-blue-fg\">=</span> converter<span class=\"ansi-blue-fg\">.</span>convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1200</span>                         temp_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>                         new_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</span> in <span class=\"ansi-cyan-fg\">convert</span><span class=\"ansi-blue-fg\">(self, object, gateway_client)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    498</span>         ArrayList <span class=\"ansi-blue-fg\">=</span> JavaClass<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;java.util.ArrayList&#34;</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    499</span>         java_list <span class=\"ansi-blue-fg\">=</span> ArrayList<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 500</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">for</span> element <span class=\"ansi-green-fg\">in</span> object<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    501</span>             java_list<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">(</span>element<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    502</span>         <span class=\"ansi-green-fg\">return</span> java_list\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    342</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    343</span>     <span class=\"ansi-green-fg\">def</span> __iter__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 344</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Column is not iterable&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    345</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    346</span>     <span class=\"ansi-red-fg\"># string methods</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: Column is not iterable</div>"]}}],"execution_count":10},{"cell_type":"code","source":["array(struct(lit(None).cast(\"string\").alias(\"value\"), lit(0).alias(\"amount\")))"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"stackhelp-6","notebookId":2147617582185568},"nbformat":4,"nbformat_minor":0}
