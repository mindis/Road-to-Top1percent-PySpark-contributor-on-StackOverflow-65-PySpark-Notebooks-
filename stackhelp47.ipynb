{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list1=[[1,\"\"\"{\"elements\": [{{\"sev\": {\"imp\":\"M\",\"TIME\":\"20\",\"min_lim\":\"10\"}},{\"sev\": {\"imp\":\"H\",\"TIME\":\"10\",\"min_lim\":\"5\",\"max_lim\":\"10\"}},{\"sev\": {\"imp\":\"C\",\"TIME\":\"5\",\"min_lim\":\"1\",\"max_lim\":\"4\"}}}]}\"\"\"]]\n\n#list2=[[1,[{\"elements\": [{{\"sev\": {\"imp\":\"M\",\"TIME\":\"20\",\"min_lim\":\"10\"}},{\"sev\": {\"imp\":\"H\",\"TIME\":\"10\",\"min_lim\":\"5\",\"max_lim\":\"10\"}},{\"sev\": {\"imp\":\"C\",\"TIME\":\"5\",\"min_lim\":\"1\",\"max_lim\":\"4\"}}}]}]]]\n\ndf=spark.createDataFrame(list1,['ID','data'])\n#df1=spark.createDataFrame(list2,['ID','data'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------------------+\n ID|                data|\n+---+--------------------+\n  1|{&#34;elements&#34;: [{{&#34;...|\n+---+--------------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.select(F.schema_of_json('{\"elements\": [{{\"sev\": {\"imp\" : \"M\", \"TIME\" : \"20\",\"min_lim\" : \"10\"}}}]}')).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2112672745328028&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>schema_of_json<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;{&#34;elements&#34;: [{{&#34;sev&#34;: {&#34;imp&#34; : &#34;M&#34;, &#34;TIME&#34; : &#34;20&#34;,&#34;min_lim&#34; : &#34;10&#34;}}}]}&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    550</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-DF-ACL clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    551</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 552</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>collectToPython<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    553</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> BatchedSerializer<span class=\"ansi-blue-fg\">(</span>PickleSerializer<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    554</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1870.collectToPython.\n: com.fasterxml.jackson.core.JsonParseException: Unexpected character (&#39;{&#39; (code 123)): was expecting double-quote to start field name\n at [Source: java.io.InputStreamReader@5ff706b1; line: 1, column: 17]\n\tat com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1581)\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:533)\n\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:462)\n\tat com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddName(ReaderBasedJsonParser.java:1502)\n\tat com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:624)\n\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:29)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:134)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:152)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema$.inferField(JsonInferSchema.scala:137)\n\tat org.apache.spark.sql.catalyst.expressions.SchemaOfJson$$anonfun$9.apply(jsonExpressions.scala:764)\n\tat org.apache.spark.sql.catalyst.expressions.SchemaOfJson$$anonfun$9.apply(jsonExpressions.scala:762)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2787)\n\tat org.apache.spark.sql.catalyst.expressions.SchemaOfJson.eval(jsonExpressions.scala:762)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:52)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:284)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsDown$1.apply(QueryPlan.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:84)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:45)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1.applyOrElse(expressions.scala:44)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.transformDown(AnalysisHelper.scala:149)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:268)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:44)\n\tat org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.apply(expressions.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$optimizedPlan$1.apply(QueryExecution.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$optimizedPlan$1.apply(QueryExecution.scala:97)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:96)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$2.apply(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$toString$2.apply(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.stringOrError(QueryExecution.scala:133)\n\tat org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:104)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:242)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:172)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3357)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["list=[[[1,1,1,1,1,2,2,1,1,1,1,1,1],[1,2,3,4,5,6,7,8,9,10,11,12,13]]]\n\n\ndf=spark.createDataFrame(list,['value','day'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+\n               value|                 day|\n+--------------------+--------------------+\n[1, 1, 1, 1, 1, 2...|[1, 2, 3, 4, 5, 6...|\n+--------------------+--------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df=df.withColumn(\"zipped\", F.explode(F.arrays_zip(\"value\",\"day\"))).select(\"zipped.*\")\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+\nvalue|day|\n+-----+---+\n    1|  1|\n    1|  2|\n    1|  3|\n    1|  4|\n    1|  5|\n    2|  6|\n    2|  7|\n    1|  8|\n    1|  9|\n    1| 10|\n    1| 11|\n    1| 12|\n    1| 13|\n+-----+---+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["w1=Window().orderBy(\"day\")\ndf.withColumn(\"lag\", F.when(F.lag(\"value\").over(w1)!=F.col(\"value\"), F.lit(1)).otherwise(F.lit(0)))\\\n  .withColumn(\"group\", F.sum(\"lag\").over(w1) + 1).drop(\"lag\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+-----+\nvalue|day|group|\n+-----+---+-----+\n    1|  1|    1|\n    1|  2|    1|\n    1|  3|    1|\n    1|  4|    1|\n    1|  5|    1|\n    2|  6|    2|\n    2|  7|    2|\n    1|  8|    3|\n    1|  9|    3|\n    1| 10|    3|\n    1| 11|    3|\n    1| 12|    3|\n    1| 13|    3|\n+-----+---+-----+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["list=[['Apple','A'],\n      ['Google','G'],\n      ['Facebook','F']]\n\ndf=spark.createDataFrame(list,['col1','col2'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----+\n    col1|col2|\n+--------+----+\n   Apple|   A|\n  Google|   G|\nFacebook|   F|\n+--------+----+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nw=Window().orderBy(\"col3\")\narr=[\"SFO\",\"LA\",\"NYC\"]\n\ndf.withColumn(\"col3\", F.array(*[F.lit(x) for x in arr]))\\\n  .withColumn(\"rownum\", F.row_number().over(w))\\\n  .withColumn(\"col3\", F.expr(\"\"\"element_at(col3,rownum)\"\"\")).drop(\"rownum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----+----+\n    col1|col2|col3|\n+--------+----+----+\n   Apple|   A| SFO|\n  Google|   G|  LA|\nFacebook|   F| NYC|\n+--------+----+----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["list=[[[{\"service_id\": \"S121\", \"price\": 1256}, {\"service_id\": \"S022\", \"price\": 1149}],'a01','2020-01-17'],\n       [[{\"service_id\": \"S121\", \"price\": 1256}, {\"service_id\": \"S022\", \"price\": 1149}],'a01','2020-01-17']]\n\ndf=spark.createDataFrame(list,['contents','account_id','date'])\ndf.show(truncate=False) \ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------+----------+----------+\ncontents                                                                  |account_id|date      |\n+--------------------------------------------------------------------------+----------+----------+\n[[price -&gt; 1256, service_id -&gt; S121], [price -&gt; 1149, service_id -&gt; S022]]|a01       |2020-01-17|\n[[price -&gt; 1256, service_id -&gt; S121], [price -&gt; 1149, service_id -&gt; S022]]|a01       |2020-01-17|\n+--------------------------------------------------------------------------+----------+----------+\n\nroot\n-- contents: array (nullable = true)\n    |-- element: map (containsNull = true)\n    |    |-- key: string\n    |    |-- value: string (valueContainsNull = true)\n-- account_id: string (nullable = true)\n-- date: string (nullable = true)\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.withColumn(\"keys\",F.explode(F.expr(\"\"\"transform(contents, x->map_values(x))\"\"\")))\\\n  .withColumn(\"price\", F.col(\"keys\")[0])\\\n  .withColumn(\"service_id\", F.col(\"keys\")[1]).drop(\"keys\",\"contents\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+-----+----------+\naccount_id|date      |price|service_id|\n+----------+----------+-----+----------+\na01       |2020-01-17|1256 |S121      |\na01       |2020-01-17|1149 |S022      |\na01       |2020-01-17|1256 |S121      |\na01       |2020-01-17|1149 |S022      |\n+----------+----------+-----+----------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["list=[[1,1,'ABC','ABC'],\n      [2,1,'ABC','ABC'],\n      [3,6,'ABCDE','ABCDE'],\n      [4,3,'ABCDE','ABC'],\n      [5,7,'ABCDE','ABC']]\n\ndf=spark.createDataFrame(list,['TranID','RevID','CheckingCol','CheckingCol2'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----+-----------+------------+\nTranID|RevID|CheckingCol|CheckingCol2|\n+------+-----+-----------+------------+\n     1|    1|        ABC|         ABC|\n     2|    1|        ABC|         ABC|\n     3|    6|      ABCDE|       ABCDE|\n     4|    3|      ABCDE|         ABC|\n     5|    7|      ABCDE|         ABC|\n+------+-----+-----------+------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf.withColumn(\"TotalMatch\", when((col(\"RevID\").contains(col(\"TranID\"))) & (col(\"CheckingCol\") == col(\"CheckingCol2\")), lit(\"Yes\")).otherwise(lit(\"No\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----+-----------+------------+----------+\nTranID|RevID|CheckingCol|CheckingCol2|TotalMatch|\n+------+-----+-----------+------------+----------+\n     1|    1|        ABC|         ABC|       Yes|\n     2|    1|        ABC|         ABC|        No|\n     3|    6|      ABCDE|       ABCDE|        No|\n     4|    3|      ABCDE|         ABC|        No|\n     5|    7|      ABCDE|         ABC|        No|\n+------+-----+-----------+------------+----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["list=[[1,40],\n     [1,30],\n     [2,10],\n     [2,90],\n     [3,20],\n     [3,10],\n     [4,2],\n     [4,5]]\n\ndf=spark.createDataFrame(list,['id','value'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+\n id|value|\n+---+-----+\n  1|   40|\n  1|   30|\n  2|   10|\n  2|   90|\n  3|   20|\n  3|   10|\n  4|    2|\n  4|    5|\n+---+-----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["w1=Window().partitionBy(\"id\")\nw=Window().partitionBy()\ndf.withColumn(\"grouped_total\",F.sum(\"value\").over(w1))\\\n  .withColumn(\"anti_grouped_total\", (F.sum(\"value\").over(w))-F.col(\"grouped_total\"))\\\n  .groupBy(\"id\").agg(F.first(\"grouped_total\").alias(\"grouped_total\"),\\\n                     F.first(\"anti_grouped_total\").alias(\"anti_grouped_total\"))\\\n  .drop(\"value\").orderBy(\"id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------+------------------+\n id|grouped_total|anti_grouped_total|\n+---+-------------+------------------+\n  1|           70|               137|\n  2|          100|               107|\n  3|           30|               177|\n  4|            7|               200|\n+---+-------------+------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql import functions as F\nw = Window().partitionBy()\ndf.groupBy(\"id\").agg(F.sum(\"value\").alias(\"grouped_total\"))\\\n          .withColumn(\"anti_grouped_total\",F.sum(\"grouped_total\").over(w)-F.col(\"grouped_total\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------+------------------+\n id|grouped_total|anti_grouped_total|\n+---+-------------+------------------+\n  1|           70|               137|\n  3|           30|               177|\n  2|          100|               107|\n  4|            7|               200|\n+---+-------------+------------------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["+---+-------------+------------------+\n| id|grouped_total|anti_grouped_total|\n+---+-------------+------------------+\n|  1|           70|               137|\n|  2|          100|               107|\n|  3|           30|               177|\n|  4|            7|               200|\n+---+-------------+------------------+"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"anti_id_1\", F.when(F.col(\"id\")==1, F.lit('1')).otherwise(F.lit('Not_1')))\\\n  .groupBy(\"anti_id_1\").agg(F.sum(\"value\").alias(\"sum\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---+\nanti_id_1|sum|\n+---------+---+\n        1| 70|\n    Not_1|137|\n+---------+---+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["list=[['pen',10],\n      ['book',40],\n      ['bottle',80],\n      ['glass',55]]\n\ndf=spark.createDataFrame(list,['product','cost'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----+\nproduct|cost|\n+-------+----+\n    pen|  10|\n   book|  40|\n bottle|  80|\n  glass|  55|\n+-------+----+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.withColumn(\"JSON\", F.to_json(F.create_map(F.lit(\"product\"),F.col(\"product\"),F.lit(\"cost\"),F.col(\"cost\")))).drop(\"product\",\"cost\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+\nJSON                            |\n+--------------------------------+\n{&#34;product&#34;:&#34;pen&#34;,&#34;cost&#34;:&#34;10&#34;}   |\n{&#34;product&#34;:&#34;book&#34;,&#34;cost&#34;:&#34;40&#34;}  |\n{&#34;product&#34;:&#34;bottle&#34;,&#34;cost&#34;:&#34;80&#34;}|\n{&#34;product&#34;:&#34;glass&#34;,&#34;cost&#34;:&#34;55&#34;} |\n+--------------------------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["list=[[1582749601000]]\n\ndf=spark.createDataFrame(list,['millis'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+\n       millis|\n+-------------+\n1582749601000|\n+-------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.withColumn(\"as_date\",F.from_unixtime(F.col(\"millis\")/1000)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-------------------+\n       millis|            as_date|\n+-------------+-------------------+\n1582749601000|2020-02-26 20:40:01|\n+-------------+-------------------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.withColumn('as_date', F.from_unixtime((F.col('millis')/1000))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-------------------+\n       millis|            as_date|\n+-------------+-------------------+\n1582749601000|2020-02-26 20:40:01|\n+-------------+-------------------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["list1=[['test1','20','ls',10],\n      ['test2', '','baz',15],\n      ['test3', '','az',19]]\n\ndf1=spark.createDataFrame(list1,['Name','Age','Address','Id'])\n\nlist2=[['test4','20','bas',10],\n       ['test5','','baz',25],\n       ['test6','40','az',19]]\n\ndf2=spark.createDataFrame(list2,['Name','Age','Address','Id'])\n\ndf1.show()\n\ndf2.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+-------+---+\n Name|Age|Address| Id|\n+-----+---+-------+---+\ntest1| 20|     ls| 10|\ntest2|   |    baz| 15|\ntest3|   |     az| 19|\n+-----+---+-------+---+\n\n+-----+---+-------+---+\n Name|Age|Address| Id|\n+-----+---+-------+---+\ntest4| 20|    bas| 10|\ntest5|   |    baz| 25|\ntest6| 40|     az| 19|\n+-----+---+-------+---+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["list1=[[[1, 2, 3, 4, 5, 6],[3.42102562348192E-6, 4.2159323917750995E-6, 3.924587540944015E-6, 4.167182871752131E-6, 4.109192066532302E-6, 4.297804458327455E-6]],\n[[1, 2, 3, 4, 5, 6],[1.384402399630826E-5, 9.913141993957704E-6, 1.1145077060247102E-5, 1.1005472165326649E-5, 1.1004462921073546E-5, 1.1004462921073546E-5]]]\ndf=spark.createDataFrame(list1,['x','y'])\n\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>x</th><th>y</th></tr></thead><tbody><tr><td>List(1, 2, 3, 4, 5, 6)</td><td>List(3.42102562348192E-6, 4.2159323917750995E-6, 3.924587540944015E-6, 4.167182871752131E-6, 4.109192066532302E-6, 4.297804458327455E-6)</td></tr><tr><td>List(1, 2, 3, 4, 5, 6)</td><td>List(1.384402399630826E-5, 9.913141993957704E-6, 1.1145077060247102E-5, 1.1005472165326649E-5, 1.1004462921073546E-5, 1.1004462921073546E-5)</td></tr></tbody></table></div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.type import *\nimport numpy as np\ndef get_slope_func(x,y,order=1):\n    coeffs = np.polyfit(x, y, order)\n    slope = coeffs[-2]\n    return float(slope)\nget_slope = F.udf(get_slope_func, returnType=DoubleType())\n\ndf.select(get_slope(F.col(\"x\"), F.col(\"y\")).alias(\"slope\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\nslope|\n+-----+\nnull |\nnull |\n+-----+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["╔═══════╦═════╦═════════╦════╗\n║ Name  ║ Age ║ Address ║ Id ║\n╠═══════╬═════╬═════════╬════╣\n║ test1 ║ 20  ║ ls      ║ 10 ║\n╠═══════╬═════╬═════════╬════╣\n║ test2 ║ 40  ║  az     ║ 19 ║\n╚═══════╩═════╩═════════╩════╝"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["df = spark.createDataFrame([(1,10,\"a\"),(3,2,\"a\"),(1,2,\"b\"),(2,5,\"a\"),(2,1,\"b\"),(9,0,\"b\"),(4,1,\"b\"),(7,8,\"a\"),(3,8,\"b\"),(2,5,\"a\"),(0,0,\"a\"),(4,3,\"a\")],[\"time\", \"value\", \"class\"] )\ndf.orderBy(\"class\",\"time\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+\ntime|value|class|\n+----+-----+-----+\n   0|    0|    a|\n   1|   10|    a|\n   2|    5|    a|\n   2|    5|    a|\n   3|    2|    a|\n   4|    3|    a|\n   7|    8|    a|\n   1|    2|    b|\n   2|    1|    b|\n   3|    8|    b|\n   4|    1|    b|\n   9|    0|    b|\n+----+-----+-----+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["w1=Window().partitionBy(\"class\").orderBy(\"time\",\"value\")\nw2=Window().partitionBy(\"class\").orderBy('rownum')\ndf.withColumn('rownum', F.row_number().over(w1))\\\n  .withColumn('cumsum_value', F.sum(\"value\").over(w2)).drop('rownum').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+-----+------------+\ntime|value|class|cumsum_value|\n+----+-----+-----+------------+\n   1|    2|    b|           2|\n   2|    1|    b|           3|\n   3|    8|    b|          11|\n   4|    1|    b|          12|\n   9|    0|    b|          12|\n   0|    0|    a|           0|\n   1|   10|    a|          10|\n   2|    5|    a|          15|\n   2|    5|    a|          20|\n   3|    2|    a|          22|\n   4|    3|    a|          25|\n   7|    8|    a|          33|\n+----+-----+-----+------------+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["list1=[[3,'7/1/2019','Current'],\n       [1,'6/9/2019','Expired'],\n       [1,'1/1/2019','Current']]\n\ndata=spark.createDataFrame(list1,['USER','DATE','USER_STATE'])\n\nlist2=[[1,'7/1/2018',10.00],\n       [1,'5/1/2019',40.00],\n       [1,'2/2/2019',10.00],\n       [3,'1/2/2019',15.00]]\n\naggregation=spark.createDataFrame(list2,['USER','CHARGEDATE','AMOUNTPAID'])\n\ndata.show()\naggregation.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+----------+\nUSER|    DATE|USER_STATE|\n+----+--------+----------+\n   3|7/1/2019|   Current|\n   1|6/9/2019|   Expired|\n   1|1/1/2019|   Current|\n+----+--------+----------+\n\n+----+----------+----------+\nUSER|CHARGEDATE|AMOUNTPAID|\n+----+----------+----------+\n   1|  7/1/2018|      10.0|\n   1|  5/1/2019|      40.0|\n   1|  2/2/2019|      10.0|\n   3|  1/2/2019|      15.0|\n+----+----------+----------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["data.join(aggregation,['USER'])\\\n      .withColumn(\"DATE\",F.to_date(\"DATE\",\"M/d/yyyy\"))\\\n      .withColumn(\"CHARGEDATE\", F.to_date(\"CHARGEDATE\", \"M/d/yyyy\"))\\\n      .filter(\"DATE>CHARGEDATE\")\\\n      .groupBy(\"USER\",\"DATE\",\"USER_STATE\").agg(F.mean(\"AMOUNTPAID\").alias(\"mean_amount_paid\"))\\\n      .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------+----------+----------------+\nUSER|      DATE|USER_STATE|mean_amount_paid|\n+----+----------+----------+----------------+\n   1|2019-06-09|   Expired|            20.0|\n   1|2019-01-01|   Current|            10.0|\n   3|2019-07-01|   Current|            15.0|\n+----+----------+----------+----------------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["table1.createOrReplaceTempView(\"data\")\ntable2.createOrReplaceTempView(\"aggregation\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["spark.sql(\"\"\"select d.user, date, user_state, avg(amountpaid) as mean_amount_paid from data d join aggregation a on d.user = a.user where d.date > a.chargedate group by d.user, date, user_state\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+--------+----------+----------------+\nuser|    date|user_state|mean_amount_paid|\n+----+--------+----------+----------------+\n   1|6/9/2019|   Expired|            25.0|\n   3|7/1/2019|   Current|            15.0|\n+----+--------+----------+----------------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["+---+---------- +----------+---------------          \n|USER|   DATE   |USER_STATE|MEAN_AMOUNTPAID|   \n+---+---------- +----------+--------------- \n|  3 | 7/1/2019 |  Current |    15.00      |\n|  1 | 6/9/2019 |  Expired |    20.00      | \n|  1 | 1/1/2019 |  Current |    10.00      |\n+----+----------+----------+--------------- "],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"stackhelp47","notebookId":3270316468668464},"nbformat":4,"nbformat_minor":0}
