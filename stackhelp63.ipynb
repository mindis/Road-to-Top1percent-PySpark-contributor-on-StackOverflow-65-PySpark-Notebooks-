{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[  0,   'a',        1],\n      [1,   'a',        1],\n      [2,   'a',        1],\n      [3,   'a',        1]]\n\ndf=spark.createDataFrame(list,['day','user','raw_score'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+---------+\nday|user|raw_score|\n+---+----+---------+\n  0|   a|        1|\n  1|   a|        1|\n  2|   a|        1|\n  3|   a|        1|\n+---+----+---------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["list=[[  0,   'a',        4],\n      [1,   'a',        1],\n      [2,   'a',        2],\n      [3,   'a',        0]]\n\ndf=spark.createDataFrame(list,['day','user','raw_score'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+---------+\nday|user|raw_score|\n+---+----+---------+\n  0|   a|        4|\n  1|   a|        1|\n  2|   a|        2|\n  3|   a|        0|\n+---+----+---------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n   .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"raw_score\",\"score\")))\\\n   .select(\"user\", \"zip.*\")\\\n   .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+-----+\nuser|day|raw_score|score|\n+----+---+---------+-----+\na   |0  |4        |4.0  |\na   |1  |1        |4.6  |\na   |2  |2        |6.14 |\na   |3  |0        |5.526|\n+----+---+---------+-----+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["w=Window().partitionBy(\"user\").orderBy(\"day\")\ndf.withColumn(\"raw_score\", F.when(F.row_number().over(w)==1, F.lit(0)).otherwise(F.col(\"raw_score\")))\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> array(x,i))\"\"\"))\\\n   .withColumn(\"score\", F.col(\"raw_score1\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(score,cast(0.9 as double)\\\n                                                            ,(acc,y)->IF(x[1]<=y[1],(acc*x[0])+x[0],0)))\"\"\"))\\\n   .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------+--------------------------------+--------------------+\nuser|raw_score   |raw_score1                      |score               |\n+----+------------+--------------------------------+--------------------+\na   |[0, 1, 1, 1]|[[0, 0], [1, 1], [1, 2], [1, 3]]|[0.0, 3.0, 2.0, 1.0]|\n+----+------------+--------------------------------+--------------------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df.show() #sample dataframe\n#+---+----+---------+\n#|day|user|raw_score|\n#+---+----+---------+\n#|  0|   a|        1|\n#|  1|   a|        1|\n#|  2|   a|        1|\n#|  3|   a|        1|\n#+---+----+---------+\n\n\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"todays_score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n   .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"raw_score\",\"todays_score\")))\\\n   .select(\"user\", \"zip.*\")\\\n   .show(truncate=False)\n\n\n#+----+---+---------+------------+\n#|user|day|raw_score|todays_score|\n#+----+---+---------+------------+\n#|a   |0  |1        |1.0         |\n#|a   |1  |1        |1.9         |\n#|a   |2  |1        |2.71        |\n#|a   |3  |1        |3.439       |\n#+----+---+---------+------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+---------+\nday|user|raw_score|\n+---+----+---------+\n  0|   a|        1|\n  1|   a|        1|\n  2|   a|        1|\n  3|   a|        1|\n+---+----+---------+\n\n+----+---+---------+------------+\nuser|day|raw_score|todays_score|\n+----+---+---------+------------+\na   |0  |1        |1.0         |\na   |1  |1        |1.9         |\na   |2  |1        |2.71        |\na   |3  |1        |3.439       |\n+----+---+---------+------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n   .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------+------------+--------------------------------+-----------------------+\nuser|raw_score   |day         |raw_score1                      |score                  |\n+----+------------+------------+--------------------------------+-----------------------+\na   |[1, 1, 1, 1]|[0, 1, 2, 3]|[[1, 0], [1, 1], [1, 2], [1, 3]]|[1.0, 1.9, 2.71, 3.439]|\n+----+------------+------------+--------------------------------+-----------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"raw_score2\", F.col(\"raw_score1\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"transform(raw_score2, x-> aggregate(raw_score1,cast(0 as double)\\\n                                                 ,(acc,y)->IF(x.index>y.index,0,((acc*0.9)+y.raw))))\"\"\"))\\\n   .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------+------------+--------------------------------+--------------------------------+-----------------------+\nuser|raw_score   |day         |raw_score1                      |raw_score2                      |score                  |\n+----+------------+------------+--------------------------------+--------------------------------+-----------------------+\na   |[1, 2, 3, 4]|[0, 1, 2, 3]|[[1, 0], [2, 1], [3, 2], [4, 3]]|[[1, 0], [2, 1], [3, 2], [4, 3]]|[9.049, 8.32, 6.7, 4.0]|\n+----+------------+------------+--------------------------------+--------------------------------+-----------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"sort_array(transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw)))\"\"\"))\\\n   .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"raw_score\",\"score\")))\\\n   .select(\"user\", \"zip.*\")\\\n   .show(truncate=False)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+-----------------+\nuser|day|raw_score|score            |\n+----+---+---------+-----------------+\na   |0  |1        |1.0              |\na   |1  |2        |2.9              |\na   |2  |3        |5.609999999999999|\na   |3  |4        |9.049            |\n+----+---+---------+-----------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"raw_score2\", F.col(\"raw_score1\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"transform(raw_score2, x-> aggregate(filter(raw_score1,z-> z[0]<=x\n   \n   \n   \n   ,cast(0 as double)\\\n                                                 ,(acc,y)->IF(x.index>y.index,0,((acc*0.9)+y.raw))))\"\"\"))\\\n   .show(truncate=False)\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n   .withColumn(\"raw_score2\", F.col(\"raw_score1\"))\\\n   .withColumn(\"score\", F.expr(\"\"\"filter(raw_score1,z-> z.index<=1)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------+------------+--------------------+--------------------+----------------+\nuser|   raw_score|         day|          raw_score1|          raw_score2|           score|\n+----+------------+------------+--------------------+--------------------+----------------+\n   a|[1, 2, 3, 4]|[0, 1, 2, 3]|[[1, 0], [2, 1], ...|[[1, 0], [2, 1], ...|[[1, 0], [2, 1]]|\n+----+------------+------------+--------------------+--------------------+----------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df.groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"))\\\n  .withColumn(\"score\", F.expr(\"\"\"transform(raw_score,(x,i)-> array(x,i))\"\"\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------+--------------------------------+\nuser|raw_score   |score                           |\n+----+------------+--------------------------------+\na   |[1, 1, 1, 1]|[[1, 0], [1, 1], [1, 2], [1, 3]]|\n+----+------------+--------------------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["+---+----+---------+------------+\n|day|user|raw_score|todays_score| # Here's the math:\n+---+----+---------+------------+\n|  0|   a|        1|         1.0| (0 * .90) + 1\n|  1|   a|        1|         1.9| (1.0 * .90) + 1\n|  2|   a|        1|        2.71| (1.9 * .90) + 1\n|  3|   a|        1|       3.439| (2.71 * .90) + 1\n+---+----+---------+------------+"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["list=[[2003],\n      [2004],\n      [2014],\n      [2015],\n      [2008],\n      [1995]]\n\ndf=spark.createDataFrame(list,['year'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\nyear|\n+----+\n2003|\n2004|\n2014|\n2015|\n2008|\n1995|\n+----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["list=[['2003-04-16 00:00:00',2003],\n            ['2004-05-28 00:00:00',2004],\n            ['2014-11-26 00:00:00',2014],\n            ['2015-02-06 00:00:00',2015],\n            ['2008-06-27 00:00:00',2008]]\n\ndf=spark.createDataFrame(list,['movie_theatrical_release_date_upd','year'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------+----+\nmovie_theatrical_release_date_upd|year|\n+---------------------------------+----+\n              2003-04-16 00:00:00|2003|\n              2004-05-28 00:00:00|2004|\n              2014-11-26 00:00:00|2014|\n              2015-02-06 00:00:00|2015|\n              2008-06-27 00:00:00|2008|\n+---------------------------------+----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"decade\", F.expr(\"\"\"int(year-right(year,1))\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+\nyear|decade|\n+----+------+\n2003|  2000|\n2004|  2000|\n2014|  2010|\n2015|  2010|\n2008|  2000|\n1995|  1990|\n+----+------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"decade\", F.regexp_replace(\"year\",'\\d(?!.*\\d)','0')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+\nyear|decade|\n+----+------+\n2003|  2000|\n2004|  2000|\n2014|  2010|\n2015|  2010|\n2008|  2000|\n1995|  1990|\n+----+------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"decade\", F.expr(\"\"\"concat(substring(year,1,length(year)-1),0)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+\nyear|decade|\n+----+------+\n2003|  2000|\n2004|  2000|\n2014|  2010|\n2015|  2010|\n2008|  2000|\n1995|  1990|\n+----+------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.withColumn(\"decade\", F.floor(F.col(\"year\")/10)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+\nyear|decade|\n+----+------+\n2003|   200|\n2004|   200|\n2014|   201|\n2015|   201|\n2008|   200|\n1995|   199|\n+----+------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"decade\", (F.floor(F.col(\"year\")/10)*10).cast(\"int\")).show()\n\n#+----+------+\n#|year|decade|\n#+----+------+\n#|2003|  2000|\n#|2004|  2000|\n#|2014|  2010|\n#|2015|  2010|\n#|2008|  2000|\n#+----+------+\n\ndf.withColumn(\"decade\", F.to_date(\"\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+\nyear|decade|\n+----+------+\n2003|  2000|\n2004|  2000|\n2014|  2010|\n2015|  2010|\n2008|  2000|\n1995|  1990|\n+----+------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["list=[[ '2018-08-01 06:01:00'  ,    1],\n      ['2018-08-01 06:01:30'  ,    1],\n      ['2018-08-01 09:00:00'  ,    1],\n      ['2018-08-01 09:00:00'  ,    2],\n      ['2018-08-01 10:15:43'  ,    2],\n      ['2018-08-01 11:00:01'  ,    3],\n      ['2018-08-01 06:00:13'  ,    4],\n      ['2018-08-01 13:00:00'  ,    4],\n      ['2018-08-13 14:00:00'  ,    5],\n      ['2018-08-13 14:15:03'  ,    5],\n      ['2018-08-13 14:45:08'  ,    5],\n      ['2018-08-13 14:50:00'  ,    5]]\n\ndf=spark.createDataFrame(list,['timestamp','ship'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----+\n          timestamp|ship|\n+-------------------+----+\n2018-08-01 06:01:00|   1|\n2018-08-01 06:01:30|   1|\n2018-08-01 09:00:00|   1|\n2018-08-01 09:00:00|   2|\n2018-08-01 10:15:43|   2|\n2018-08-01 11:00:01|   3|\n2018-08-01 06:00:13|   4|\n2018-08-01 13:00:00|   4|\n2018-08-13 14:00:00|   5|\n2018-08-13 14:15:03|   5|\n2018-08-13 14:45:08|   5|\n2018-08-13 14:50:00|   5|\n+-------------------+----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["list=[[ '2018-08-01 06:01:00'  ,  '2018-08-01 06:01:00' ,1],\n      ['2018-08-01 06:01:30'  ,   '2018-08-01 06:01:00' ,2]]\n\ndf=spark.createDataFrame(list,['timestamp','timestamp2','number'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+------+\n          timestamp|         timestamp2|number|\n+-------------------+-------------------+------+\n2018-08-01 06:01:00|2018-08-01 06:01:00|     1|\n2018-08-01 06:01:30|2018-08-01 06:01:00|     2|\n+-------------------+-------------------+------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["cols=['timestamp','timestamp2']\ndf=df.select(*[F.to_timestamp(x).cast('long').alias(x) for x in df.columns])\n\ndf.select(*[x for x in dt.columns if x not in cols],*[F.from_unixtime(x).alias(x) for x in cols]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-------------------+-------------------+\nnumber|          timestamp|         timestamp2|\n+------+-------------------+-------------------+\n     1|2018-08-01 06:01:00|2018-08-01 06:01:00|\n     2|2018-08-01 06:01:30|2018-08-01 06:01:00|\n+------+-------------------+-------------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["df.withColumn(\"struct\", F.struct(*[F.struct(F.from_unixtime(x)).alias(x) for x in cols]))\\\n  .select(*[F.col(\"struct.{}.col1\".format(x)).alias(x) for x in cols]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+\n          timestamp|         timestamp2|\n+-------------------+-------------------+\n2018-08-01 06:01:00|2018-08-01 06:01:00|\n2018-08-01 06:01:30|2018-08-01 06:01:00|\n+-------------------+-------------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from functools import reduce\nfrom pyspark.sql.functions import *\nnew_df = (reduce(\nlambda df, cols: df.withColumn(cols, from_unixtime(col(cols), \"yyyy-MM-dd HH:mm:ss\")),\ndf.columns,\ndf))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["new_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+-------------------+\n          timestamp|         timestamp2|             number|\n+-------------------+-------------------+-------------------+\n2018-08-01 06:01:00|2018-08-01 06:01:00|1970-01-01 00:00:01|\n2018-08-01 06:01:30|2018-08-01 06:01:00|1970-01-01 00:00:02|\n+-------------------+-------------------+-------------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["df.select(*[x for x in df.columns if x not in cols],*[F.from_unixtime(x).alias(x) for x in cols])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: DataFrame[number: bigint, timestamp: string, timestamp2: string]</div>"]}}],"execution_count":28},{"cell_type":"code","source":[" df=     dt.withColumn('PL_start', from_unixtime('PL_start', \"yyyy-MM-dd HH:mm:ss\")) \\\n            .withColumn('PL_end??'  , from_unixtime('PL_end', \"yyyy-MM-dd HH:mm:ss\"))   \\\n            .withColumn('MU_start', from_unixtime('MU_start', \"yyyy-MM-dd HH:mm:ss\")) \\\n            .withColumn('MU_end'  , from_unixtime('MU_end', \"yyyy-MM-dd HH:mm:ss\"))   \\\n            .withColumn('PU_start', from_unixtime('PU_start', \"yyyy-MM-dd HH:mm:ss\")) \\\n            .withColumn('PU_end'  , from_unixtime('PU_end', \"yyyy-MM-dd HH:mm:ss\"))   \\\n            .withColumn('RE_start', from_unixtime('RE_start', \"yyyy-MM-dd HH:mm:ss\")) \\\n            .withColumn('RE_end'  , from_unixtime('RE_end', \"yyyy-MM-dd HH:mm:ss\"))   \\"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"ship\").orderBy(F.unix_timestamp(\"timestamp\")).rangeBetween(-7199, Window.currentRow)\nw2=Window().partitionBy(\"ship\").orderBy(\"timestamp\")\nw3=Window().orderBy(\"ship\",\"timestamp\")\n\ndf.withColumn(\"trip\", F.sum(F.when(F.row_number().over(w2)==1, F.lit(1))\\\n                       .when(F.size(F.collect_list(\"ship\").over(w1))==1, F.lit(1))\\\n                       .otherwise(F.lit(0))).over(w3)).orderBy(\"ship\",\"timestamp\").show()\n\n#+-------------------+----+----+\n#|          timestamp|ship|trip|\n#+-------------------+----+----+\n#|2018-08-01 06:01:00|   1|   1|\n#|2018-08-01 06:01:30|   1|   1|\n#|2018-08-01 09:00:00|   1|   2|\n#|2018-08-01 09:00:00|   2|   3|\n#|2018-08-01 10:15:43|   2|   3|\n#|2018-08-01 11:00:01|   3|   4|\n#|2018-08-01 06:00:13|   4|   5|\n#|2018-08-01 13:00:00|   4|   6|\n#|2018-08-13 14:00:00|   5|   7|\n#|2018-08-13 14:15:03|   5|   7|\n#|2018-08-13 14:45:08|   5|   7|\n#|2018-08-13 14:50:00|   5|   7|\n#+-------------------+----+----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----+----+\n          timestamp|ship|trip|\n+-------------------+----+----+\n2018-08-01 06:01:00|   1|   1|\n2018-08-01 06:01:30|   1|   1|\n2018-08-01 09:00:00|   1|   2|\n2018-08-01 09:00:00|   2|   3|\n2018-08-01 10:15:43|   2|   3|\n2018-08-01 11:00:01|   3|   4|\n2018-08-01 06:00:13|   4|   5|\n2018-08-01 13:00:00|   4|   6|\n2018-08-13 14:00:00|   5|   7|\n2018-08-13 14:15:03|   5|   7|\n2018-08-13 14:45:08|   5|   7|\n2018-08-13 14:50:00|   5|   7|\n+-------------------+----+----+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["+----------------------+------+-------+\n|        timestamp     | ship | trip  |\n+----------------------+------+-------+\n| 2018-08-01 06:01:00  |    1 |    1  | # start new ship number\n| 2018-08-01 06:01:30  |    1 |    1  | # still within 2 hours of same ship number\n| 2018-08-01 09:00:00  |    1 |    2  | # more than 2 hours of same ship number = new trip\n| 2018-08-01 09:00:00  |    2 |    3  | # new ship number = new trip\n| 2018-08-01 10:15:43  |    2 |    3  | # still within 2 hours of same ship number\n| 2018-08-01 11:00:01  |    3 |    4  | # new ship number = new trip\n| 2018-08-01 06:00:13  |    4 |    5  | # new ship number = new trip\n| 2018-08-01 13:00:00  |    4 |    6  | # more than 2 hours of same ship number = new trip\n| 2018-08-13 14:00:00  |    5 |    7  | # new ship number = new trip\n| 2018-08-13 14:15:03  |    5 |    7  | # still within 2 hours of same ship number\n| 2018-08-13 14:45:08  |    5 |    7  | # still within 2 hours of same ship number\n| 2018-08-13 14:50:00  |    5 |    7  | # still within 2 hours of same ship number\n+-----------------------------+-------+"],"metadata":{},"outputs":[],"execution_count":31}],"metadata":{"name":"stackhelp63","notebookId":2540702947610358},"nbformat":4,"nbformat_minor":0}
