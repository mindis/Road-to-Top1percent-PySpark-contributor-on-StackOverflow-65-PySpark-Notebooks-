{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[['2019-04-26 01:19:10','2019-04-26 01:19:35']]\n\ndf=spark.createDataFrame(list,['prev_time','CAL_COMPLETION_TIME'])\n\ndf1=df.withColumn(\"CAL_COMPLETION_TIME\", F.to_timestamp(\"CAL_COMPLETION_TIME\"))\\\n  .withColumn(\"prev_time\", F.to_timestamp(\"prev_time\"))\ndf1.createOrReplaceTempView(\"df1\")\ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+\n          prev_time|CAL_COMPLETION_TIME|\n+-------------------+-------------------+\n2019-04-26 01:19:10|2019-04-26 01:19:35|\n+-------------------+-------------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df1.withColumn(\"sub\", F.when(((F.col(\"CAL_COMPLETION_TIME\").cast(\"long\")-F.col(\"prev_time\").cast(\"long\"))/60 < 30), F.lit(\"LESSTHAN30\"))\\\n               .otherwise(F.lit(\"GREATERTHAN\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+----------+\n          prev_time|CAL_COMPLETION_TIME|       sub|\n+-------------------+-------------------+----------+\n2019-04-26 01:19:10|2019-04-26 01:19:35|LESSTHAN30|\n+-------------------+-------------------+----------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df1.createOrReplaceTempView(\"df1\")\nspark.sql(\"select prev_time, CAL_COMPLETION_TIME, IF(((CAST(CAL_COMPLETION_TIME as bigint) - CAST(prev_time as bigint))/60)<30,'LESSTHAN30','GREATER') as difference_duration from df1\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-------------------+-------------------+\n          prev_time|CAL_COMPLETION_TIME|difference_duration|\n+-------------------+-------------------+-------------------+\n2019-04-26 01:19:10|2019-04-26 01:19:35|         LESSTHAN30|\n+-------------------+-------------------+-------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["case \n    when t1.prev_time <> t1.prev_time_calc and t1.\"CAL_COMPLETION_TIME\" - t1.prev_time < interval '30 min' \n      then t1.next_time_calc - t1.prev_time_calc\n    when (t1.next_time <> t1.next_time_calc and t1.next_time - t1.\"CAL_COMPLETION_TIME\" < interval '30 min') or (t1.next_time - t1.\"CAL_COMPLETION_TIME\" < interval '30 min')\n      then t1.next_time_calc - t1.\"CAL_COMPLETION_TIME\"\n  else null\n  end min_diff"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["list=[[1,1,5],\n       [1,2,7],\n        [1,3,6],\n        [1,4,9],\n        [1,5,4],\n        [1,6,3],\n        [2,1,9],\n        [2,2,10],\n        [2,3,8],\n        [2,4,12],\n         [2,5,4]]\n\ndf=spark.createDataFrame(list,['id','rowID','value'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+-----+\n id|rowID|value|\n+---+-----+-----+\n  1|    1|    5|\n  1|    2|    7|\n  1|    3|    6|\n  1|    4|    9|\n  1|    5|    4|\n  1|    6|    3|\n  2|    1|    9|\n  2|    2|   10|\n  2|    3|    8|\n  2|    4|   12|\n  2|    5|    4|\n+---+-----+-----+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\ndef grouped_map(df1):\n   for i in range(1, len(df1)):\n        if df1.loc[i, 'value']>df1.loc[i-1,'value']:\n           df1.loc[i,'value']=df1.loc[i-1,'value']\n\n   return df1\ndf.groupby(\"id\").apply(grouped_map).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+-----+\n id|rowID|value|\n+---+-----+-----+\n  1|    1|    5|\n  1|    2|    5|\n  1|    3|    5|\n  1|    4|    5|\n  1|    5|    4|\n  1|    6|    3|\n  2|    1|    9|\n  2|    2|    9|\n  2|    3|    8|\n  2|    4|    8|\n  2|    5|    4|\n+---+-----+-----+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf, PandasUDFType\n@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)\ndef grouped_map(df1):\n   for i in range(1, len(df1)):\n        if df1.loc[i, 'value']>df1.loc[i-1,'value']:\n           df1.loc[i,'value']=df1.loc[i-1,'value']\n            \n   return df1\ndf.groupby().apply(grouped_map).show() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+\nrowID|value|\n+-----+-----+\n    1|    5|\n    2|    5|\n    3|    5|\n    4|    5|\n    5|    4|\n    6|    3|\n+-----+-----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["def scalar(df1):\n   for i in range(1, len(df1)):\n        if df1.loc[i, 'value']>df1.loc[i-1,'value']:\n           df1.loc[i,'value']=df1.loc[i-1,'value']\n\ndf1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rowID</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf, PandasUDFType\n@pandas_udf(\"rowID integer, value double\", PandasUDFType.GROUPED_MAP)  \ndef normalize(pdf):\n    import pandas as pd\n    value = pdf.value\n    return pdf.assign(value=value.mean())\ndf.groupby().apply(normalize).show() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----------------+\nrowID|            value|\n+-----+-----------------+\n    1|5.666666666666667|\n    2|5.666666666666667|\n    3|5.666666666666667|\n    4|5.666666666666667|\n    5|5.666666666666667|\n    6|5.666666666666667|\n+-----+-----------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf, PandasUDFType\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))  \n@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)  \ndef normalize(pdf):\n    v = pdf.v\n    return pdf.assign(v=v.mean())\ndf.groupby().apply(normalize).show() "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n id|  v|\n+---+---+\n  1|4.2|\n  1|4.2|\n  2|4.2|\n  2|4.2|\n  2|4.2|\n+---+---+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\ndef add_one(a):\n    for i in range(1,len(a)):\n       if a[i]>a[i-1]:\n           a[i]=a[i-1]\n    return a\nudf1= F.udf(add_one, ArrayType(IntegerType()))\ndf.agg(F.collect_list(\"rowID\").alias(\"rowID\"),F.collect_list(\"value\").alias(\"value\"))\\\n  .withColumn(\"value\", udf1(\"value\"))\\\n  .withColumn(\"zipped\", F.explode(F.struct((\"rowID\"),(\"value\")))).select(\"zipped.*\").show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o14391.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;explode(named_struct(&#39;rowID&#39;, `rowID`, &#39;value&#39;, `value`))&#39; due to data type mismatch: input to function explode should be array or map type, not struct&lt;rowID:array&lt;bigint&gt;,value:array&lt;int&gt;&gt;;;\n&#39;Project [rowID#7222, value#7230, explode(named_struct(rowID, rowID#7222, value, value#7230)) AS zipped#7233]\n+- Project [rowID#7222, add_one(value#7224) AS value#7230]\n   +- Aggregate [collect_list(rowID#7143L, 0, 0) AS rowID#7222, collect_list(value#7144L, 0, 0) AS value#7224]\n      +- LogicalRDD [rowID#7143L, value#7144L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.GeneratedMethodAccessor304.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-114847202514273&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> df<span class=\"ansi-blue-fg\">.</span>agg<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>collect_list<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rowID&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rowID&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>collect_list<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;value&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;value&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;value&#34;</span><span class=\"ansi-blue-fg\">,</span> udf1<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;value&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">---&gt; 11</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;zipped&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>struct<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;rowID&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;value&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;zipped.*&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;explode(named_struct(&#39;rowID&#39;, `rowID`, &#39;value&#39;, `value`))&#39; due to data type mismatch: input to function explode should be array or map type, not struct&lt;rowID:array&lt;bigint&gt;,value:array&lt;int&gt;&gt;;;\\n&#39;Project [rowID#7222, value#7230, explode(named_struct(rowID, rowID#7222, value, value#7230)) AS zipped#7233]\\n+- Project [rowID#7222, add_one(value#7224) AS value#7230]\\n   +- Aggregate [collect_list(rowID#7143L, 0, 0) AS rowID#7222, collect_list(value#7144L, 0, 0) AS value#7224]\\n      +- LogicalRDD [rowID#7143L, value#7144L], false\\n&#34;</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\ndef add_one(a):\n    for i in range(1,len(a)):\n       if a[i]>a[i-1]:\n           a[i]=a[i-1]\n    return a\nudf1= F.udf(add_one, ArrayType(IntegerType()))\ndf.agg(F.collect_list(\"rowID\").alias(\"rowID\"),F.collect_list(\"value\").alias(\"value\"))\\\n  .withColumn(\"value\", udf1(\"value\"))\\\n  .withColumn(\"zipped\", F.explode(F.arrays_zip(\"rowID\",\"value\"))).select(\"zipped.*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+\nrowID|value|\n+-----+-----+\n    1|    5|\n    2|    5|\n    3|    5|\n    4|    5|\n    5|    4|\n    6|    3|\n+-----+-----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["a=[5,7,6,9,4,3]\nfor i in range(1,len(a)):\n  if a[i]>a[i-1]:\n    a[i]=a[i-1]\n\n    \na"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[209]: [5, 5, 5, 5, 4, 3]</div>"]}}],"execution_count":14},{"cell_type":"code","source":["for i in range(1,len(a)):\n  print(a[i-1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">5\n7\n6\n9\n4\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["w=Window().partitionBy().orderBy(\"rowID\")\ndf.withColumn(\"lag\", F.lag(\"value\").over(w))\\\n  .withColumn(\"lag2\", F.when((F.col(\"value\")>F.col(\"lag\")),F.lit(1)).otherwise(F.lit(0))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+----+----+\nrowID|value| lag|lag2|\n+-----+-----+----+----+\n    1|    5|null|   0|\n    2|    7|   5|   1|\n    3|    6|   7|   0|\n    4|    9|   6|   1|\n    5|    4|   9|   0|\n    6|    3|   4|   0|\n+-----+-----+----+----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["import pandas as pd\nimport datetime\n\ndata = {'date': ['2014-01-01', '2014-01-02', '2014-01-03', '2014-01-04', '2014-01-05', '2014-01-06', '2014-01-07'],\n     'id': [1, 2, 2, 3, 4, 4, 5], 'name': ['Darren', 'Sabrina', 'Steve', 'Sean', 'Ray', 'Stef', 'Dany']}\ndata = pd.DataFrame(data)\ndata['date'] = pd.to_datetime(data['date'])\ndf=spark.createDataFrame(data)\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---+-------+\n               date| id|   name|\n+-------------------+---+-------+\n2014-01-01 00:00:00|  1| Darren|\n2014-01-02 00:00:00|  2|Sabrina|\n2014-01-03 00:00:00|  2|  Steve|\n2014-01-04 00:00:00|  3|   Sean|\n2014-01-05 00:00:00|  4|    Ray|\n2014-01-06 00:00:00|  4|   Stef|\n2014-01-07 00:00:00|  5|   Dany|\n+-------------------+---+-------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf=spark.createDataFrame(data)\n\nw=Window().partitionBy(\"id\").orderBy((F.col(\"date\")).cast(\"long\")).rangeBetween(-(86400*2),Window.currentRow)\ndf.withColumn(\"no_distinct\", F.size(F.array_distinct(F.collect_list(\"name\").over(w))))\\\n  .withColumn(\"no_distinct\", F.when(F.col(\"no_distinct\")>1, F.lit(1)).otherwise(F.lit(0)))\\\n  .orderBy(F.col(\"date\")).show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---+-------+-----------+\n               date| id|   name|no_distinct|\n+-------------------+---+-------+-----------+\n2014-01-01 00:00:00|  1| Darren|          0|\n2014-01-02 00:00:00|  2|Sabrina|          0|\n2014-01-03 00:00:00|  2|  Steve|          1|\n2014-01-04 00:00:00|  3|   Sean|          0|\n2014-01-05 00:00:00|  4|    Ray|          0|\n2014-01-06 00:00:00|  4|   Stef|          1|\n2014-01-07 00:00:00|  5|   Dany|          0|\n+-------------------+---+-------+-----------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["for i in df1.collect()[3]:\n  print(i)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2014-01-04 00:00:00\n3\nSean\n0\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["df1.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[179]: [Row(date=datetime.datetime(2014, 1, 1, 0, 0), id=1, name=&#39;Darren&#39;, no_distinct=0),\n Row(date=datetime.datetime(2014, 1, 2, 0, 0), id=2, name=&#39;Sabrina&#39;, no_distinct=0),\n Row(date=datetime.datetime(2014, 1, 3, 0, 0), id=2, name=&#39;Steve&#39;, no_distinct=1),\n Row(date=datetime.datetime(2014, 1, 4, 0, 0), id=3, name=&#39;Sean&#39;, no_distinct=0),\n Row(date=datetime.datetime(2014, 1, 5, 0, 0), id=4, name=&#39;Ray&#39;, no_distinct=0),\n Row(date=datetime.datetime(2014, 1, 6, 0, 0), id=4, name=&#39;Stef&#39;, no_distinct=1),\n Row(date=datetime.datetime(2014, 1, 7, 0, 0), id=5, name=&#39;Dany&#39;, no_distinct=0)]</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df1.dtypes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[181]: [(&#39;date&#39;, &#39;timestamp&#39;),\n (&#39;id&#39;, &#39;bigint&#39;),\n (&#39;name&#39;, &#39;string&#39;),\n (&#39;no_distinct&#39;, &#39;int&#39;)]</div>"]}}],"execution_count":21},{"cell_type":"code","source":["list=[['profile','CREATE_CARD','2020-02-24 03:07:04']]\ndf=spark.createDataFrame(list,['attribute','operation','timestamp'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-----------+-------------------+\nattribute|  operation|          timestamp|\n+---------+-----------+-------------------+\n  profile|CREATE_CARD|2020-02-24 03:07:04|\n+---------+-----------+-------------------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df1=df.withColumn(\"params\", F.array(F.struct(F.lit('kobo1'),F.lit('kobo2'),F.lit('kobo3'))))\ndf2=df.withColumn(\"params\", F.array(F.struct(F.lit('kobo1'),F.lit('kobo2'),F.lit('kobo3'),F.lit('kobo4'),F.lit('kobo5'))))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["import re\nstring=df1.dtypes[3][1]\nd=re.findall('\\d+', string )\na=','.join(['x.'+'col'+x for x in d])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["import re\nstring=df2.dtypes[3][1]\nd=re.findall('\\d+', string )\na=','.join(['x.'+'col'+x for x in d])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["import re\nstring=df1.dtypes[3][1]\nd=re.findall('\\d+', string )\na=','.join(['x.'+'col'+x for x in d])\na='x.col1,x.col2,x.col3'\ndf1.withColumn(\"params\", F.flatten(F.expr(\"transform(params, x -> array('\"+ a +\"'))\"))).printSchema()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- attribute: string (nullable = true)\n-- operation: string (nullable = true)\n-- timestamp: string (nullable = true)\n-- params: array (nullable = false)\n    |-- element: string (containsNull = false)\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df1=df.withColumn(\"params\", F.struct(F.lit('kobo1'),F.lit('kobo2'),F.lit('kobo3')))\ndf2=df.withColumn(\"params\", F.struct(F.lit('kobo1'),F.lit('kobo2'),F.lit('kobo3'),F.lit('kobo4'),F.lit('kobo5')))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["df1=df1.withColumn(\"params\", F.array(*(['params.*'])))\ndf2=df2.withColumn(\"params\", F.array(*(['params.*'])))\n\n\ndf1.union(df2).show(truncate=False)\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-----------+-------------------+-----------------------------------+\nattribute|operation  |timestamp          |params                             |\n+---------+-----------+-------------------+-----------------------------------+\nprofile  |CREATE_CARD|2020-02-24 03:07:04|[kobo1, kobo2, kobo3]              |\nprofile  |CREATE_CARD|2020-02-24 03:07:04|[kobo1, kobo2, kobo3, kobo4, kobo5]|\n+---------+-----------+-------------------+-----------------------------------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["df1.withColumn(\"params\", F.flatten(\"params\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o969.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;flatten(`params`)&#39; due to data type mismatch: The argument should be an array of arrays, but &#39;`params`&#39; is of struct&lt;col1:string,col2:string,col3:string&gt; type.;;\n&#39;Project [attribute#2, operation#3, timestamp#4, flatten(params#52) AS params#94]\n+- Project [attribute#2, operation#3, timestamp#4, named_struct(col1, kobo1, col2, kobo2, col3, kobo3) AS params#52]\n   +- LogicalRDD [attribute#2, operation#3, timestamp#4], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.GeneratedMethodAccessor304.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1878396162231379&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;params&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>flatten<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;params&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;flatten(`params`)&#39; due to data type mismatch: The argument should be an array of arrays, but &#39;`params`&#39; is of struct&lt;col1:string,col2:string,col3:string&gt; type.;;\\n&#39;Project [attribute#2, operation#3, timestamp#4, flatten(params#52) AS params#94]\\n+- Project [attribute#2, operation#3, timestamp#4, named_struct(col1, kobo1, col2, kobo2, col3, kobo3) AS params#52]\\n   +- LogicalRDD [attribute#2, operation#3, timestamp#4], false\\n&#34;</div>"]}}],"execution_count":29},{"cell_type":"code","source":["df1.withColumn(\"params\", F.expr(\"\"\"transform(array(params),x-> x)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-----------+-------------------+--------------------+\nattribute|  operation|          timestamp|              params|\n+---------+-----------+-------------------+--------------------+\n  profile|CREATE_CARD|2020-02-24 03:07:04|[[kobo1, kobo2, k...|\n+---------+-----------+-------------------+--------------------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["list=[['11/9/18 14:11'],\n      ['11/9/18 14:27'],\n      ['11/9/18 14:42'],\n      ['11/9/18 14:57']]\n\ndf=spark.createDataFrame(list,['timestamp'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+\n    timestamp|\n+-------------+\n11/9/18 14:11|\n11/9/18 14:27|\n11/9/18 14:42|\n11/9/18 14:57|\n+-------------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\",\"dd/MM/yy HH:mm\"))\\\n  .withColumn(\"time\",F.date_format(\"timestamp\",\"HH:mm\"))\\\n  .withColumn(\"current_date_formated\",F.date_format(\"timestamp\", \"M/d/yy\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----+---------------------+\n          timestamp| time|current_date_formated|\n+-------------------+-----+---------------------+\n2018-09-11 14:11:00|14:11|              9/11/18|\n2018-09-11 14:27:00|14:27|              9/11/18|\n2018-09-11 14:42:00|14:42|              9/11/18|\n2018-09-11 14:57:00|14:57|              9/11/18|\n+-------------------+-----+---------------------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["schema = StructType([StructField(\"col1\", StringType())\\\n                   ,StructField(\"col2\", IntegerType())\\\n                   ,StructField(\"row_number\", IntegerType())])\n\ndata = [['X', 1, 1], ['Y', 0, 2], ['Z', 2, 3], ['A', 1, 4], ['B', 0, 5], ['C', 0, 6], ['D', 2, 7], ['P', 1, 8], ['Q', 2, 9]]\n\ndf = spark.createDataFrame(data,schema=schema)\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----------+\ncol1|col2|row_number|\n+----+----+----------+\n   X|   1|         1|\n   Y|   0|         2|\n   Z|   2|         3|\n   A|   1|         4|\n   B|   0|         5|\n   C|   0|         6|\n   D|   2|         7|\n   P|   1|         8|\n   Q|   2|         9|\n+----+----+----------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["w=Window().orderBy(\"row_number\") \ndf=df.withColumn(\"incremental_sum\", F.sum(\"col2\").over(w)).withColumn(\"lag\", F.lag(\"incremental_sum\").over(w)).withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")%3==0, F.col(\"lag\")).otherwise(F.col(\"incremental_sum\"))) w2=Window().partitionBy(\"incremental_sum\")\ndf.withColumn(\"name\", F.concat_ws(\"\", F.collect_list(df.col1).over(w2))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-2482587633947714&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">2</span>\n<span class=\"ansi-red-fg\">    df=df.withColumn(&#34;incremental_sum&#34;, F.sum(&#34;col2&#34;).over(w)).withColumn(&#34;lag&#34;, F.lag(&#34;incremental_sum&#34;).over(w)).withColumn(&#34;incremental_sum&#34;, F.when(F.col(&#34;incremental_sum&#34;)%3==0, F.col(&#34;lag&#34;)).otherwise(F.col(&#34;incremental_sum&#34;))) w2=Window().partitionBy(&#34;incremental_sum&#34;)</span>\n                                                                                                                                                                                                                                           ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> invalid syntax\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql.functions import when\nw=Window().orderBy(\"row_number\")\ndf.withColumn(\"incremental_sum\", F.sum(\"col2\").over(w))\\\n  .withColumn(\"lag\", F.lag(\"incremental_sum\").over(w))\\\n  .withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")%3==0, F.col(\"lag\")).otherwise(F.col(\"incremental_sum\")))\\\n  .groupBy(\"incremental_sum\").agg(F.concat_ws(\"\",F.collect_list(\"col1\")).alias(\"output_col\")).drop(\"incremental_sum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\noutput_col|\n+----------+\n       XYZ|\n      ABCD|\n        PQ|\n+----------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql.functions import when\nw=Window().orderBy(\"row_number\")\nw1=Window().orderBy(F.col(\"row_number\"))\nw2=Window().partitionBy(\"incremental_sum\")\ndf.withColumn(\"incremental_sum\", F.sum(\"col2\").over(w))\\\n  .withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")%3==0, F.col(\"incremental_sum\")).otherwise(F.lit(0)))\\\n  .withColumn(\"lag\", F.lag(\"incremental_sum\").over(w1))\\\n  .withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")==3, F.lit(0)).when(F.col(\"lag\")==3, F.col(\"lag\")).otherwise(F.col(\"incremental_sum\"))).drop(\"lag\")\\\n  .withColumn(\"incremental_sum\", F.sum(\"incremental_sum\").over(w))\\\n  .withColumn(\"output_col\", F.collect_list(\"col1\").over(w2)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----------+---------------+----------+\ncol1|col2|row_number|incremental_sum|output_col|\n+----+----+----------+---------------+----------+\n   X|   1|         1|              0| [X, Y, Z]|\n   Y|   0|         2|              0| [X, Y, Z]|\n   Z|   2|         3|              0| [X, Y, Z]|\n   A|   1|         4|              3| [A, B, C]|\n   B|   0|         5|              3| [A, B, C]|\n   C|   0|         6|              3| [A, B, C]|\n   D|   2|         7|              9|    [D, P]|\n   P|   1|         8|              9|    [D, P]|\n   Q|   2|         9|             18|       [Q]|\n+----+----+----------+---------------+----------+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql.functions import when\nw=Window().orderBy(\"row_number\")\nw2=Window().partitionBy(\"incremental_sum\")\ndf.withColumn(\"incremental_sum\", F.sum(\"col2\").over(w))\\\n  .withColumn(\"lag\", F.lag(\"incremental_sum\").over(w))\\\n  .withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")%3==0, F.col(\"lag\")).otherwise(F.col(\"incremental_sum\")))\\\n  .withColumn(\"output_col\", F.array_join(F.collect_list(\"col1\").over(w2),''))\\\n  .select(F.col(\"output_col\")).distinct().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\noutput_col|\n+----------+\n       XYZ|\n      ABCD|\n        PQ|\n+----------+\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["from pyspark.sql.functions import when\nw=Window().orderBy(\"row_number\")\ndf.withColumn(\"incremental_sum\", F.sum(\"col2\").over(w))\\\n  .withColumn(\"lag\", F.lag(\"incremental_sum\").over(w))\\\n  .withColumn(\"incremental_sum\", F.when(F.col(\"incremental_sum\")%3==0, F.col(\"lag\")).otherwise(F.col(\"incremental_sum\")))\\\n  .groupBy(\"incremental_sum\").agg(F.array_join(F.collect_list(\"col1\"),\"\").alias(\"output_col\")).drop(\"incremental_sum\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\noutput_col|\n+----------+\n       XYZ|\n      ABCD|\n        PQ|\n+----------+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["from pyspark.sql import functions as F    \n    df1=df_json.select(\"id\",\"type\",\"name\",\"ppu\",\"topping\",\"batters.*\")\\\n           .withColumn(\"batter\", F.explode(\"batter\"))\\\n           .select(\"id\",\"type\",\"name\",\"ppu\",\"topping\",\"batter\")\n    df1.withColumn(\"topping\", F.explode(\"topping\")).select(\"id\",\"type\",\"name\",\"ppu\",\"topping.*\",\"batter.*\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["list=[[['a', 1.0], [['b', 5.0]]],[[['c', 1.0]], [['d', 2.0]]]]\ndf=spark.createDataFrame(list,['1','2'])\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- 1: array (nullable = true)\n    |-- element: array (containsNull = true)\n    |    |-- element: string (containsNull = true)\n-- 2: array (nullable = true)\n    |-- element: array (containsNull = true)\n    |    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["  timestamp\n11/9/18 14:11\n11/9/18 14:27\n11/9/18 14:42\n11/9/18 14:57"],"metadata":{},"outputs":[],"execution_count":42}],"metadata":{"name":"stackhelp41","notebookId":2539022226029898},"nbformat":4,"nbformat_minor":0}
