{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["df1 = spark.createDataFrame(pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': ['one', 'two', 'three', 'five']}))\ndf2 = spark.createDataFrame(pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': ['five', 'six', None, None]}))\ndf1.show()\n\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+\nlkey|value|\n+----+-----+\n foo|  one|\n bar|  two|\n baz|three|\n foo| five|\n+----+-----+\n\n+----+-----+\nrkey|value|\n+----+-----+\n foo| five|\n bar|  six|\n baz| null|\n foo| null|\n+----+-----+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf1.withColumnRenamed(\"value\",\"value_x\")\\\n    .join(df2.withColumnRenamed(\"value\",\"value_y\"),F.col(\"lkey\")==F.col(\"rkey\"),'left')\\\n    .withColumn(\"value_x\", F.when(F.col(\"value_y\").isNotNull(),F.col(\"value_y\")).otherwise(F.col(\"value_x\"))).show()\n\n\n#+----+-------+----+-------+\n#|lkey|value_x|rkey|value_y|\n#+----+-------+----+-------+\n#| bar|    six| bar|    six|\n#| foo|   five| foo|   five|\n#| foo|    one| foo|   null|\n#| foo|   five| foo|   five|\n#| foo|   five| foo|   null|\n#| baz|  three| baz|   null|\n#+----+-------+----+-------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-------+----+-------+\nlkey|value_x|rkey|value_y|\n+----+-------+----+-------+\n bar|    six| bar|    six|\n foo|   five| foo|   five|\n foo|    one| foo|   null|\n foo|   five| foo|   five|\n foo|   five| foo|   null|\n baz|  three| baz|   null|\n+----+-------+----+-------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["  lkey value_x rkey value_y\n0  foo    five  foo    five\n1  foo     one  foo     NaN\n2  bar     six  bar     six\n3  baz   three  baz     NaN\n4  foo    five  foo    five\n5  foo    five  foo     NaN"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["list=[[95110]]\n\ndf=spark.createDataFrame(list,['code'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\n code|\n+-----+\n95110|\n+-----+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["list=[[\"1 2 3 4 5 6 7 8 9 10\"]]\n\ndf=spark.createDataFrame(list,['x'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.select(*[(F.split(\"x\",' ')[i]).alias(str(i)) for i in range(100)])\n\n#== Physical Plan ==\n#*(1) Project [split(x#200,  )[0] AS 0#1708, split(x#200,  )[1] AS 1#1709, split(x#200,  )[2] AS 2#1710, split(x#200,  )[3] AS 3#1711, split(x#200,  )[4] AS 4#1712, split(x#200,  )[5] AS 5#1713, split(x#200,  )[6] AS 6#1714, split(x#200,  )[7] AS 7#1715, split(x#200,  )[8] AS 8#1716, split(x#200,  )[9] AS 9#1717, split(x#200,  )[10] AS 10#1718, split(x#200,  )[11] AS 11#1719, split(x#200,  )[12] AS 12#1720, split(x#200,  )[13] AS 13#1721, split(x#200,  )[14] AS 14#1722, split(x#200,  )[15] AS 15#1723, split(x#200,  )[16] AS 16#1724, split(x#200,  )[17] AS 17#1725, split(x#200,  )[18] AS 18#1726, split(x#200,  )[19] AS 19#1727, split(x#200,  )[20] AS 20#1728, split(x#200,  )[21] AS 21#1729, split(x#200,  )[22] AS 22#1730, split(x#200,  )[23] AS 23#1731, ... 76 more fields]\n#+- *(1) Scan ExistingRDD[x#200]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(1) Project [split(x#200,  )[0] AS 0#1708, split(x#200,  )[1] AS 1#1709, split(x#200,  )[2] AS 2#1710, split(x#200,  )[3] AS 3#1711, split(x#200,  )[4] AS 4#1712, split(x#200,  )[5] AS 5#1713, split(x#200,  )[6] AS 6#1714, split(x#200,  )[7] AS 7#1715, split(x#200,  )[8] AS 8#1716, split(x#200,  )[9] AS 9#1717, split(x#200,  )[10] AS 10#1718, split(x#200,  )[11] AS 11#1719, split(x#200,  )[12] AS 12#1720, split(x#200,  )[13] AS 13#1721, split(x#200,  )[14] AS 14#1722, split(x#200,  )[15] AS 15#1723, split(x#200,  )[16] AS 16#1724, split(x#200,  )[17] AS 17#1725, split(x#200,  )[18] AS 18#1726, split(x#200,  )[19] AS 19#1727, split(x#200,  )[20] AS 20#1728, split(x#200,  )[21] AS 21#1729, split(x#200,  )[22] AS 22#1730, split(x#200,  )[23] AS 23#1731, ... 76 more fields]\n+- *(1) Scan ExistingRDD[x#200]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf\\\n  .withColumn(\"x\", F.split('x',' '))\\\n  .select(*[(F.col(\"x\")[i]).alias(str(i)) for i in range(100)]).drop(\"x\").explain()\n\n#== Physical Plan ==\n#*(1) Project [x#1908[0] AS 0#1910, x#1908[1] AS 1#1911, x#1908[2] AS 2#1912, x#1908[3] AS 3#1913, x#1908[4] AS 4#1914, x#1908[5] AS 5#1915, x#1908[6] AS 6#1916, x#1908[7] AS 7#1917, x#1908[8] AS 8#1918, x#1908[9] AS 9#1919, x#1908[10] AS 10#1920, x#1908[11] AS 11#1921, x#1908[12] AS 12#1922, x#1908[13] AS 13#1923, x#1908[14] AS 14#1924, x#1908[15] AS 15#1925, x#1908[16] AS 16#1926, x#1908[17] AS 17#1927, x#1908[18] AS 18#1928, x#1908[19] AS 19#1929, x#1908[20] AS 20#1930, x#1908[21] AS 21#1931, x#1908[22] AS 22#1932, x#1908[23] AS 23#1933, ... 76 more fields]\n#+- *(1) Project [split(x#200,  ) AS x#1908]\n#   +- *(1) Scan ExistingRDD[x#200]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(1) Project [x#2210[0] AS 0#2212, x#2210[1] AS 1#2213, x#2210[2] AS 2#2214, x#2210[3] AS 3#2215, x#2210[4] AS 4#2216, x#2210[5] AS 5#2217, x#2210[6] AS 6#2218, x#2210[7] AS 7#2219, x#2210[8] AS 8#2220, x#2210[9] AS 9#2221, x#2210[10] AS 10#2222, x#2210[11] AS 11#2223, x#2210[12] AS 12#2224, x#2210[13] AS 13#2225, x#2210[14] AS 14#2226, x#2210[15] AS 15#2227, x#2210[16] AS 16#2228, x#2210[17] AS 17#2229, x#2210[18] AS 18#2230, x#2210[19] AS 19#2231, x#2210[20] AS 20#2232, x#2210[21] AS 21#2233, x#2210[22] AS 22#2234, x#2210[23] AS 23#2235, ... 76 more fields]\n+- *(1) Project [split(x#200,  ) AS x#2210]\n   +- *(1) Scan ExistingRDD[x#200]\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"contains_5digit\", F.expr(\"\"\"IF(length(code)==5,1,0)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---------------+\n code|contains_5digit|\n+-----+---------------+\n95110|              1|\n+-----+---------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df = spark.createDataFrame([[{\"x\": 30.0, \"pool\": 20.0, \"helium\": 10.0}, -5],\n                           [{\"x\": 40.0, \"pool\": 30.0, \"helium\": 20.0}, 5]],[\"col1\", \"col2\"])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----+\n                col1|col2|\n+--------------------+----+\n[pool -&gt; 20.0, x ...|  -5|\n[pool -&gt; 30.0, x ...|   5|\n+--------------------+----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["+------------------------------+-----+\n|col1                          |col2 |\n+------------------------------+-----+\n|\"x: 1.0, y: 2.0, z: 3.0\"      | 5.0 |\n|\"x: 4.0, y: 5.0, z: 6.0\"      | 5.0 |\n+------------------------------+-----+"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(df.withColumn(\"col1\", F.col(\"col1\").cast(\"string\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col1</th><th>col2</th></tr></thead><tbody><tr><td>[pool -> 20.0, x -> 30.0, helium -> 10.0]</td><td>-5</td></tr><tr><td>[pool -> 30.0, x -> 40.0, helium -> 20.0]</td><td>5</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"code","source":["df.withColumn(\"col1\",F.concat(F.lit('\"'),F.col(\"col1\").cast(\"string\"),F.lit('\"'))).show(truncate=False)\n\n#+------------------------------+----+\n#|col1                          |col2|\n#+------------------------------+----+\n#|{pool:20.0,x:30.0,helium:10.0}|-5  |\n#|{pool:30.0,x:40.0,helium:20.0}|5   |\n#+------------------------------+----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------------------------+----+\ncol1                                       |col2|\n+-------------------------------------------+----+\n&#34;[pool -&gt; 20.0, x -&gt; 30.0, helium -&gt; 10.0]&#34;|-5  |\n&#34;[pool -&gt; 30.0, x -&gt; 40.0, helium -&gt; 20.0]&#34;|5   |\n+-------------------------------------------+----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': ['one', 'two', 'three', 'five']})\ndf2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': ['five', 'six', nan, nan]})final_reference = clean_reference.withColumn(\"OutputItemNameByValue\",\\\n                       f.when((f.col(\"OutputItemNameByValue\") == \" \")|                              \n                             (f.col(\"PrimaryLookupAttributeValue\") ==\\\n                              \"TRIANA_DEFAULT\"), f.col(\"OutputItemNameByValue\"))\\\n                                             .otherwise(f.lit(\"Not Implemented\")))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["final_reference = clean_reference.withColumn(\"OutputItemNameByValue\",F.expr(\"clean_reference.OutputItemNameByValue == \" \" or\\   clean_reference.PrimaryLookupAttributeValue == 'TRIANA_DEFAULT', clean_reference.OutputItemNameByValue,Not Implemented\""],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df = spark.createDataFrame([(0, 'a', 1),\n                                 (1, 'a', 1),\n                                 (2, 'a', 1),\n                                 (3, 'a', 1)],['day', 'user', 'raw_score'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+---------+\nday|user|raw_score|\n+---+----+---------+\n  0|   a|        1|\n  1|   a|        1|\n  2|   a|        1|\n  3|   a|        1|\n+---+----+---------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\n\n@pandas_udf(df.withColumn(\"raw_score\", F.lit(1.2456)).schema, PandasUDFType.GROUPED_MAP)\ndef grouped_map(df):\n     for i in range(1,len(df)):\n          df.loc[i,'raw_score']=(df.loc[i-1,'raw_score'] * 0.9)+1   \n\n     return df\ndf\\\n  .groupby(\"user\").apply(grouped_map).show()\n\n#+---+----+---------+\n#|day|user|raw_score|\n#+---+----+---------+\n#|  0|   a|      1.0|\n#|  1|   a|      1.9|\n#|  2|   a|     2.71|\n#|  3|   a|    3.439|\n#+---+----+---------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+---------+\nday|user|raw_score|\n+---+----+---------+\n  0|   a|      1.0|\n  1|   a|      1.9|\n  2|   a|     2.71|\n  3|   a|    3.439|\n+---+----+---------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.withColumn(\"raw_score\", F.lit(1.2456)).schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: StructType(List(StructField(day,LongType,true),StructField(user,StringType,true),StructField(raw_score,DoubleType,false)))</div>"]}}],"execution_count":19},{"cell_type":"code","source":["list=[[4  ,8  ,1  ,1 ,2  ,2  ,8   ,3  ,5],\n      [7  ,5  ,6  ,1 ,6  ,2  ,3   ,3  ,1],\n      [7  ,3  ,5  ,1 ,9  ,2  ,4   ,3  ,7]]\n\ndf=spark.createDataFrame(list,['x','y','z','a','ad','b','bd','c','cd'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+---+---+---+---+---+\n  x|  y|  z|  a| ad|  b| bd|  c| cd|\n+---+---+---+---+---+---+---+---+---+\n  4|  8|  1|  1|  2|  2|  8|  3|  5|\n  7|  5|  6|  1|  6|  2|  3|  3|  1|\n  7|  3|  5|  1|  9|  2|  4|  3|  7|\n+---+---+---+---+---+---+---+---+---+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.withColumn(\"zip\", F.arrays_zip(F.array('a','b','c'),F.array('ad','bd','cd')))\\\n  .withColumn(\"min\", F.array_min(F.array(\"ad\",\"bd\",\"cd\")))\\\n  .printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- x: long (nullable = true)\n-- y: long (nullable = true)\n-- z: long (nullable = true)\n-- a: long (nullable = true)\n-- ad: long (nullable = true)\n-- b: long (nullable = true)\n-- bd: long (nullable = true)\n-- c: long (nullable = true)\n-- cd: long (nullable = true)\n-- zip: array (nullable = false)\n    |-- element: struct (containsNull = false)\n    |    |-- 0: long (nullable = true)\n    |    |-- 1: long (nullable = true)\n-- min: long (nullable = true)\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.withColumn(\"XX\", F.array('a','b','c')).withColumn(\"XD\", F.array('ad','bd','cd'))\\\n  .withColumn(\"min\", F.array_min(\"XD\")).withColumn(\"zip\", F.arrays_zip(\"XX\",\"XD\"))\\\n  .withColumn(\"id\", F.expr(\"\"\"filter(zip,x-> x.XD=min)\"\"\"))\\\n  .select(\"x\",\"y\",\"z\", (F.col(\"id.XX\")[0]).alias(\"id\"))\\\n  .show(truncate=False)\n\n#+---+---+---+---+\n#|x  |y  |z  |id |\n#+---+---+---+---+\n#|4  |8  |1  |1  |\n#|7  |5  |6  |3  |\n#|7  |3  |5  |2  |\n#+---+---+---+---+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+\nx  |y  |z  |id |\n+---+---+---+---+\n4  |8  |1  |1  |\n7  |5  |6  |3  |\n7  |3  |5  |2  |\n+---+---+---+---+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf.withColumn(\"zip\", F.arrays_zip(F.array('a','b','c'),F.array('ad','bd','cd')))\\\n  .withColumn(\"id\", F.expr(\"\"\"filter(zip,x-> x.`1`=array_min(array(ad,bd,cd)))\"\"\"))\\\n  .select(\"x\",\"y\",\"z\", (F.col(\"id.0\")[0]).alias(\"id\")).show()\n\n#+---+---+---+---+\n#|  x|  y|  z| id|\n#+---+---+---+---+\n#|  4|  8|  1|  1|\n#|  7|  5|  6|  3|\n#|  7|  3|  5|  2|\n#+---+---+---+---+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+\n  x|  y|  z| id|\n+---+---+---+---+\n  4|  8|  1|  1|\n  7|  5|  6|  3|\n  7|  3|  5|  2|\n+---+---+---+---+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["list=[['a', 2, 10],\n      ['a' ,1 ,14],\n      ['a' ,4 ,5],\n      ['b' ,1 ,4]]\n\ndf=spark.createDataFrame(list,['user','day','amount'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+------+\nuser|day|amount|\n+----+---+------+\n   a|  2|    10|\n   a|  1|    14|\n   a|  4|     5|\n   b|  1|     4|\n+----+---+------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["a=df.agg(F.min(\"day\"),F.max(\"day\")).collect()[0]\n\na[0]\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: 1</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(F.lit(0))\n\ndf.withColumn(\"boundaries\", F.sequence(F.min(\"day\").over(w),F.max(\"day\").over(w),F.lit(1)))\\\n  .groupBy(\"user\").agg(F.collect_list(\"day\").alias('day'),F.collect_list(\"amount\").alias('amount')\\\n   ,F.first(\"boundaries\").alias(\"boundaries\")).withColumn(\"boundaries\", F.array_except(\"boundaries\",\"day\"))\\\n  .withColumn(\"day\",F.flatten(F.array(\"day\",\"boundaries\"))).drop(\"boundaries\")\\\n  .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"amount\")))\\\n  .select(\"user\",\"zip.day\", F.when(F.col(\"zip.amount\").isNull(),\\\n                                   F.lit(0)).otherwise(F.col(\"zip.amount\")).alias(\"amount\")).show()\n#+----+---+------+\n#|user|day|amount|\n#+----+---+------+\n#|   a|  2|    10|\n#|   a|  1|    14|\n#|   a|  4|     5|\n#|   a|  3|     0|\n#|   b|  1|     4|\n#|   b|  2|     0|\n#|   b|  3|     0|\n#|   b|  4|     0|\n#+----+---+------+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+------+\nuser|day|amount|\n+----+---+------+\n   b|  1|     4|\n   b|  2|     0|\n   b|  3|     0|\n   b|  4|     0|\n   a|  2|    10|\n   a|  1|    14|\n   a|  4|     5|\n   a|  3|     0|\n+----+---+------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["#+----+---+------+\n#|user|day|amount|\n#+----+---+------+\n#|   a|  2|    10|\n#|   a|  1|    14|\n#|   a|  4|     5|\n#|   a|  3|     0|\n#|   b|  1|     4|\n#|   b|  2|     0|\n#|   b|  3|     0|\n#|   b|  4|     0|\n#+----+---+------+"],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"stackhelp66","notebookId":3452041799759507},"nbformat":4,"nbformat_minor":0}
