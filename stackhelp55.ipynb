{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[\"\"\"{'id': 1, 'name': 'Slatan', 'surname': 'Yav', 'age': 24}\"\"\"],\n[\"\"\"{'id': 2, 'name': 'Nikos', 'surname': 'Stef', 'age': 34}\"\"\"],\n[\"\"\"{'id': 3, 'name': 'Panos', 'surname': 'Rodes', 'age': 44}\"\"\"]]\n\ndf=spark.createDataFrame(list,['Body_decoded'])\n\n\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------+\nBody_decoded                                             |\n+---------------------------------------------------------+\n{&#39;id&#39;: 1, &#39;name&#39;: &#39;Slatan&#39;, &#39;surname&#39;: &#39;Yav&#39;, &#39;age&#39;: 24} |\n{&#39;id&#39;: 2, &#39;name&#39;: &#39;Nikos&#39;, &#39;surname&#39;: &#39;Stef&#39;, &#39;age&#39;: 34} |\n{&#39;id&#39;: 3, &#39;name&#39;: &#39;Panos&#39;, &#39;surname&#39;: &#39;Rodes&#39;, &#39;age&#39;: 44}|\n+---------------------------------------------------------+\n\nroot\n-- Body_decoded: string (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.show() #sample dataframe\n\n#+---------------------------------------------------------+\n#|Body_decoded                                             |\n#+---------------------------------------------------------+\n#|{'id': 1, 'name': 'Slatan', 'surname': 'Yav', 'age': 24} |\n#|{'id': 2, 'name': 'Nikos', 'surname': 'Stef', 'age': 34} |\n#|{'id': 3, 'name': 'Panos', 'surname': 'Rodes', 'age': 44}|\n#+---------------------------------------------------------+\n#root\n #|-- Body_decoded: string (nullable = true) #schema\n\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nschema=MapType(StringType(),StringType())\ncolumns_selected=['name','surname']\n\ndf.withColumn(\"Body_decoded\", F.from_json(F.regexp_replace(\"Body_decoded\",'[^\"\\d,]?(\\d+)',\"'$1'\")\\\n                                              ,schema))\\\n  .select(*[\"Body_decoded.{}\".format(x) for x in columns_selected]).printSchema()\n\n#+------+-------+\n#|name  |surname|\n#+------+-------+\n#|Slatan|Yav    |\n#|Nikos |Stef   |\n#|Panos |Rodes  |\n#+------+-------+\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n        Body_decoded|\n+--------------------+\n{&#39;id&#39;: 1, &#39;name&#39;:...|\n{&#39;id&#39;: 2, &#39;name&#39;:...|\n{&#39;id&#39;: 3, &#39;name&#39;:...|\n+--------------------+\n\nroot\n-- name: string (nullable = true)\n-- surname: string (nullable = true)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["schema = StructType(\n    [\n      StructField('name', StringType(), True),\n      StructField('surname', IntegerType(), True)\n    ]\n)\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\n#schema=MapType(StringType(),StringType())\n\ndf.withColumn(\"Body_decoded\", F.from_json(F.regexp_replace(\"Body_decoded\",'[^\"\\d,]?(\\d+)',\"'$1'\")\\\n                                              ,schema))\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\nBody_decoded|\n+------------+\n        null|\n        null|\n        null|\n+------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["list=[['John'    ,  'PIPE'],\n      ['Hema'     , 'BALL-KG'],\n      ['Basha'     ,'BALL-KG'],\n      ['Hari'      ,'BALL'],\n      ['Bijju'     ,'BAG']]\n\ndf=spark.createDataFrame(list,['Name','Product'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+\n Name|Product|\n+-----+-------+\n John|   PIPE|\n Hema|BALL-KG|\nBasha|BALL-KG|\n Hari|   BALL|\nBijju|    BAG|\n+-----+-------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.withColumn(\"Product\", F.when(F.col(\"Product\")==\"BALL-KG\",F.lit(\"BALL\")).otherwise(F.col(\"Product\")))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+\n Name|Product|\n+-----+-------+\n John|   PIPE|\n Hema|   BALL|\nBasha|   BALL|\n Hari|   BALL|\nBijju|    BAG|\n+-----+-------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["list=[['2020-05-06 12:20:16','Danger Zone','iPhone11','16.17.17.17',-7,'usr1'],\n      ['2020-05-06 12:28:46','Test Zone','iPhone11','16.17.17.17',0,'usr1'],\n      ['2020-05-06 12:23:36','Test Zone','iPhone11','16.17.17.17',0,'usr1'],\n      ['2020-05-06 12:23:36','Test Zone','Samsung','18.18.17.17',0,'usr3']]\n\ndf=spark.createDataFrame(list,['Col1','Col2','Col3','Col4','Col5','Col6'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\n               Col1|       Col2|    Col3|       Col4|Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|\n2020-05-06 12:28:46|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n2020-05-06 12:23:36|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n2020-05-06 12:23:36|  Test Zone| Samsung|18.18.17.17|   0|usr3|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, -1)\nw2=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(1, 300)\ndf.withColumn(\"collect1\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w1))\\\n.withColumn(\"collect2\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w2))\\\n.withColumn(\"collect1\", F.when((F.size(\"collect1\")==0) &\\\n(F.size(\"collect2\")!=0),F.col(\"collect2\")[0])\\\n.otherwise(F.col(\"collect1\")[0])).drop(\"collect2\")\\\n.filter(\"\"\"collect1[0]!=col2 and collect1[1]=col3 and collect1[2]=col4\\\n and (collect1[3]+7=col5 or collect1[3]-7=col5)\"\"\").drop(\"collect1\")\\\n.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.17|0   |usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["\n#+-------------------+-----------+--------+-----------+----+----+\n#|               Col1|       Col2|    Col3|       Col4|Col5|Col6|\n#+-------------------+-----------+--------+-----------+----+----+\n#|2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|\n#|2020-05-06 12:28:46|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n#|2020-05-06 12:23:36|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n#|2020-05-06 12:23:36|  Test Zone| Samsung|18.18.17.17|   0|usr3|\n#+-------------------+-----------+--------+-----------+----+----+\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, -1)\nw2=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(1, 300)\ndf.withColumn(\"collect1\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w1))\\\n  .withColumn(\"collect2\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w2))\\\n  .withColumn(\"collect1\", F.when((F.size(\"collect1\")==0) &\\\n                                 (F.size(\"collect2\")!=0),F.col(\"collect2\")[0])\\\n              .otherwise(F.col(\"collect1\")[0])).drop(\"collect2\")\\\n  .filter(\"\"\"collect1[0]!=col2 and collect1[1]=col3 and collect1[2]=col4\\\n             and (collect1[3]+7=col5 or collect1[3]-7=col5)\"\"\").drop(\"collect1\")\\\n  .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.17|0   |usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["\n#+-------------------+-----------+--------+-----------+----+----+\n#|               Col1|       Col2|    Col3|       Col4|Col5|Col6|\n#+-------------------+-----------+--------+-----------+----+----+\n#|2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|\n#|2020-05-06 12:28:46|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n#|2020-05-06 12:23:36|  Test Zone|iPhone11|16.17.17.17|   0|usr1|\n#|2020-05-06 12:23:36|  Test Zone| Samsung|18.18.17.17|   0|usr3|\n#+-------------------+-----------+--------+-----------+----+----+\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, -1)\nw2=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(1, 300)\ndf.withColumn(\"collect1\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w1))\\\n  .withColumn(\"collect2\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w2))\\\n  .withColumn(\"collect1\", F.when((F.size(\"collect1\")==0) &\\\n                                 (F.size(\"collect2\")!=0),F.col(\"collect2\")).otherwise(F.col(\"collect1\"))).drop(\"collect2\")\\\n  .filter(\"\"\"exists(collect1,x-> x[0]!=col2 and x[1]==col3 and x[2]==col4 and (x[3]+7=col5 or x[3]-7=col5)=True)\"\"\")\\\n  .drop(\"collect1\").show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[".filter(\"\"\"exists(collect1,x-> x[0]!=col2 and x[1]==col3 and x[2]==col4 and (x[3]+7=col5 or x[3]-7=col5)=True)\"\"\")\\"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["w1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, -1)\nw2=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(1, 300)\ndf.withColumn(\"collect1\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w1))\\\n.withColumn(\"collect2\", F.collect_list(F.array(*[\"col2\",\"col3\",\"col4\",\"col5\"])).over(w2))\\\n.withColumn(\"collect1\", F.when((F.size(\"collect1\")==0) &\\\n(F.size(\"collect2\")!=0),F.col(\"collect2\")).otherwise(F.col(\"collect1\"))).drop(\"collect2\")\\\n.filter(\"\"\"exists(collect1,x-> x[0]!=col2 and x[1]==col3 and x[2]==col4 and (x[3]+7=col5 or x[3]-7=col5)=True)\"\"\")\\\n.drop(\"collect1\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.17|0   |usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":[".filter(\"\"\"exists(collect1,x-> x[0]!=col2 and x[1]==col3 and x[2]==col4 and (x[3]+7=col5 or x[3]-7=col5)=True)\"\"\")\\"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\ndf.withColumn(\"collect\", F.collect_list(F.struct(*[F.col(x).alias(x) for x in ['col3','col4','col5','col2']])).over(w1))\\\n  .withColumn(\"collect1\", F.collect_list(\"col5\").over(w1))\\\n  .filter(\"\"\"exists(collect,x-> x.col3==col3 and x.col4==col4)\"\"\")\\\n  .filter(\"\"\"exists(collect1,x->x=-7)=True and exists(collect1,x->x=0)=True\"\"\").drop(\"collect\",\"collect1\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.17|0   |usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#So if you look at the data only Row1 and Row3 satisfy the conditions where the logons happen within 5 mins of duration and by the same user where col5 changes from -7 to 0.All the Col3 and Col4 value remains same within 5 minutes of duration.Even when the value is -7 the Col2 value is Danger Zone in Row1"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\ndf.withColumn(\"collect\", F.collect_list(F.array(*[F.col(x) for x in ['col3','col4','col5','col2']])).over(w1))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|collect                                                                          |\n+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\n2020-05-06 12:23:36|Test Zone  |Samsung |18.18.17.17|0   |usr3|[[Samsung, 18.18.17.17, 0, Test Zone]]                                           |\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|[[iPhone11, 16.17.17.17, -7, Danger Zone], [iPhone11, 16.17.17.18, 0, Test Zone]]|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.18|0   |usr1|[[iPhone11, 16.17.17.17, -7, Danger Zone], [iPhone11, 16.17.17.18, 0, Test Zone]]|\n2020-05-06 12:28:46|Test Zone  |iPhone11|16.17.17.17|0   |usr1|[[iPhone11, 16.17.17.17, 0, Test Zone]]                                          |\n+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\ndf.withColumn(\"collect\", F.collect_list(F.array(F.struct(*[F.col(x).alias(x) for x in ['col3','col4','col5','col2']]))).over(w1))\\\n  .printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Col1: string (nullable = true)\n-- Col2: string (nullable = true)\n-- Col3: string (nullable = true)\n-- Col4: string (nullable = true)\n-- Col5: long (nullable = true)\n-- Col6: string (nullable = true)\n-- collect: array (nullable = true)\n    |-- element: array (containsNull = true)\n    |    |-- element: struct (containsNull = false)\n    |    |    |-- col3: string (nullable = true)\n    |    |    |-- col4: string (nullable = true)\n    |    |    |-- col5: long (nullable = true)\n    |    |    |-- col2: string (nullable = true)\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\ndf.withColumn(\"collect\", F.collect_list(F.array(*[F.col(x).alias(x) for x in ['col3','col4','col5','col2']])).over(w1))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\nCol1               |Col2       |Col3    |Col4       |Col5|Col6|collect                                                                          |\n+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\n2020-05-06 12:23:36|Test Zone  |Samsung |18.18.17.17|0   |usr3|[[Samsung, 18.18.17.17, 0, Test Zone]]                                           |\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|-7  |usr1|[[iPhone11, 16.17.17.17, -7, Danger Zone], [iPhone11, 16.17.17.17, 0, Test Zone]]|\n2020-05-06 12:23:36|Test Zone  |iPhone11|16.17.17.17|0   |usr1|[[iPhone11, 16.17.17.17, -7, Danger Zone], [iPhone11, 16.17.17.17, 0, Test Zone]]|\n2020-05-06 12:28:46|Test Zone  |iPhone11|16.17.17.17|0   |usr1|[[iPhone11, 16.17.17.17, 0, Test Zone]]                                          |\n+-------------------+-----------+--------+-----------+----+----+---------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\n\ndf.withColumn(\"collect\", F.collect_list(\"col5\").over(w1))\\\n  .filter(\"\"\"exists(collect,x->x=-7)=True and exists(collect,x->x=0)=True\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+-------+\n               Col1|       Col2|    Col3|       Col4|Col5|Col6|collect|\n+-------------------+-----------+--------+-----------+----+----+-------+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|[-7, 0]|\n2020-05-06 12:23:36|  Test Zone| Phone11|16.17.17.17|   0|usr1|[-7, 0]|\n+-------------------+-----------+--------+-----------+----+----+-------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\n\ndf.withColumn(\"collect\", F.collect_list(\"col5\").over(w1))\\\n  .filter(\"\"\"array_contains(collect,-7)=True and array_contains(collect,0)=True\"\"\").drop(\"collect\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\n               Col1|       Col2|    Col3|       Col4|Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|\n2020-05-06 12:23:36|  Test Zone| Phone11|16.17.17.17|   0|usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window.partitionBy(\"Col6\").orderBy(F.unix_timestamp(\"Col1\",\"yyyy-MM-dd HH:mm:ss\")).rangeBetween(-300, 300)\n\ndf.withColumn(\"collect\", F.collect_list(\"col5\").over(w1))\\\n  .filter(\"\"\"size(filter(collect,x->x!=-7 and x!=0))=size(collect)-2\"\"\").drop(\"collect\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+-----------+--------+-----------+----+----+\n               Col1|       Col2|    Col3|       Col4|Col5|Col6|\n+-------------------+-----------+--------+-----------+----+----+\n2020-05-06 12:20:16|Danger Zone|iPhone11|16.17.17.17|  -7|usr1|\n2020-05-06 12:23:36|  Test Zone| Phone11|16.17.17.17|   0|usr1|\n+-------------------+-----------+--------+-----------+----+----+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["Col1,Col2,Col3,Col4,Col5,Col6\n2020-05-06 12:20:16,Danger Zone,iPhone11,16.17.17.17,-7,usr1-----Row1\n2020-05-06 12:23:36,Test Zone,Phone11,16.17.17.17,0,us1----------Row3"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["list=[['2019-09-17',         22],\n      ['2019-09-17',          11],\n      ['2019-09-17',           9]]\n\ndf=spark.createDataFrame(list,['start_date','daypart_hour'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+\nstart_date|daypart_hour|\n+----------+------------+\n2019-09-17|          22|\n2019-09-17|          11|\n2019-09-17|           9|\n+----------+------------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["df.withColumn(\"start_dt_ts\", F.to_timestamp(F.concat(\"start_date\",\"daypart_hour\"),\"yyyy-MM-ddHH\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+-------------------+\nstart_date|daypart_hour|        start_dt_ts|\n+----------+------------+-------------------+\n2019-09-17|          22|2019-09-17 22:00:00|\n2019-09-17|          11|2019-09-17 11:00:00|\n2019-09-17|           9|2019-09-17 09:00:00|\n+----------+------------+-------------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["df.withColumn(\"start_dt_ts\", \\\n              F.date_format(F.to_timestamp(F.concat(\"start_date\",\"daypart_hour\"),\"yyyy-MM-ddHH\"),\\\n                                          \"yyyy-MM-dd hh:ss:SSa\")).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+---------------------+\nstart_date|daypart_hour|start_dt_ts          |\n+----------+------------+---------------------+\n2019-09-17|22          |2019-09-17 10:00:00PM|\n2019-09-17|11          |2019-09-17 11:00:00AM|\n2019-09-17|9           |2019-09-17 09:00:00AM|\n+----------+------------+---------------------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df1 = spark.createDataFrame(\n     [('ll',5),\n     ('yy',6)],\n     ('x','days'))\n\ndf = spark.createDataFrame(\n        [\n    ('ll','2020-01-05','1','10'),\n    ('ll','2020-01-06','1','10'),\n    ('ll','2020-01-07','1','10'),\n    ('ll','2020-01-08','1','10'),\n    ('ll','2020-01-09','1','10'),\n    ('ll','2020-01-10','1','10'),\n    ('ll','2020-01-11','1','20'),\n    ('ll','2020-01-12','1','10'),\n    ('ll','2020-01-05','2','30'),\n    ('ll','2020-01-06','2','30'),\n    ('ll','2020-01-07','2','30'),\n    ('ll','2020-01-08','2','40'),\n    ('ll','2020-01-09','2','30'),\n    ('ll','2020-01-10','2','10'),\n    ('ll','2020-01-11','2','10'),\n    ('ll','2020-01-12','2','10'),\n    ('yy','2020-01-05','1','20'),\n    ('yy','2020-01-06','1','20'),\n    ('yy','2020-01-07','1','20'),\n    ('yy','2020-01-08','1','20'),\n    ('yy','2020-01-09','1','20'),\n    ('yy','2020-01-10','1','40'),\n    ('yy','2020-01-11','1','20'),\n    ('yy','2020-01-12','1','20'),\n    ('yy','2020-01-05','2','40'),\n    ('yy','2020-01-06','2','40'),\n    ('yy','2020-01-07','2','40'),\n    ('yy','2020-01-08','2','40'),\n    ('yy','2020-01-09','2','40'),\n    ('yy','2020-01-10','2','40'),\n    ('yy','2020-01-11','2','60'),\n    ('yy','2020-01-12','2','40')],\n        ('x','date','flag','value'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["df1.show()\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+\n  x|days|\n+---+----+\n ll|   5|\n yy|   6|\n+---+----+\n\n+---+----------+----+-----+\n  x|      date|flag|value|\n+---+----------+----+-----+\n ll|2020-01-05|   1|   10|\n ll|2020-01-06|   1|   10|\n ll|2020-01-07|   1|   10|\n ll|2020-01-08|   1|   10|\n ll|2020-01-09|   1|   10|\n ll|2020-01-10|   1|   10|\n ll|2020-01-11|   1|   20|\n ll|2020-01-12|   1|   10|\n ll|2020-01-05|   2|   30|\n ll|2020-01-06|   2|   30|\n ll|2020-01-07|   2|   30|\n ll|2020-01-08|   2|   40|\n ll|2020-01-09|   2|   30|\n ll|2020-01-10|   2|   10|\n ll|2020-01-11|   2|   10|\n ll|2020-01-12|   2|   10|\n yy|2020-01-05|   1|   20|\n yy|2020-01-06|   1|   20|\n yy|2020-01-07|   1|   20|\n yy|2020-01-08|   1|   20|\n+---+----------+----+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"x\",\"flag\")\nw1=Window().partitionBy(\"x\",\"flag\").orderBy(F.to_date(\"date\",\"yyyy-dd-MM\"))\n\ndf.join(df1,['x'])\\\n  .withColumn(\"result\", F.collect_list(\"value\").over(w))\\\n  .withColumn(\"rowNum\", F.row_number().over(w1)-1)\\\n  .withColumn(\"result\", F.expr(\"\"\"aggregate(transform(result,(x,i)->array(x,i)),0,(acc,x)-> \\\n                             IF((int(x[1])>=rowNum)and(int(x[1])<days+rowNum),int(x[0])+acc,acc))\"\"\"))\\\n  .drop(\"flag\",\"rowNum\",\"days\").show()\n "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+-----+------+\n  x|      date|value|result|\n+---+----------+-----+------+\n ll|2020-01-05|   10|    50|\n ll|2020-01-06|   10|    50|\n ll|2020-01-07|   10|    60|\n ll|2020-01-08|   10|    60|\n ll|2020-01-09|   10|    50|\n ll|2020-01-10|   10|    40|\n ll|2020-01-11|   20|    30|\n ll|2020-01-12|   10|    10|\n ll|2020-01-05|   30|   160|\n ll|2020-01-06|   30|   140|\n ll|2020-01-07|   30|   120|\n ll|2020-01-08|   40|   100|\n ll|2020-01-09|   30|    60|\n ll|2020-01-10|   10|    30|\n ll|2020-01-11|   10|    20|\n ll|2020-01-12|   10|    10|\n yy|2020-01-05|   20|   140|\n yy|2020-01-06|   20|   140|\n yy|2020-01-07|   20|   140|\n yy|2020-01-08|   20|   120|\n yy|2020-01-09|   20|   100|\n yy|2020-01-10|   40|    80|\n yy|2020-01-11|   20|    40|\n yy|2020-01-12|   20|    20|\n yy|2020-01-05|   40|   240|\n yy|2020-01-06|   40|   260|\n yy|2020-01-07|   40|   260|\n yy|2020-01-08|   40|   220|\n yy|2020-01-09|   40|   180|\n yy|2020-01-10|   40|   140|\n yy|2020-01-11|   60|   100|\n yy|2020-01-12|   40|    40|\n+---+----------+-----+------+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["    +---+----------+----+-----+------+\n    |  x|      date|flag|value|result|\n    +---+----------+----+-----+------+\n    | ll|2020-01-05|   1|   10|    50|\n    | ll|2020-01-06|   1|   10|    50|\n    | ll|2020-01-07|   1|   10|    60|\n    | ll|2020-01-08|   1|   10|    60|\n    | ll|2020-01-09|   1|   10|    50|\n    | ll|2020-01-10|   1|   10|    40|\n    | ll|2020-01-11|   1|   20|    30|\n    | ll|2020-01-12|   1|   10|    10|\n    | ll|2020-01-05|   2|   30|   170|\n    | ll|2020-01-06|   2|   30|   140|\n    | ll|2020-01-07|   2|   30|   120|\n    | ll|2020-01-08|   2|   40|   100|\n    | ll|2020-01-09|   2|   30|    60|\n    | ll|2020-01-10|   2|   10|    30|\n    | ll|2020-01-11|   2|   10|    20|\n    | ll|2020-01-12|   2|   10|    10|\n    | yy|2020-01-05|   1|   20|   140|\n    | yy|2020-01-06|   1|   20|   140|\n    | yy|2020-01-07|   1|   20|   140|\n    | yy|2020-01-08|   1|   20|   120|\n    | yy|2020-01-09|   1|   20|   100|\n    | yy|2020-01-10|   1|   40|    80|\n    | yy|2020-01-11|   1|   20|    40|\n    | yy|2020-01-12|   1|   20|    20|\n    | yy|2020-01-05|   2|   40|   240|\n    | yy|2020-01-06|   2|   40|   260|\n    | yy|2020-01-07|   2|   40|   260|\n    | yy|2020-01-08|   2|   40|   220|\n    | yy|2020-01-09|   2|   40|   180|\n    | yy|2020-01-10|   2|   40|   140|\n    | yy|2020-01-11|   2|   60|   100|\n    | yy|2020-01-12|   2|   40|    40|\n    +---+----------+----+-----+------+"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["list=[[None,'val1','val2','val','A'],       \n      ['val4',None,'val5','val','B'],       \n      [None,None,'val7','val','C'],     \n      [None,'val1','val8','val','A']]   \n\ndf=spark.createDataFrame(list,['col1','col2','col3','col4','event_type'])\n\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+----------+\ncol1|col2|col3|col4|event_type|\n+----+----+----+----+----------+\nnull|val1|val2| val|         A|\nval4|null|val5| val|         B|\nnull|null|val7| val|         C|\nnull|val1|val8| val|         A|\n+----+----+----+----+----------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df.show() #sampledata\n#+----+----+----+----------+\n#|col1|col2|col3|event_type|\n#+----+----+----+----------+\n#|null|val1|val2|         A|\n#|val4|null|val5|         B|\n#|null|null|val7|         C|\n#|null|val1|val8|         A|\n#+----+----+----+----------+\n\n\nfrom pyspark.sql import functions as F\n\ndf.groupBy(\"event_type\").agg\\\n(F.first(F.concat_ws(\",\",*[(F.when(F.col(x).isNotNull(), F.lit(x)))\\\n                           for x in df.columns if x!='event_type'])).alias(\"non_null_columns\")).show()\n\n\n#+----------+----------------+\n#|event_type|non_null_columns|\n#+----------+----------------+\n#|         B|       col1,col3|\n#|         C|            col3|\n#|         A|       col2,col3|\n#+----------+----------------+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+----------+\ncol1|col2|col3|col4|event_type|\n+----+----+----+----+----------+\nnull|val1|val2| val|         A|\nval4|null|val5| val|         B|\nnull|null|val7| val|         C|\nnull|val1|val8| val|         A|\n+----+----+----+----+----------+\n\n+----------+----------------+\nevent_type|non_null_columns|\n+----------+----------------+\n         B|  col1,col3,col4|\n         C|       col3,col4|\n         A|  col2,col3,col4|\n+----------+----------------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"stackhelp55","notebookId":3353535127349970},"nbformat":4,"nbformat_minor":0}
