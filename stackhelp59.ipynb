{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nlist=[['2019-02-23','2019-02-20',          2],\n      ['2019-03-20','2019-02-20',          7],\n      ['2019-03-21', '2019-02-21',         12],\n      ['2019-03-22', '2019-02-22',         27],\n      ['2019-03-23', '2019-02-23',         91]]\n\ndf=spark.createDataFrame(list,['AsofDate','oneMonthAgo','value'])\n\ndf.show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["list=[[10      ,       'Blue'],\n      [5        ,      'Green'],\n      [21        ,     'Red']]\n\ndf=spark.createDataFrame(list,['Count','Value'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+\nCount|Value|\n+-----+-----+\n   10| Blue|\n    5|Green|\n   21|  Red|\n+-----+-----+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.withColumn(\"map\", F.create_map(\"Value\",\"Count\")).select(\"map\").toJSON().collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: [&#39;{&#34;map&#34;:{&#34;Blue&#34;:10}}&#39;, &#39;{&#34;map&#34;:{&#34;Green&#34;:5}}&#39;, &#39;{&#34;map&#34;:{&#34;Red&#34;:21}}&#39;]</div>"]}}],"execution_count":4},{"cell_type":"code","source":["print(a)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MapPartitionsRDD[1017] at toJavaRDD at NativeMethodAccessorImpl.java:0\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["cDf = spark.createDataFrame([(None, None), (1, None), (None, 2),(1,2)], (\"a\", \"b\"))\ncDf.show()\n\ncDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+\n   a|   b|\n+----+----+\nnull|null|\n   1|null|\nnull|   2|\n   1|   2|\n+----+----+\n\n+--------------+\ncoalesce(a, b)|\n+--------------+\n          null|\n             1|\n             2|\n             1|\n+--------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["list=[[ 'Copper',   '2019-01-09',        2.6945,                 2.6838],\n      ['Copper',   '2019-01-23',        2.6838,                 2.6838],\n      ['Zinc',  '2019-01-23',        1.1829,                 1.1829],\n      ['Zinc',   '2019-06-26',        1.1918,                 1.1918],\n      ['Aluminum',   '2019-01-02',        0.8363,                 0.8342],\n      ['Aluminum',   '2019-01-09',        0.8342,                 0.8342],\n      ['Aluminum',   '2019-01-23',        0.8555,                 0.8342],\n      ['Aluminum',   '2019-04-03',        0.8461,                 0.8461],\n      ['Aluminum',   '2019-05-03',        0.7461,                 0.7461],\n     ['Aluminum',   '2019-05-09',        0.9123,                 0.7461],\n     ['Aluminum',   '2019-06-15',        0.6515,                 0.6515]]\n\ndf=spark.createDataFrame(list,['material','purchase_date','mkt_prc_usd_lb','min_mkt_prc_over_1month'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-------------+--------------+-----------------------+\nmaterial|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|\n+--------+-------------+--------------+-----------------------+\n  Copper|   2019-01-09|        2.6945|                 2.6838|\n  Copper|   2019-01-23|        2.6838|                 2.6838|\n    Zinc|   2019-01-23|        1.1829|                 1.1829|\n    Zinc|   2019-06-26|        1.1918|                 1.1918|\nAluminum|   2019-01-02|        0.8363|                 0.8342|\nAluminum|   2019-01-09|        0.8342|                 0.8342|\nAluminum|   2019-01-23|        0.8555|                 0.8342|\nAluminum|   2019-04-03|        0.8461|                 0.8461|\nAluminum|   2019-05-03|        0.7461|                 0.7461|\nAluminum|   2019-05-09|        0.9123|                 0.7461|\nAluminum|   2019-06-15|        0.6515|                 0.6515|\n+--------+-------------+--------------+-----------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\ndays= lambda i: i * 86400\nw2 = (Window()\n           .partitionBy(\"material\")\n           .orderBy(col(\"purchase_date\").cast(\"timestamp\").cast(\"long\"))\n           .rangeBetween(-days(15), days(15)))\n\n\ndf.withColumn(\"first\",\\\n              expr(\"\"\"IF(mkt_prc_usd_lb=min_mkt_prc_over_1month,purchase_date,null)\"\"\"))\\\n  .withColumn(\"second\", first(\"first\", True).over(w2))\\\n  .withColumn(\"date_of_min_price\", coalesce(\"first\",\"second\"))\\\n  .show()\n\n#+--------+-------------+--------------+-----------------------+-----------------+\n#|material|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|date_of_min_price|\n#+--------+-------------+--------------+-----------------------+-----------------+\n#|  Copper|   2019-01-09|        2.6945|                 2.6838|       2019-01-23|\n#|  Copper|   2019-01-23|        2.6838|                 2.6838|       2019-01-23|\n#|    Zinc|   2019-01-23|        1.1829|                 1.1829|       2019-01-23|\n#|    Zinc|   2019-06-26|        1.1918|                 1.1918|       2019-06-26|\n#|Aluminum|   2019-01-02|        0.8363|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-01-09|        0.8342|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-01-23|        0.8555|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-04-03|        0.8461|                 0.8461|       2019-04-03|\n#+--------+-------------+--------------+-----------------------+-----------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-------------+--------------+-----------------------+----------+----------+-----------------+\nmaterial|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|     first|    second|date_of_min_price|\n+--------+-------------+--------------+-----------------------+----------+----------+-----------------+\n  Copper|   2019-01-09|        2.6945|                 2.6838|      null|2019-01-23|       2019-01-23|\n  Copper|   2019-01-23|        2.6838|                 2.6838|2019-01-23|2019-01-23|       2019-01-23|\n    Zinc|   2019-01-23|        1.1829|                 1.1829|2019-01-23|2019-01-23|       2019-01-23|\n    Zinc|   2019-06-26|        1.1918|                 1.1918|2019-06-26|2019-06-26|       2019-06-26|\nAluminum|   2019-01-02|        0.8363|                 0.8342|      null|2019-01-09|       2019-01-09|\nAluminum|   2019-01-09|        0.8342|                 0.8342|2019-01-09|2019-01-09|       2019-01-09|\nAluminum|   2019-01-23|        0.8555|                 0.8342|      null|2019-01-09|       2019-01-09|\nAluminum|   2019-04-03|        0.8461|                 0.8461|2019-04-03|2019-04-03|       2019-04-03|\nAluminum|   2019-05-03|        0.7461|                 0.7461|2019-05-03|2019-05-03|       2019-05-03|\nAluminum|   2019-05-09|        0.9123|                 0.7461|      null|2019-05-03|       2019-05-03|\nAluminum|   2019-06-15|        0.6515|                 0.6515|2019-06-15|2019-06-15|       2019-06-15|\n+--------+-------------+--------------+-----------------------+----------+----------+-----------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\ndays= lambda i: i * 86400\nw2 = (Window()\n           .partitionBy(\"material\")\n           .orderBy(col(\"purchase_date\").cast(\"timestamp\").cast(\"long\"))\n           .rangeBetween(-days(15), days(15)))\n\n\ndf.withColumn(\"first\",\\\n              expr(\"\"\"IF(mkt_prc_usd_lb=min_mkt_prc_over_1month,purchase_date,null)\"\"\"))\\\n  .withColumn(\"date_of_min_price\", first(\"first\", True).over(w2)).drop(\"first\")\\\n  .show()\n\n#+--------+-------------+--------------+-----------------------+-----------------+\n#|material|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|date_of_min_price|\n#+--------+-------------+--------------+-----------------------+-----------------+\n#|  Copper|   2019-01-09|        2.6945|                 2.6838|       2019-01-23|\n#|  Copper|   2019-01-23|        2.6838|                 2.6838|       2019-01-23|\n#|    Zinc|   2019-01-23|        1.1829|                 1.1829|       2019-01-23|\n#|    Zinc|   2019-06-26|        1.1918|                 1.1918|       2019-06-26|\n#|Aluminum|   2019-01-02|        0.8363|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-01-09|        0.8342|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-01-23|        0.8555|                 0.8342|       2019-01-09|\n#|Aluminum|   2019-04-03|        0.8461|                 0.8461|       2019-04-03|\n#+--------+-------------+--------------+-----------------------+-----------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-------------+--------------+-----------------------+-----------------+\nmaterial|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|date_of_min_price|\n+--------+-------------+--------------+-----------------------+-----------------+\n  Copper|   2019-01-09|        2.6945|                 2.6838|       2019-01-23|\n  Copper|   2019-01-23|        2.6838|                 2.6838|       2019-01-23|\n    Zinc|   2019-01-23|        1.1829|                 1.1829|       2019-01-23|\n    Zinc|   2019-06-26|        1.1918|                 1.1918|       2019-06-26|\nAluminum|   2019-01-02|        0.8363|                 0.8342|       2019-01-09|\nAluminum|   2019-01-09|        0.8342|                 0.8342|       2019-01-09|\nAluminum|   2019-01-23|        0.8555|                 0.8342|       2019-01-09|\nAluminum|   2019-04-03|        0.8461|                 0.8461|       2019-04-03|\nAluminum|   2019-05-03|        0.7461|                 0.7461|       2019-05-03|\nAluminum|   2019-05-09|        0.9123|                 0.7461|       2019-05-03|\nAluminum|   2019-06-15|        0.6515|                 0.6515|       2019-06-15|\n+--------+-------------+--------------+-----------------------+-----------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\ndays= lambda i: i * 86400\nw2 = (Window()\n           .partitionBy(\"material\")\n           .orderBy(col(\"purchase_date\").cast(\"timestamp\").cast(\"long\"))\n           .rangeBetween(-days(15), days(15)))\n\n\ndf.withColumn(\"first\",\\\n              expr(\"\"\"IF(mkt_prc_usd_lb=min_mkt_prc_over_1month,purchase_date,null)\"\"\"))\\\n  .withColumn(\"date_of_min_price\", first(\"first\", True).over(w2)).drop(\"first\")\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-------------+--------------+-----------------------+-----------------+\nmaterial|purchase_date|mkt_prc_usd_lb|min_mkt_prc_over_1month|date_of_min_price|\n+--------+-------------+--------------+-----------------------+-----------------+\n  Copper|   2019-01-09|        2.6945|                 2.6838|       2019-01-23|\n  Copper|   2019-01-23|        2.6838|                 2.6838|       2019-01-23|\n    Zinc|   2019-01-23|        1.1829|                 1.1829|       2019-01-23|\n    Zinc|   2019-06-26|        1.1918|                 1.1918|       2019-06-26|\nAluminum|   2019-01-02|        0.8363|                 0.8342|       2019-01-09|\nAluminum|   2019-01-09|        0.8342|                 0.8342|       2019-01-09|\nAluminum|   2019-01-23|        0.8555|                 0.8342|       2019-01-09|\nAluminum|   2019-04-03|        0.8461|                 0.8461|       2019-04-03|\nAluminum|   2019-05-03|        0.7461|                 0.7461|       2019-05-03|\nAluminum|   2019-05-09|        0.9123|                 0.7461|       2019-05-03|\nAluminum|   2019-06-15|        0.6515|                 0.6515|       2019-06-15|\n+--------+-------------+--------------+-----------------------+-----------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["days= lambda i: i * 86400\nw2 = (Window()\n           .partitionBy(\"material\")\n           .orderBy(col(\"purchase_date\").cast(\"timestamp\").cast(\"long\"))\n           .rangeBetween(-days(15), days(15)))\n\n\ndf.withColumn(\"date_of_min_price\",\\\n              F.when(F.col(\"mkt_prc_usd_lb\")==F.col(\"min_mkt_prc_over_1month\"),F.col(\"purchase_date\"))\\\n               .otherwise(F.first(\"date_of_min_price\", True).over(w2))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o8989.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;`date_of_min_price`&#39; given input columns: [material, purchase_date, mkt_prc_usd_lb, min_mkt_prc_over_1month];;\n&#39;Project [material#7592, purchase_date#7593, mkt_prc_usd_lb#7594, min_mkt_prc_over_1month#7595, CASE WHEN (mkt_prc_usd_lb#7594 = min_mkt_prc_over_1month#7595) THEN purchase_date#7593 ELSE first(&#39;date_of_min_price, true) windowspecdefinition(material#7592, cast(cast(purchase_date#7593 as timestamp) as bigint) ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1296000, 1296000)) END AS date_of_min_price#9583]\n+- LogicalRDD [material#7592, purchase_date#7593, mkt_prc_usd_lb#7594, min_mkt_prc_over_1month#7595], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:120)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:361)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:82)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3543)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1377)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2342)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2309)\n\tat sun.reflect.GeneratedMethodAccessor456.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4162875704520563&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> df.withColumn(&#34;date_of_min_price&#34;,\\\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>               F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;mkt_prc_usd_lb&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;min_mkt_prc_over_1month&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;purchase_date&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">---&gt; 10</span><span class=\"ansi-red-fg\">                .otherwise(F.first(&#34;date_of_min_price&#34;, True).over(w2))).show()\n</span>\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;`date_of_min_price`&#39; given input columns: [material, purchase_date, mkt_prc_usd_lb, min_mkt_prc_over_1month];;\\n&#39;Project [material#7592, purchase_date#7593, mkt_prc_usd_lb#7594, min_mkt_prc_over_1month#7595, CASE WHEN (mkt_prc_usd_lb#7594 = min_mkt_prc_over_1month#7595) THEN purchase_date#7593 ELSE first(&#39;date_of_min_price, true) windowspecdefinition(material#7592, cast(cast(purchase_date#7593 as timestamp) as bigint) ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -1296000, 1296000)) END AS date_of_min_price#9583]\\n+- LogicalRDD [material#7592, purchase_date#7593, mkt_prc_usd_lb#7594, min_mkt_prc_over_1month#7595], false\\n&#34;</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndays= lambda i: i * 86400\nw2 = (Window()\n           .partitionBy(\"material\")\n           .orderBy(col(\"purchase_date\").cast(\"timestamp\").cast(\"long\"))\n           .rangeBetween(-days(15), days(15)))\n\ndf.withColumn('d', F.expr(\"min(struct(mkt_prc_usd_lb as min_mkt_prc_over_1month, purchase_date as date_of_min_price))\").over(w2)).select(\"*\", \"d.*\").drop('d').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4162875704520561&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>            <span class=\"ansi-blue-fg\">.</span>orderBy<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;purchase_date&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;timestamp&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>cast<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;long&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>            .rangeBetween(-days(15), days(15)))\n<span class=\"ansi-green-fg\">----&gt; 8</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;d&#39;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;min(struct(mkt_prc_usd_lb as min_mkt_prc_over_1month, purchase_date as date_of_min_price))&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>over<span class=\"ansi-blue-fg\">(</span>w2<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;*&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;d.*&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;d&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    382</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    383</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 384</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    386</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o9436.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 426.0 failed 1 times, most recent failure: Lost task 1.0 in stage 426.0 (TID 4803, localhost, executor driver): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.UnsafeRow cannot be cast to java.lang.Comparable\n\tat java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:653)\n\tat java.util.PriorityQueue.siftUp(PriorityQueue.java:648)\n\tat java.util.PriorityQueue.offer(PriorityQueue.java:345)\n\tat java.util.PriorityQueue.add(PriorityQueue.java:322)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.MinMaxRemovableLike.update(MinMax.scala:50)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.MinMaxRemovableLike.update(MinMax.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:692)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.RemovableAggregateProcessor.add(RemovableAggregateProcessor.scala:161)\n\tat org.apache.spark.sql.execution.window.RemovableSlidingWindowFunctionFrame.write(WindowFunctionFrame.scala:483)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:516)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:413)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:159)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:158)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:270)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:57)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2905)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3517)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3501)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3496)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3496)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2848)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:279)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:316)\n\tat sun.reflect.GeneratedMethodAccessor435.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.UnsafeRow cannot be cast to java.lang.Comparable\n\tat java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:653)\n\tat java.util.PriorityQueue.siftUp(PriorityQueue.java:648)\n\tat java.util.PriorityQueue.offer(PriorityQueue.java:345)\n\tat java.util.PriorityQueue.add(PriorityQueue.java:322)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.MinMaxRemovableLike.update(MinMax.scala:50)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.MinMaxRemovableLike.update(MinMax.scala:35)\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.update(interfaces.scala:692)\n\tat com.databricks.sql.catalyst.expressions.aggregation.removable.RemovableAggregateProcessor.add(RemovableAggregateProcessor.scala:161)\n\tat org.apache.spark.sql.execution.window.RemovableSlidingWindowFunctionFrame.write(WindowFunctionFrame.scala:483)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:516)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:413)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:159)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:158)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["list=[['0023' ,    'g'],\n      ['0025' ,    'h'],\n      ['0026' ,    'x'],\n      ['0031' ,    'y'],\n      ['0034' ,    'z']]\n\ndf=spark.createDataFrame(list,['time','data'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+\ntime|data|\n+----+----+\n0023|   g|\n0025|   h|\n0026|   x|\n0031|   y|\n0034|   z|\n+----+----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["df.show() #sample dataframe\n#+----+----+\n#|time|data|\n#+----+----+\n#|0023|   g|\n#|0025|   h|\n#|0026|   x|\n#|0031|   y|\n#|0034|   z|\n#+----+----+\n\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\")\ndf.withColumn(\"lag1\", F.lag(\"time\").over(w)).withColumn(\"lag2\", F.lag(\"data\").over(w))\\\n  .withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"lag1\",\"lag2\")).over(w),False)).drop(\"lag1\",\"lag2\")\\\n  .withColumn(\"collect\", F.expr(\"\"\"filter(collect,(x,i)-> i<=1)\"\"\"))\\\n  .withColumn(\"collect\", F.when(F.size(\"collect\")<2, F.flatten(F.array_repeat(\"collect\",2)))\\\n                          .otherwise(F.col(\"collect\")))\\\n  .select(\"time\",\"data\",*[(F.col(\"collect\")[x][y]).alias(\"{}\".format(z))\\\n                          for x,y,z in zip([0,0,1,1],[0,1,0,1],['time1','data1','time2','data2'])])\\\n  .show(truncate=False)\n\n\n#+----+----+-----+-----+-----+-----+\n#|time|data|time1|data1|time2|data2|\n#+----+----+-----+-----+-----+-----+\n#|0023|g   |null |null |null |null |\n#|0025|h   |0023 |g    |null |null |\n#|0026|x   |0025 |h    |0023 |g    |\n#|0031|y   |0026 |x    |0025 |h    |\n#|0034|z   |0031 |y    |0026 |x    |\n#+----+----+-----+-----+-----+-----+\n\n#+----+----+-----+-----+-----+-----+\n#|time|data|time1|data1|time2|data2|\n#+----+----+-----+-----+-----+-----+\n#|0023|g   |null |null |null |null |\n#|0025|h   |0023 |g    |null |null |\n#|0026|x   |0025 |h    |0023 |g    |\n#|0031|y   |0026 |x    |0025 |h    |\n#|0034|z   |0031 |y    |0026 |x    |\n#+----+----+-----+-----+-----+-----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3228451159491666&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> w<span class=\"ansiyellow\">=</span>Window<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>orderBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;time&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 17</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>lag<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;time&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>lag<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;data&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>sort_array<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>collect_list<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>array<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>drop<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>expr<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;&quot;&quot;filter(collect,(x,i)-&gt; i&lt;=1)&quot;&quot;&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>when<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>size<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">&lt;</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>flatten<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>array_repeat<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>                          <span class=\"ansiyellow\">.</span>otherwise<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>col<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;time&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;data&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">*</span><span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>col<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span>x<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span>y<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>alias<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;{}&quot;</span><span class=\"ansiyellow\">.</span>format<span class=\"ansiyellow\">(</span>z<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>                          <span class=\"ansigreen\">for</span> x<span class=\"ansiyellow\">,</span>y<span class=\"ansiyellow\">,</span>z <span class=\"ansigreen\">in</span> zip<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;time1&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&apos;data1&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&apos;time2&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&apos;data2&apos;</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span>truncate<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     18</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     19</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">withColumn</span><span class=\"ansiblue\">(self, colName, col)</span>\n<span class=\"ansigreen\">   2012</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">   2013</span>         <span class=\"ansigreen\">assert</span> isinstance<span class=\"ansiyellow\">(</span>col<span class=\"ansiyellow\">,</span> Column<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;col should be Column&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2014</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span>colName<span class=\"ansiyellow\">,</span> col<span class=\"ansiyellow\">.</span>_jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2015</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2016</span>     <span class=\"ansiyellow\">@</span>ignore_unicode_prefix<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: &quot;The number of lambda function arguments &apos;2&apos; does not match the number of arguments expected by the higher order function &apos;1&apos;.; line 1 pos 15&quot;</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\")\ndf.withColumn(\"lag1\", F.lag(\"time\").over(w)).withColumn(\"lag2\", F.lag(\"data\").over(w))\\\n  .withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"lag1\",\"lag2\")).over(w),False)).drop(\"lag1\",\"lag2\")\\\n  .withColumn(\"collect\", F.when(F.size(\"collect\")>1,F.array(F.element_at(\"collect\",1),F.element_at(\"collect\",2)))\\\n                          .otherwise(F.col(\"collect\")))\\\n  .withColumn(\"collect\", F.when(F.size(\"collect\")<2, F.flatten(F.array_repeat(\"collect\",2)))\\\n                          .otherwise(F.col(\"collect\")))\\\n  .select(\"time\",\"data\",*[(F.col(\"collect\")[x][y]).alias(\"{}\".format(z))\\\n                          for x,y,z in zip([0,0,1,1],[0,1,0,1],['time1','data1','time2','data2'])])\\\n  .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+-----+-----+-----+-----+\ntime|data|time1|data1|time2|data2|\n+----+----+-----+-----+-----+-----+\n0023|g   |null |null |null |null |\n0025|h   |0023 |g    |null |null |\n0026|x   |0025 |h    |0023 |g    |\n0031|y   |0026 |x    |0025 |h    |\n0034|z   |0031 |y    |0026 |x    |\n+----+----+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\")\ndf.withColumn(\"lag1\", F.lag(\"time\").over(w)).withColumn(\"lag2\", F.lag(\"data\").over(w))\\\n  .withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"lag1\",\"lag2\")).over(w),False)).drop(\"lag1\",\"lag2\")\\\n  .withColumn(\"collect\", F.expr(\"\"\"filter(collect,(x,i)-> i<=1)\"\"\"))\\\n  .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4034056067542266&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> w<span class=\"ansiyellow\">=</span>Window<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>orderBy<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;time&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>df<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>lag<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;time&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>lag<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;data&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>sort_array<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>collect_list<span class=\"ansiyellow\">(</span>F<span class=\"ansiyellow\">.</span>array<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>over<span class=\"ansiyellow\">(</span>w<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>drop<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;lag1&quot;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&quot;lag2&quot;</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;collect&quot;</span><span class=\"ansiyellow\">,</span> F<span class=\"ansiyellow\">.</span>expr<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;&quot;&quot;filter(collect,(x,i)-&gt; i&lt;=1)&quot;&quot;&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>  <span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span>truncate<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">withColumn</span><span class=\"ansiblue\">(self, colName, col)</span>\n<span class=\"ansigreen\">   2012</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">   2013</span>         <span class=\"ansigreen\">assert</span> isinstance<span class=\"ansiyellow\">(</span>col<span class=\"ansiyellow\">,</span> Column<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;col should be Column&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2014</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>withColumn<span class=\"ansiyellow\">(</span>colName<span class=\"ansiyellow\">,</span> col<span class=\"ansiyellow\">.</span>_jc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2015</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2016</span>     <span class=\"ansiyellow\">@</span>ignore_unicode_prefix<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: &quot;The number of lambda function arguments &apos;2&apos; does not match the number of arguments expected by the higher order function &apos;1&apos;.; line 1 pos 15&quot;</div>"]}}],"execution_count":16},{"cell_type":"code","source":["w=Window().orderBy(\"time\").rowsBetween(-2,Window.currentRow)\ndf.withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"time\",\"data\")).over(w),False)).show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df.withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"time\",\"data\")).over(w),False)).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+-------------------------------------------------------+\ntime|data|collect                                                |\n+----+----+-------------------------------------------------------+\n0023|g   |[[0023, g]]                                            |\n0025|h   |[[0025, h], [0023, g]]                                 |\n0026|x   |[[0026, x], [0025, h], [0023, g]]                      |\n0031|y   |[[0031, y], [0026, x], [0025, h], [0023, g]]           |\n0034|z   |[[0034, z], [0031, y], [0026, x], [0025, h], [0023, g]]|\n+----+----+-------------------------------------------------------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["cols=[1,2]\n\nw=Window().orderBy(\"time\")\nw1=Window().partitionBy()\ndf.withColumn(\"lag1\", F.lag(\"time\").over(w))\\\n  .withColumn(\"lag2\", F.lag(\"data\").over(w))\\\n  .withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"lag1\",\"lag2\")).over(w),False))\\\n  .withColumn(\"max\", F.max(F.size(\"collect\")).over(w1))\\\n  .withColumn(\"size\", F.col(\"max\")-F.size(\"collect\"))\\\n  .withColumn(\"array_repeat\", F.expr(\"\"\"array_repeat(array(null,null),size-1)\"\"\"))\\\n  .withColumn(\"flatten\", F.when(F.col(\"max\")!=F.size(\"collect\"),\\\n                                F.flatten(F.array(\"collect\",\"array_repeat\")))\\\n                          .otherwise(F.expr(\"\"\"filter(collect,(x,i)-> i!=max-1)\"\"\")))\\\n  .select(\"time\",\"data\",\"flatten\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+--------------------------------------------+\ntime|data|flatten                                     |\n+----+----+--------------------------------------------+\n0023|g   |[[,], [,], [,], [,]]                        |\n0025|h   |[[0023, g], [,], [,], [,]]                  |\n0026|x   |[[0025, h], [0023, g], [,], [,]]            |\n0031|y   |[[0026, x], [0025, h], [0023, g], [,]]      |\n0034|z   |[[0031, y], [0026, x], [0025, h], [0023, g]]|\n+----+----+--------------------------------------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["df.show() #sample dataframe\n#+----+----+\n#|time|data|\n#+----+----+\n#|0023|   g|\n#|0025|   h|\n#|0026|   x|\n#|0031|   y|\n#|0034|   z|\n#+----+----+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\").rowsBetween(-2,Window.currentRow)\n\ndf.withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"time\",\"data\")).over(w),False))\n  .withColumn(\"collect\", F.expr(\"\"\"filter(collect,(x,i)-> i<=1)\"\"\"))\\\n  .withColumn(\"collect\", F.when(F.size(\"collect\")<2, F.flatten(F.array_repeat(\"collect\",2)))\\\n                          .otherwise(F.col(\"collect\")))\\\n  .select(\"time\",\"data\",(F.col(\"collect\")[0][0]).alias(\"time1\"),\\\n                         (F.col(\"collect\")[0][1]).alias(\"data1\"),\\\n                         (F.col(\"collect\")[1][0]).alias(\"time2\"),\\\n                         (F.col(\"collect\")[1][1]).alias(\"data2\"))\\\n  .show(truncate=False)\n\n\n*[F.col(\"collet\")[x][y]).alias(\"{}\".format(z)) for x,y,z in zip([0,0,1,1],[0,1,0,1],['time1','data1','time2','data2'])]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+-----+-----+-----+-----+\ntime|data|time1|data1|time2|data2|\n+----+----+-----+-----+-----+-----+\n0023|g   |null |null |null |null |\n0025|h   |0023 |g    |null |null |\n0026|x   |0025 |h    |0023 |g    |\n0031|y   |0026 |x    |0025 |h    |\n0034|z   |0031 |y    |0026 |x    |\n+----+----+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.show() #sample dataframe\n#+----+----+\n#|time|data|\n#+----+----+\n#|0023|   g|\n#|0025|   h|\n#|0026|   x|\n#|0031|   y|\n#|0034|   z|\n#+----+----+\n\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\")\ndf.withColumn(\"lag1\", F.lag(\"time\").over(w)).withColumn(\"lag2\", F.lag(\"data\").over(w))\\\n  .withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"lag1\",\"lag2\")).over(w),False)).drop(\"lag1\",\"lag2\")\\\n  .show(truncate=False)\n\n\n#+----+----+-----+-----+-----+-----+\n#|time|data|time1|data1|time2|data2|\n#+----+----+-----+-----+-----+-----+\n#|0023|g   |null |null |null |null |\n#|0025|h   |0023 |g    |null |null |\n#|0026|x   |0025 |h    |0023 |g    |\n#|0031|y   |0026 |x    |0025 |h    |\n#|0034|z   |0031 |y    |0026 |x    |\n#+----+----+-----+-----+-----+-----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+\ntime|data|\n+----+----+\n0023|   g|\n0025|   h|\n0026|   x|\n0031|   y|\n0034|   z|\n+----+----+\n\n+----+----+-------------------------------------------------+\ntime|data|collect                                          |\n+----+----+-------------------------------------------------+\n0023|g   |[[,]]                                            |\n0025|h   |[[0023, g], [,]]                                 |\n0026|x   |[[0025, h], [0023, g], [,]]                      |\n0031|y   |[[0026, x], [0025, h], [0023, g], [,]]           |\n0034|z   |[[0031, y], [0026, x], [0025, h], [0023, g], [,]]|\n+----+----+-------------------------------------------------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.show() #sample dataframe\n#+----+----+\n#|time|data|\n#+----+----+\n#|0023|   g|\n#|0025|   h|\n#|0026|   x|\n#|0031|   y|\n#|0034|   z|\n#+----+----+\n\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"time\").rowsBetween(-2,Window.currentRow)\n\n\ndf.withColumn(\"collect\", F.sort_array(F.collect_list(F.array(\"time\",\"data\")).over(w),False))\\\n  .show(truncate=False)\n\n\n#+----+----+-----+-----+-----+-----+\n#|time|data|time1|data1|time2|data2|\n#+----+----+-----+-----+-----+-----+\n#|0023|g   |null |null |null |null |\n#|0025|h   |0023 |g    |null |null |\n#|0026|x   |0025 |h    |0023 |g    |\n#|0031|y   |0026 |x    |0025 |h    |\n#|0034|z   |0031 |y    |0026 |x    |\n#+----+----+-----+-----+-----+-----+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+\ntime|data|\n+----+----+\n0023|   g|\n0025|   h|\n0026|   x|\n0031|   y|\n0034|   z|\n+----+----+\n\n+----+----+---------------------------------+\ntime|data|collect                          |\n+----+----+---------------------------------+\n0023|g   |[[0023, g]]                      |\n0025|h   |[[0025, h], [0023, g]]           |\n0026|x   |[[0026, x], [0025, h], [0023, g]]|\n0031|y   |[[0031, y], [0026, x], [0025, h]]|\n0034|z   |[[0034, z], [0031, y], [0026, x]]|\n+----+----+---------------------------------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["+----+----+-------------------------------------------------+\n|time|data|collect                                          |\n+----+----+-------------------------------------------------+\n|0023|g   |[[,]]                                            |\n|0025|h   |[[0023, g], [,]]                                 |\n|0026|x   |[[0025, h], [0023, g], [,]]                      |\n|0031|y   |[[0026, x], [0025, h], [0023, g], [,]]           |\n|0034|z   |[[0031, y], [0026, x], [0025, h], [0023, g], [,]]|\n+----+----+-------------------------------------------------+"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["time | data | time | data | time | data\n0023 |    g | null | null | null | null\n0025 |    h | 0023 |    g | null | null\n0026 |    x | 0025 |    h | 0023 |    g\n0031 |    y | 0026 |    x | 0025 |    h \n0034 |    z | 0031 |    y | 0026 |    x "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["list=[['[\"A\",\"B\"]']]\n\ndf=spark.createDataFrame(list,['col'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+\n      col|\n+---------+\n[&#34;A&#34;,&#34;B&#34;]|\n+---------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"new_col\", F.split(F.regexp_replace(\"col\", '\\[|]| |\"', ''),\",\")).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- col: string (nullable = true)\n-- new_col: array (nullable = true)\n    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["list=[[ 10 ,        10 , '11/10/20' , 'Water Polo'],\n      [ 20 ,        20 , '11/22/19' , 'Cricket']]\n\ndf1=spark.createDataFrame(list,['Latitude','Longitude','Date','Event'])\n\ndf1.show()\n\n\nlist1=[[ 20 ,        20 ,'11/10/20' ,      90],\n       [ 12 ,        12 , '11/10/20' ,      80],\n       [10 ,       10 , '11/22/19' ,      34],\n       [18 ,        18 , '11/22/19' ,      45]]\n\ndf2=spark.createDataFrame(list1,['Latitude','Longitude','Date','Weather'])\n\ndf2.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+--------+----------+\nLatitude|Longitude|    Date|     Event|\n+--------+---------+--------+----------+\n      10|       10|11/10/20|Water Polo|\n      20|       20|11/22/19|   Cricket|\n+--------+---------+--------+----------+\n\n+--------+---------+--------+-------+\nLatitude|Longitude|    Date|Weather|\n+--------+---------+--------+-------+\n      20|       20|11/10/20|     90|\n      12|       12|11/10/20|     80|\n      10|       10|11/22/19|     34|\n      18|       18|11/22/19|     45|\n+--------+---------+--------+-------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"Date\")\n\ndf1.join(df2.withColumnRenamed(\"Latitude\",\"Latitude1\")\\\n            .withColumnRenamed(\"Longitude\",\"Longitude1\"),['Date'])\\\n   .withColumn(\"Distance\",F.sqrt(F.pow(F.col(\"Latitude\")-F.col(\"Latitude1\"),2)+\\\n                                 F.pow(F.col(\"Longitude\")-F.col(\"Longitude1\"),2)))\\\n   .withColumn(\"min_Distance\", F.min(\"Distance\").over(w))\\\n   .filter('Distance=min_Distance')\\\n   .select(\"Latitude\",\"Longitude\",\"Date\",\"Weather\",\"Event\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+--------+-------+----------+\nLatitude|Longitude|    Date|Weather|     Event|\n+--------+---------+--------+-------+----------+\n      10|       10|11/10/20|     80|Water Polo|\n      20|       20|11/22/19|     45|   Cricket|\n+--------+---------+--------+-------+----------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"Date\")\n\ncond=F.min(F.sqrt(F.pow(F.col(\"Latitude\")-F.col(\"Latitude1\"),F.lit(2))+\\\n                                 F.pow(F.col(\"Longitude\")-F.col(\"Longitude1\"),F.lit(2)))).over(w)==\\\n         F.sqrt(F.pow(F.col(\"Latitude\")-F.col(\"Latitude1\"),F.lit(2))+\\\n                                 F.pow(F.col(\"Longitude\")-F.col(\"Longitude1\"),F.lit(2)))\n             \n\ndf1.join(df2.withColumnRenamed(\"Latitude\",\"Latitude1\")\\\n            .withColumnRenamed(\"Longitude\",\"Longitude1\")\\\n            .withColumnRenamed(\"Date\",\"Date1\"),cond).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2542866440325154&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     12</span> df1.join(df2.withColumnRenamed(&#34;Latitude&#34;,&#34;Latitude1&#34;)\\\n<span class=\"ansi-green-intense-fg ansi-bold\">     13</span>             <span class=\"ansi-blue-fg\">.</span>withColumnRenamed<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Longitude&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;Longitude1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">---&gt; 14</span><span class=\"ansi-red-fg\">             .withColumnRenamed(&#34;Date&#34;,&#34;Date1&#34;),cond).show()\n</span>\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    382</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    383</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 384</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    386</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3041.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 264.0 failed 1 times, most recent failure: Lost task 0.0 in stage 264.0 (TID 3970, localhost, executor driver): java.lang.UnsupportedOperationException: Cannot evaluate expression: min(SQRT((POWER(cast((input[0, bigint, true] - input[4, bigint, true]) as double), 2.0) + POWER(cast((input[1, bigint, true] - input[5, bigint, true]) as double), 2.0)))) windowspecdefinition(input[2, string, true], specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$()))\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:321)\n\tat org.apache.spark.sql.catalyst.expressions.WindowExpression.doGenCode(windowExpressions.scala:285)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:144)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1$$anonfun$apply$1.apply(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1$$anonfun$apply$1.apply(EdgeExpressionCodegen.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1.apply(EdgeExpressionCodegen.scala:244)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1.apply(EdgeExpressionCodegen.scala:244)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:242)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.streamlineIRCodeGen(EdgeExpressionCodegen.scala:293)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.streamlineCodeGen(EdgeExpressionCodegen.scala:285)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1195)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:179)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:49)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:40)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1527)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1524)\n\tat org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1.apply(CartesianProductExec.scala:84)\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1.apply(CartesianProductExec.scala:81)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:852)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:852)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:270)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:57)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2905)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3517)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3501)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3496)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1$$anonfun$apply$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:217)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:98)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:74)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:169)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3496)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2634)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2848)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:279)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:316)\n\tat sun.reflect.GeneratedMethodAccessor435.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException: Cannot evaluate expression: min(SQRT((POWER(cast((input[0, bigint, true] - input[4, bigint, true]) as double), 2.0) + POWER(cast((input[1, bigint, true] - input[5, bigint, true]) as double), 2.0)))) windowspecdefinition(input[2, string, true], specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$()))\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable$class.doGenCode(Expression.scala:321)\n\tat org.apache.spark.sql.catalyst.expressions.WindowExpression.doGenCode(windowExpressions.scala:285)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:144)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1$$anonfun$apply$1.apply(EdgeExpressionCodegen.scala:246)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1$$anonfun$apply$1.apply(EdgeExpressionCodegen.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1.apply(EdgeExpressionCodegen.scala:244)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$$anonfun$genCodeWithFallback$1.apply(EdgeExpressionCodegen.scala:244)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:242)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.streamlineIRCodeGen(EdgeExpressionCodegen.scala:293)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.streamlineCodeGen(EdgeExpressionCodegen.scala:285)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.doGenCode(EdgeExpressionCodegen.scala:1195)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCode(EdgeExpressionCodegen.scala:179)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:118)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:49)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:40)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1527)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1524)\n\tat org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:391)\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1.apply(CartesianProductExec.scala:84)\n\tat org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1.apply(CartesianProductExec.scala:81)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:852)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:852)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:353)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:317)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:537)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["list=[[7005410,               544,            1, 0.0026476993,'aa'],\n      [7005410,               549,             2,-2.6975607E-4,'aa'],\n      [7005410,               626,             3, 2.0409889E-4,'bb'],\n      [7005410,               840,             2, 3.6301462E-5,'cc'],\n      [7005410,              1192,             3, 2.2148499E-5,'dd']]\n\ndf=spark.createDataFrame(list,['partner','productId','mediumtype','prediction','yo'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---------+----------+-------------+---+\npartner|productId|mediumtype|   prediction| yo|\n+-------+---------+----------+-------------+---+\n7005410|      544|         1| 0.0026476993| aa|\n7005410|      549|         2|-2.6975607E-4| aa|\n7005410|      626|         3| 2.0409889E-4| bb|\n7005410|      840|         2| 3.6301462E-5| cc|\n7005410|     1192|         3| 2.2148499E-5| dd|\n+-------+---------+----------+-------------+---+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df.filter('yo=\"aa\"').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---------+----------+-------------+---+\npartner|productId|mediumtype|   prediction| yo|\n+-------+---------+----------+-------------+---+\n7005410|      544|         1| 0.0026476993| aa|\n7005410|      549|         2|-2.6975607E-4| aa|\n+-------+---------+----------+-------------+---+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["df.filter(\"yo='aa'\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---------+----------+-------------+---+\npartner|productId|mediumtype|   prediction| yo|\n+-------+---------+----------+-------------+---+\n7005410|      544|         1| 0.0026476993| aa|\n7005410|      549|         2|-2.6975607E-4| aa|\n+-------+---------+----------+-------------+---+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"partner\").orderBy(F.col(\"prediction\").asc())\n\ndf.withColumn(\"rowNum\", F.row_number().over(w))\\\n  .filter('rowNum<=5').drop(\"rowNum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+---------+----------+-------------+\npartner|productId|mediumtype|   prediction|\n+-------+---------+----------+-------------+\n7005410|      549|         2|-2.6975607E-4|\n7005410|     1192|         3| 2.2148499E-5|\n7005410|      840|         2| 3.6301462E-5|\n7005410|      626|         3| 2.0409889E-4|\n7005410|      544|         1| 0.0026476993|\n+-------+---------+----------+-------------+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["df.filter('(row_number() over (partition by partner order by productId)) <=5').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o348.filter.\n: org.apache.spark.sql.AnalysisException: It is not allowed to use window functions inside WHERE and HAVING clauses;\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:46)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$apply$28.applyOrElse(Analyzer.scala:2124)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$$anonfun$apply$28.applyOrElse(Analyzer.scala:2121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$.apply(Analyzer.scala:2121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ExtractWindowExpressions$.apply(Analyzer.scala:1962)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:200)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:67)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3548)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1527)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1541)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4497108577834593&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;(row_number() over (partition by partner order by productId)) &lt;=5&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">filter</span><span class=\"ansi-blue-fg\">(self, condition)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1388</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   1389</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">,</span> basestring<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1390</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1391</span>         <span class=\"ansi-green-fg\">elif</span> isinstance<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1392</span>             jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;It is not allowed to use window functions inside WHERE and HAVING clauses;&#39;</div>"]}}],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"stackhelp59","notebookId":4497108577834587},"nbformat":4,"nbformat_minor":0}
