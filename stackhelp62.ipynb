{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[1 , 'insert' , 1],\n      [3  ,    'seed'  ,1],\n      [2   , 'update'  ,1],\n      [7  ,'snapshot'  ,1],\n      [4   , 'insert'  ,2],\n      [5    ,'update'  ,2],\n      [6    ,'delete'  ,2]]\n\ndf=spark.createDataFrame(list,['a','b','c'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+---+\n  a|       b|  c|\n+---+--------+---+\n  1|  insert|  1|\n  3|    seed|  1|\n  2|  update|  1|\n  7|snapshot|  1|\n  4|  insert|  2|\n  5|  update|  2|\n  6|  delete|  2|\n+---+--------+---+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.filter(F.coalesce(*[F.when(F.col(x).like('%searchterm%'),F.lit(1)) for x in df.columns])==1).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+---+\n  a|     b|  c|\n+---+------+---+\n  1|insert|  1|\n  4|insert|  2|\n+---+------+---+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ncategories=['insert', 'seed', 'update', 'snapshot', 'delete']\n\ncols=[(F.when(F.col(\"b\")==x,F.lit(y))) for x,y in zip(categories,[x for x in (range(1, len(categories)+1))])]\n\ndf.orderBy(\"c\",F.coalesce(*cols)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+---+\n  a|       b|  c|\n+---+--------+---+\n  1|  insert|  1|\n  3|    seed|  1|\n  2|  update|  1|\n  7|snapshot|  1|\n  4|  insert|  2|\n  5|  update|  2|\n  6|  delete|  2|\n+---+--------+---+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["cols"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[72]: [Column&lt;b&#39;CASE WHEN (b = insert) THEN 1 END&#39;&gt;,\n Column&lt;b&#39;CASE WHEN (b = seed) THEN 2 END&#39;&gt;,\n Column&lt;b&#39;CASE WHEN (b = update) THEN 3 END&#39;&gt;,\n Column&lt;b&#39;CASE WHEN (b = snapshot) THEN 4 END&#39;&gt;,\n Column&lt;b&#39;CASE WHEN (b = delete) THEN 5 END&#39;&gt;]</div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\ncoalesce(CASE WHEN (b = insert) THEN 1 ELSE NULL END, CASE WHEN (b = seed) THEN 2 ELSE NULL END, CASE WHEN (b = update) THEN 3 ELSE NULL END, CASE WHEN (b = snapshot) THEN 4 ELSE NULL END, CASE WHEN (b = delete) THEN 5 ELSE NULL END)|\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n                                                                                                                                                                                                                                        1|\n                                                                                                                                                                                                                                        2|\n                                                                                                                                                                                                                                        3|\n                                                                                                                                                                                                                                        4|\n                                                                                                                                                                                                                                        1|\n                                                                                                                                                                                                                                        3|\n                                                                                                                                                                                                                                        5|\n+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.select(F.coalesce(*[(F.when(F.col(\"b\")==x,F.lit(y))) for x,y in zipd])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1420.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;coalesce()&#39; due to data type mismatch: input to function coalesce requires at least one argument;;\n&#39;Project [coalesce() AS coalesce()#117]\n+- LogicalRDD [a#65L, b#66, c#67L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:82)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3543)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1377)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1496441157636671&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>coalesce<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;b&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span>x<span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>lit<span class=\"ansi-blue-fg\">(</span>y<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> x<span class=\"ansi-blue-fg\">,</span>y <span class=\"ansi-green-fg\">in</span> zipd<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">select</span><span class=\"ansi-blue-fg\">(self, *cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1350</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">12</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">15</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1351</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1352</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jcols<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>cols<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1353</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1354</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;coalesce()&#39; due to data type mismatch: input to function coalesce requires at least one argument;;\\n&#39;Project [coalesce() AS coalesce()#117]\\n+- LogicalRDD [a#65L, b#66, c#67L], false\\n&#34;</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.orderBy(\"c\", F.coalesce(*[(F.when(F.col(\"b\")==x,y)).alias(x) for x,y in zipd])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o425.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;coalesce()&#39; due to data type mismatch: input to function coalesce requires at least one argument;;\n&#39;Sort [c#20L ASC NULLS FIRST, coalesce() ASC NULLS FIRST], true\n+- LogicalRDD [a#18L, b#19, c#20L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:200)\n\tat org.apache.spark.sql.Dataset.&lt;init&gt;(Dataset.scala:206)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:67)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3548)\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:3536)\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:1247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1496441157636670&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>orderBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;c&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>coalesce<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;b&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span>x<span class=\"ansi-blue-fg\">,</span>y<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> x<span class=\"ansi-blue-fg\">,</span>y <span class=\"ansi-green-fg\">in</span> zipd<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">sort</span><span class=\"ansi-blue-fg\">(self, *cols, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1124</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1125</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1126</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>sort<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_sort_cols<span class=\"ansi-blue-fg\">(</span>cols<span class=\"ansi-blue-fg\">,</span> kwargs<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1127</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1128</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;coalesce()&#39; due to data type mismatch: input to function coalesce requires at least one argument;;\\n&#39;Sort [c#20L ASC NULLS FIRST, coalesce() ASC NULLS FIRST], true\\n+- LogicalRDD [a#18L, b#19, c#20L], false\\n&#34;</div>"]}}],"execution_count":8},{"cell_type":"code","source":["when(col(\"b\") == \"insert\", 1)\n    .when(col(\"b\") == \"seed\", 2)\n    .when(col(\"b\") == \"update\", 3)\n    .when(col(\"b\") == \"snapshot\", 4)\n    .when(col(\"b\") == \"delete\", 5)\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["list=[[1, '01_2020',   1],\n      [1, '02_2020',   4],\n      [1, '04_2020',   2],\n      [1, '05_2020',   7],\n      [1, '09_2020',   2]]\n\ndf=spark.createDataFrame(list,['id','month_year','count'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+-----+\n id|month_year|count|\n+---+----------+-----+\n  1|   01_2020|    1|\n  1|   02_2020|    4|\n  1|   04_2020|    2|\n  1|   05_2020|    7|\n  1|   09_2020|    2|\n+---+----------+-----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["df.show() #sample dataframe\n\n#+---+----------+-----+\n#| id|month_year|count|\n#+---+----------+-----+\n#|  1|   01_2020|    1|\n#|  1|   02_2020|    4|\n#|  1|   04_2020|    2|\n#|  1|   05_2020|    7|\n#|  1|   06_2020|    2|\n#+---+----------+-----+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\").orderBy(\"month_year\")\ndf.withColumn(\"month_year\", F.to_date(\"month_year\",\"MM_yyyy\"))\\\n  .withColumn(\"lead\", F.lead(\"month_year\").over(w))\\\n  .withColumn(\"month_year\", F.when((F.col(\"lead\").isNotNull())&(F.months_between(\"lead\",\"month_year\")>1),\\\n                              F.expr(\"\"\"sequence(month_year,lead - interval 1 month,interval 1 month)\"\"\"))\\\n                             .otherwise(F.array(\"month_year\")))\\\n  .withColumn(\"count1\", F.when(F.size(\"month_year\")>1, F.expr(\"\"\"array_repeat(0,size(month_year)-1)\"\"\"))\\\n                             .otherwise(F.array()))\\\n  .withColumn(\"count\", F.flatten(F.array(F.array(\"count\"),\"count1\")))\\\n  .withColumn(\"zip\", F.explode(F.arrays_zip(\"month_year\",\"count\")))\\\n  .select(\"id\",\"zip.*\").withColumn(\"month_year\", F.date_format(\"month_year\",\"MM_yyyy\")).show()\n\n\n#+---+----------+-----+\n#|id |month_year|count|\n#+---+----------+-----+\n#|1  |01_2020   |1    |\n#|1  |02_2020   |4    |\n#|1  |03_2020   |0    |\n#|1  |04_2020   |2    |\n#|1  |05_2020   |7    |\n#|1  |06_2020   |2    |\n#+---+----------+-----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+-----+\n id|month_year|count|\n+---+----------+-----+\n  1|   01_2020|    1|\n  1|   02_2020|    4|\n  1|   04_2020|    2|\n  1|   05_2020|    7|\n  1|   09_2020|    2|\n+---+----------+-----+\n\n+---+----------+-----+\n id|month_year|count|\n+---+----------+-----+\n  1|   01_2020|    1|\n  1|   02_2020|    4|\n  1|   03_2020|    0|\n  1|   04_2020|    2|\n  1|   05_2020|    7|\n  1|   06_2020|    0|\n  1|   07_2020|    0|\n  1|   08_2020|    0|\n  1|   09_2020|    2|\n+---+----------+-----+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["\n\nlist=[[       'ABC',        '2019-10-21 09:00:02'],\n      [   'XYZ'     ,   '2019-10-21 09:15:01'],\n      [    'DEF'     ,   '2019-10-21 08:55:00'],\n    [    'ABC'        ,'2019-10-22 09:40:00'],\n     [    'XYZ'        ,'2019-10-22 07:05:01'],\n     [   'DEF'        ,'2019-10-22 08:45:00']]\n\ndf=spark.createDataFrame(list,['user_name','working_hour'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-------------------+\nuser_name|       working_hour|\n+---------+-------------------+\n      ABC|2019-10-21 09:00:02|\n      XYZ|2019-10-21 09:15:01|\n      DEF|2019-10-21 08:55:00|\n      ABC|2019-10-22 09:40:00|\n      XYZ|2019-10-22 07:05:01|\n      DEF|2019-10-22 08:45:00|\n+---------+-------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df.show() #sample dataframe\n\n#+---------+-------------------+\n#|user_name|       working_hour|\n#+---------+-------------------+\n#|      ABC|2019-10-21 09:00:02|\n#|      XYZ|2019-10-21 09:15:01|\n#|      DEF|2019-10-21 08:55:00|\n#|      ABC|2019-10-22 09:40:00|\n#|      XYZ|2019-10-22 07:05:01|\n#|      DEF|2019-10-22 08:45:00|\n#+---------+-------------------+\n\nfrom pyspark.sql import functions as F\n\ndf\\\n  .groupBy(\"user_name\").agg(F.from_unixtime(F.mean(F.unix_timestamp\\\n                        (\"working_hour\")),'hh:mm:ss').alias(\"avg\"))\\\n                        .orderBy(\"user_name\").show()\n                            \n#+---------+--------+\n#|user_name|     avg|\n#+---------+--------+\n#|      ABC|09:20:01|\n#|      DEF|08:50:00|\n#|      XYZ|08:10:01|\n#+---------+--------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-------------------+\nuser_name|       working_hour|\n+---------+-------------------+\n      ABC|2019-10-21 09:00:02|\n      XYZ|2019-10-21 09:15:01|\n      DEF|2019-10-21 08:55:00|\n      ABC|2019-10-22 09:40:00|\n      XYZ|2019-10-22 07:05:01|\n      DEF|2019-10-22 08:45:00|\n+---------+-------------------+\n\n+---------+--------+\nuser_name|     avg|\n+---------+--------+\n      ABC|09:20:01|\n      DEF|08:50:00|\n      XYZ|08:10:01|\n+---------+--------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["df1=df.select('user_name')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["df1.collect()[0][0]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: &#39;ABC&#39;</div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.show() #sample dataframe\n\n#+---------+-------------------+\n#|user_name|       working_hour|\n#+---------+-------------------+\n#|      ABC|2019-10-21 09:00:02|\n#|      XYZ|2019-10-21 09:15:01|\n#|      DEF|2019-10-21 08:55:00|\n#|      ABC|2019-10-22 09:40:00|\n#|      XYZ|2019-10-22 07:05:01|\n#|      DEF|2019-10-22 08:45:00|\n#+---------+-------------------+\n\nfrom pyspark.sql import functions as F\n\ndf.withColumn(\"working_hour\",F.date_format(F.to_timestamp(\"working_hour\",\"yyyy-MM-dd HH:mm:ss\"),'HHmmss'))\\\n  .groupBy(\"user_name\").agg(F.collect_list(\"working_hour\").alias(\"working_hour\"))\\\n  .withColumn(\"working_hour\", F.expr(\"\"\"aggregate(working_hour,cast(0 as double),\\\n                                        (acc,x)-> acc+x,acc->int(acc/size(working_hour)))\"\"\"))\\\n  .withColumn(\"avg\", F.when(F.length(F.col(\"working_hour\"))<6, F.concat(F.lit(0),F.col(\"working_hour\")))\\\n                      .otherwise(F.col(\"working_hour\")))\\\n  .withColumn(\"avg\", F.date_format(F.to_timestamp(\"avg\",\\\n                      \"HHmmss\"),\"HH:mm:ss\")).drop(\"working_hour\").orderBy(\"user_name\").show()\n#+---------+--------+\n#|user_name|     avg|\n#+---------+--------+\n#|      ABC|09:20:01|\n#|      DEF|08:50:00|\n#|      XYZ|08:10:01|\n#+---------+--------+\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["list=[['A'  ,    'a1'    ,  None  ,   's'],\n      ['A'   ,   'a2'     , 'a2'     ,  'g'],\n      ['A'    ,  None   ,'a3'       ,'m'],\n      ['B'     , 'a2'    ,  'a2'     ,  'g'],\n      ['B'      ,'a3'     , 'a3'      , 'g']]\n\ndf=spark.createDataFrame(list,['col1','col2','col3','col4'])\n\ndf.show()\n\nlist = [\"a1\", \"a2\", \"a3\", \"b4\", \"c7\"]  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+\ncol1|col2|col3|col4|\n+----+----+----+----+\n   A|  a1|null|   s|\n   A|  a2|  a2|   g|\n   A|null|  a3|   m|\n   B|  a2|  a2|   g|\n   B|  a3|  a3|   g|\n+----+----+----+----+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.show() #sample dataframe\n\n#+----+----+----+----+\n#|col1|col2|col3|col4|\n#+----+----+----+----+\n#|   A|  a1|null|   s|\n#|   A|  a2|  a2|   g|\n#|   A|null|  a3|   m|\n#|   B|  a2|  a2|   g|\n#|   B|  a3|  a3|   g|\n#+----+----+----+----+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"col1\")\nw1=Window().partitionBy(\"col1\").orderBy(F.lit(1))\n\ndf\\\n  .withColumn(\"col1_2\",F.flatten(F.collect_list(F.array(\"col2\",\"col3\")).over(w)))\\\n  .withColumn(\"list\", F.array(*[F.lit(x) for x in list]))\\\n  .withColumn(\"except\",F.array_except(\"list\",\"col1_2\"))\\\n  .withColumn(\"rowNum\", F.row_number().over(w1))\\\n  .withColumn(\"max\",F.max(\"rowNum\").over(w))\\\n  .withColumn(\"col2\", F.when(F.col(\"rowNum\")==F.col(\"max\"), F.array_union(F.array(\"col2\"),F.col(\"except\")))\\\n                       .otherwise(F.array(F.col(\"col2\"))))\\\n  .withColumn(\"col3\", F.when(F.col(\"rowNum\")==F.col(\"max\"),F.array_union(F.array(\"col3\"),F.col(\"except\")))\\\n                       .otherwise(F.array(F.col(\"col3\")))).select(*[x for x in df.columns])\\\n  .withColumn(\"col5\", F.when(F.size(\"col2\")>1, F.expr(\"\"\"array_repeat('k',size(col2)-1)\"\"\"))\\\n                           .otherwise(F.array(\"col4\")))\\\n  .withColumn(\"col4\", F.when(F.size(\"col2\")>1,F.flatten(F.array(F.array(\"col4\"),\"col5\")))\\\n                         .otherwise(F.array(\"col4\"))).drop(\"col5\")\\\n  .withColumn(\"zipped\", F.explode(F.arrays_zip(\"col2\",\"col3\",\"col4\")))\\\n  .select(\"col1\",\"zipped.*\")\\\n  .show()\n\n#+----+----+----+----+\n#|col1|col2|col3|col4|\n#+----+----+----+----+\n#|   B|  a2|  a2|   g|\n#|   B|  a3|  a3|   g|\n#|   B|  a1|  a1|   k|\n#|   B|  b4|  b4|   k|\n#|   B|  c7|  c7|   k|\n#|   A|  a1|null|   s|\n#|   A|  a2|  a2|   g|\n#|   A|null|  a3|   m|\n#|   A|  b4|  b4|   k|\n#|   A|  c7|  c7|   k|\n#+----+----+----+----+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+\ncol1|col2|col3|col4|\n+----+----+----+----+\n   B|  a2|  a2|   g|\n   B|  a3|  a3|   g|\n   B|  a1|  a1|   k|\n   B|  b4|  b4|   k|\n   B|  c7|  c7|   k|\n   A|  a1|null|   s|\n   A|  a2|  a2|   g|\n   A|null|  a3|   m|\n   A|  b4|  b4|   k|\n   A|  c7|  c7|   k|\n+----+----+----+----+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["col1  col2     col3   col4\nA      a1      null     s\nA      a2      a2       g\nA      null    a3       m\nB      a2      a2       g\nB      a3      a3       g\n\nA      b4      b4       k\nA      c7      c7       k\nB      b4      b4       k\nB      c7      c7       k"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["list=[['some_string'               ,     'A'],\n      ['another_string'           ,     'B']]\n\ndf=spark.createDataFrame(list,['a','b'])\n\ndf.show()\n\nints=[1,2,3]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+---+\n             a|  b|\n+--------------+---+\n   some_string|  A|\nanother_string|  B|\n+--------------+---+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["ints=[1,2,3]\nfrom pyspark.sql import functions as F\ndf.withColumn(\"c\", F.explode(F.array(*[F.lit(x) for x in ints]))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+---+---+\n             a|  b|  c|\n+--------------+---+---+\n   some_string|  A|  1|\n   some_string|  A|  2|\n   some_string|  A|  3|\nanother_string|  B|  1|\nanother_string|  B|  2|\nanother_string|  B|  3|\n+--------------+---+---+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["a = spark.createDataFrame([['Alice', '2020-03-03', '1'], ['Bob', '2020-03-03', '1'], ['Bob', '2020-03-05', '2']], ['name', 'dt', 'hits'])\na.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+----+\n name|        dt|hits|\n+-----+----------+----+\nAlice|2020-03-03|   1|\n  Bob|2020-03-03|   1|\n  Bob|2020-03-05|   2|\n+-----+----------+----+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\na.groupBy(\"name\").agg(F.map_from_arrays(F.collect_list(\"dt\"),\\\n                     F.collect_list(\"hits\")).alias(\"map\")).show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------------------------------+\nname |map                               |\n+-----+----------------------------------+\nBob  |[2020-03-03 -&gt; 1, 2020-03-05 -&gt; 2]|\nAlice|[2020-03-03 -&gt; 1]                 |\n+-----+----------------------------------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["+-----+------------------------------------+\n| name|    map                             |\n+-----+------------------------------------+\n|Alice|   {'2020-03-03': 1, '2020-03-05':2}|\n|  Bob|   {'2020-03-03': 1}                |\n+-----+------------------------------------+"],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"stackhelp62","notebookId":1598839203890483},"nbformat":4,"nbformat_minor":0}
