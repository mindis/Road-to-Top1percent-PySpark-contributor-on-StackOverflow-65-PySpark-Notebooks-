{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list= [['jake.whit@joker.com','Emp1','XXX'],\n      ['cali.cartel@me.com','Emp2','YYY']]\n\ndf= spark.createDataFrame(list,['f_name','l_name'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+\nf_name|l_name|\n+------+------+\n  Emp1|   XXX|\n  Emp2|   YYY|\n+------+------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.withColumn(\"f_name\", F.expr(\"\"\"rpad(f_name, length(f_name)+1,'-')\"\"\"))\\\n  .withColumn(\"l_name\", F.expr(\"\"\"rpad(l_name, length(l_name)+1,'-')\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+\nf_name|l_name|\n+------+------+\n Emp1-|  XXX-|\n Emp2-|  YYY-|\n+------+------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import col\nimport pandas as pd\n\nschema_1 = StructType([ StructField('path_name', StringType(), True),\n                           StructField('age1', IntegerType(), True), \n                           StructField('age2', IntegerType(), True), \n                           StructField('age3', IntegerType(), True)])\n   \ndata = [('dbfs/123/sd.zip',1,2,3),('dbfs/123/ab.zip',5,6,7)]\ndf = spark.createDataFrame(data,schema_1)\ndf.show()\ndef metadata(name): #function for getting metadata in a dict\n  null=[str(n.nullable) for n in name.schema.fields] #nullability\n  types=[str(i.dataType) for i in name.schema.fields] #type \n  both = [a for a in zip(types, null)]#combine type+nullability\n  names= name.columns #names of columns\n  final = {} #create dict\n  for key in names: \n     for value in both: \n          final[key] = value\n          both.remove(value)\n          break\n  return final  \nprint(metadata(df)) \n  \n  \n'''@udf(schema_1)\ndef my_udf(scheme):\n  data = getdata(scheme)\n  List_of_rows = data.collect()  \n  return List_of_rows\n\ndef somefunction(schema_1):    \n  pd_df = pd.DataFrame(['path'], columns = [\"filename\"])  \n  return (spark.createDataFrame(pd_df)\n          .withColumn('parsed', my_udf(schema_1))\n         )\n\ndf_2 = somefunction(schema_1)\n\ndisplay(df_2)'''"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+----+----+----+\n      path_name|age1|age2|age3|\n+---------------+----+----+----+\ndbfs/123/sd.zip|   1|   2|   3|\ndbfs/123/ab.zip|   5|   6|   7|\n+---------------+----+----+----+\n\n{&#39;path_name&#39;: (&#39;StringType&#39;, &#39;True&#39;), &#39;age1&#39;: (&#39;IntegerType&#39;, &#39;True&#39;), &#39;age2&#39;: (&#39;IntegerType&#39;, &#39;True&#39;), &#39;age3&#39;: (&#39;IntegerType&#39;, &#39;True&#39;)}\nOut[31]: &#39;@udf(schema_1)\\ndef my_udf(scheme):\\n  data = getdata(scheme)\\n  List_of_rows = data.collect()  \\n  return List_of_rows\\n\\ndef somefunction(schema_1):    \\n  pd_df = pd.DataFrame([\\&#39;path\\&#39;], columns = [&#34;filename&#34;])  \\n  return (spark.createDataFrame(pd_df)\\n          .withColumn(\\&#39;parsed\\&#39;, my_udf(schema_1))\\n         )\\n\\ndf_2 = somefunction(schema_1)\\n\\ndisplay(df_2)&#39;</div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"stackhelp17","notebookId":2888401447444355},"nbformat":4,"nbformat_minor":0}
