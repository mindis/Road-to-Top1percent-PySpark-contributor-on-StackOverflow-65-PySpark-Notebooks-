{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[{\"value\": {\"employee\": {\"employeeid\": \"1234\",\"employeename\": \"ABCD\",\"contactNumber\": [{\"type\": \"Work\",\"phoneNumber\": \"1234567890\"},{\"type\": \"Home\",\"phoneNumber\": \"0987654321\"}]}}}]]\n\ndf=spark.createDataFrame(list,['json'])\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- json: map (nullable = true)\n    |-- key: string\n    |-- value: map (valueContainsNull = true)\n    |    |-- key: string\n    |    |-- value: map (valueContainsNull = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["js=[[{\n        \"value\": {\n            \"employee\": {\n                 \"employeeid\": \"1234\",\n                \"employeename\": \"ABCD\",\n                 \"contactNumber\": [\n                            {\n                                \"type\": \"Work\",\n\n                                \"phoneNumber\": \"1234567890\"\n                            },\n                            {\n                                \"type\": \"Home\",\n\n                                \"phoneNumber\": \"0987654321\"\n                            }\n                        ] }}}]]\n\ndf=spark.createDataFrame(js,['json'])\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\njson                                                                                                                                                            |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n[value -&gt; [employee -&gt; [contactNumber -&gt; [{type=Work, phoneNumber=1234567890}, {type=Home, phoneNumber=0987654321}], employeeid -&gt; 1234, employeename -&gt; ABCD]]]|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nroot\n-- json: map (nullable = true)\n    |-- key: string\n    |-- value: map (valueContainsNull = true)\n    |    |-- key: string\n    |    |-- value: map (valueContainsNull = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.select(\"json.value.employee\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n            employee|\n+--------------------+\n[contactNumber -&gt;...|\n+--------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["\"`value.employee.employeeid`\",\"string\", \"employeeid\",\"string\"),(\"`value.employee.employeename`\",\"string\", \"employeename\",\"string\"), (\"`value.employee.employeename.contactNumber.value.type`\",\"string\", \"type\",\"string\"),(\"`value.employee.employeename.contactNumber.value.phoneNumber`\",\"string\", \"phoneNumber\",\"string\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["js=[[{\"value\": {\n            \"employee\": {\n                 \"employeeid\": \"1234\",\n                \"employeename\": \"ABCD\",\n                 \"contactNumber\": [\n                            {\n                                \"type\": \"Work\",\n\n                                \"phoneNumber\": \"1234567890\"\n                            },\n                            {\n                                \"type\": \"Home\",\n\n                                \"phoneNumber\": \"0987654321\"\n                            }\n                        ] }}}]]\n\ndf=spark.createDataFrame(js,['json'])\n\ndf.show()\ndf.printSchema()\n\n#+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n#|json                                                                                                                                                            |\n#+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n#|[value -> [employee -> [contactNumber -> [{type=Work, phoneNumber=1234567890}, {type=Home, phoneNumber=0987654321}], employeeid -> 1234, employeename -> ABCD]]]|\n#+----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n#root\n #|-- json: map (nullable = true)\n #|    |-- key: string\n #|    |-- value: map (valueContainsNull = true)\n #|    |    |-- key: string\n #|    |    |-- value: map (valueContainsNull = true)\n #|    |    |    |-- key: string\n #|    |    |    |-- value: string (valueContainsNull = true)\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nschema = ArrayType(MapType(StringType(),StringType()))\ndf.select(\"json.value.employee.employeeid\",\"json.value.employee.employeename\",\\\n         \"json.value.employee.contactNumber\")\\\n  .withColumn(\"contactNumber\",F.explode(F.from_json(F.regexp_replace\\\n                                          (F.regexp_replace(\"contactNumber\",\"([\\\\w-]+)\", \"\\\"$1\\\"\")\\\n                                           ,\"\\=\",\":\"),schema)))\\\n  .select(\"employeeid\",\"employeename\",\"contactNumber.type\",\"contactNumber.phoneNumber\")\\\n  .show(truncate=False)\n\n#+----------+------------+----+-----------+\n#|employeeid|employeename|type|phoneNumber|\n#+----------+------------+----+-----------+\n#|      1234|        ABCD|Work| 1234567890|\n#|      1234|        ABCD|Home| 0987654321|\n#+----------+------------+----+-----------+  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+----+-----------+\nemployeeid|employeename|type|phoneNumber|\n+----------+------------+----+-----------+\n      1234|        ABCD|Work| 1234567890|\n      1234|        ABCD|Home| 0987654321|\n+----------+------------+----+-----------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nschema = ArrayType(MapType(StringType(),StringType()))\ndf.select(\"json.value.employee.employeeid\",\"json.value.employee.employeename\",\\\n         \"json.value.employee.contactNumber\")\\\n  .withColumn(\"contactNumber\",F.explode(F.from_json(F.regexp_replace\\\n                                          (F.regexp_replace(\"contactNumber\",\"([\\\\w-]+)\", \"\\\"$1\\\"\")\\\n                                           ,\"\\=\",\":\"),schema)))\\\n  .select(\"employeeid\",\"employeename\",\"contactNumber.type\",\"contactNumber.phoneNumber\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+----+-----------+\nemployeeid|employeename|type|phoneNumber|\n+----------+------------+----+-----------+\n1234      |ABCD        |Work|1234567890 |\n1234      |ABCD        |Home|0987654321 |\n+----------+------------+----+-----------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql import functions as F\n\nschema = ArrayType(MapType(StringType(),StringType()))\ndf.select(\"json.value.employee.employeeid\",\"json.value.employee.employeename\",\\\n         \"json.value.employee.contactNumber\")\\\n  .withColumn(\"contactNumber\",F.explode(F.from_json(F.regexp_replace\\\n                                          (F.regexp_replace(\"contactNumber\",\"([\\\\w-]+)\", \"\\\"$1\\\"\")\\\n                                           ,\"\\=\",\":\"),schema)))\\\n  .select(\"employeeid\",\"employeename\",\"contactNumber.type\",\"contactNumber.phoneNumber\")\\\n  .show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+----+-----------+\nemployeeid|employeename|type|phoneNumber|\n+----------+------------+----+-----------+\n1234      |ABCD        |Work|1234567890 |\n1234      |ABCD        |Home|0987654321 |\n+----------+------------+----+-----------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.select(F.explode(\"json.value.employee.contactNumber\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o5069.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;explode(`json`[&#39;value&#39;][&#39;employee&#39;][&#39;contactNumber&#39;])&#39; due to data type mismatch: input to function explode should be array or map type, not string;;\n&#39;Project [explode(json#634[value][employee][contactNumber]) AS List()]\n+- LogicalRDD [json#634], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat sun.reflect.GeneratedMethodAccessor428.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2523395121029089&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;json.value.employee.contactNumber&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">select</span><span class=\"ansi-blue-fg\">(self, *cols)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1350</span>         <span class=\"ansi-blue-fg\">[</span>Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Alice&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">12</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> Row<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">u&#39;Bob&#39;</span><span class=\"ansi-blue-fg\">,</span> age<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">15</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1351</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">-&gt; 1352</span><span class=\"ansi-red-fg\">         </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jcols<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>cols<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1353</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1354</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;explode(`json`[&#39;value&#39;][&#39;employee&#39;][&#39;contactNumber&#39;])&#39; due to data type mismatch: input to function explode should be array or map type, not string;;\\n&#39;Project [explode(json#634[value][employee][contactNumber]) AS List()]\\n+- LogicalRDD [json#634], false\\n&#34;</div>"]}}],"execution_count":9},{"cell_type":"code","source":["dictmale={' Bariatrics ': 13, ' Cardiovascular / Pulmonary': 347, ' Neurology': 198}\ndictfemale=[{' Bariatrics ': 13, ' Cardiovascular / Pulmonary': 347, ' Neurology': 198}]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["list=[[{\"value\": {\n            \"employee\": {\n                 \"employeeid\": \"1234\",\n                \"employeename\": \"ABCD\",\n                 \"contactNumber\": \"\"\"[{\"type\": \"Work\",\"phoneNumber\": \"1234567890\"},{\"type\": \"Home\",\"phoneNumber\": \"0987654321\"}]\"\"\"}}}]]\n\ndf=spark.createDataFrame(list,['json'])\ndf.show()\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n                json|\n+--------------------+\n[value -&gt; [employ...|\n+--------------------+\n\nroot\n-- json: map (nullable = true)\n    |-- key: string\n    |-- value: map (valueContainsNull = true)\n    |    |-- key: string\n    |    |-- value: map (valueContainsNull = true)\n    |    |    |-- key: string\n    |    |    |-- value: string (valueContainsNull = true)\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nschema = ArrayType(MapType(StringType(),StringType()))\ndf.select(F.explode(\"json\").alias(\"key\",\"value\"))\\\n  .select(\"value.employee.employeeid\",\"value.employee.employeename\",\\\n          (\"value.employee.contactNumber\"))\\\n  .withColumn(\"contactNumber\", F.explode(F.from_json(\"contactNumber\",schema)))\\\n  .select(\"employeeid\",\"employeename\",\"contactNumber.type\", \"contactNumber.phoneNumber\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------+----+-----------+\nemployeeid|employeename|type|phoneNumber|\n+----------+------------+----+-----------+\n      1234|        ABCD|Work| 1234567890|\n      1234|        ABCD|Home| 0987654321|\n+----------+------------+----+-----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["c= spark.createDataFrame(\n  [\n    ('Male', [dictmale]),\n    (\"Female\",dictfemale),\n\n  ],\n  ['Gender',\"medical_specialty\"]\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["c.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-----------------------------------------------------------------------------+\nGender|medical_specialty                                                            |\n+------+-----------------------------------------------------------------------------+\nMale  |[[ Bariatrics  -&gt; 13,  Cardiovascular / Pulmonary -&gt; 347,  Neurology -&gt; 198]]|\nFemale|[[ Bariatrics  -&gt; 13,  Cardiovascular / Pulmonary -&gt; 347,  Neurology -&gt; 198]]|\n+------+-----------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["list=[[1,'John',2],\n      [2,'Maria',4],\n      [3,'Charles',6]]\n\ndf=spark.createDataFrame(list,['ID','Name','Support'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+-------+\n ID|   Name|Support|\n+---+-------+-------+\n  1|   John|      2|\n  2|  Maria|      4|\n  3|Charles|      6|\n+---+-------+-------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["df.withColumn(\"Name\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"Name\", F.explode(F.expr(\"\"\"array_repeat(Name,int(Support))\"\"\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+-------+\n ID|   Name|Support|\n+---+-------+-------+\n  1|   John|      2|\n  1|   John|      2|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n+---+-------+-------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.withColumn(\"Name\", F.explode(F.array(*[['Name']*(F.col(\"Support\"))]))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4127867316094446&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Name&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>array<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;Name&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Support&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">_</span><span class=\"ansi-blue-fg\">(self, other)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>     <span class=\"ansi-green-fg\">def</span> _<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> other<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>         jc <span class=\"ansi-blue-fg\">=</span> other<span class=\"ansi-blue-fg\">.</span>_jc <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">else</span> other\n<span class=\"ansi-green-fg\">--&gt; 115</span><span class=\"ansi-red-fg\">         </span>njc <span class=\"ansi-blue-fg\">=</span> getattr<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>njc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>     _<span class=\"ansi-blue-fg\">.</span>__doc__ <span class=\"ansi-blue-fg\">=</span> doc\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o624.multiply.\n: java.lang.RuntimeException: Unsupported literal type class java.util.ArrayList [Name]\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:168)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$$anonfun$create$2.apply(literals.scala:168)\n\tat scala.util.Try.getOrElse(Try.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.create(literals.scala:167)\n\tat org.apache.spark.sql.functions$.typedLit(functions.scala:128)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:111)\n\tat org.apache.spark.sql.Column.$times(Column.scala:719)\n\tat org.apache.spark.sql.Column.multiply(Column.scala:734)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.withColumn(\"Name\", F.explode(F.expr(\"\"\"array(Name)*Support)\"\"\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.sql.functions.expr.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nextraneous input &#39;)&#39; expecting &lt;EOF&gt;(line 1, pos 19)\n\n== SQL ==\narray(Name)*Support)\n-------------------^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:55)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:44)\n\tat com.databricks.sql.parser.DatabricksSqlParser.parseExpression(DatabricksSqlParser.scala:46)\n\tat org.apache.spark.sql.functions$.expr(functions.scala:1366)\n\tat org.apache.spark.sql.functions.expr(functions.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">ParseException</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4127867316094445&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Name&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;array(Name)*Support)&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">expr</span><span class=\"ansi-blue-fg\">(str)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    673</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    674</span>     sc <span class=\"ansi-blue-fg\">=</span> SparkContext<span class=\"ansi-blue-fg\">.</span>_active_spark_context\n<span class=\"ansi-green-fg\">--&gt; 675</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span>str<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    676</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    677</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     72</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.parser.ParseException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 73</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> ParseException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.streaming.StreamingQueryException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     75</span>                 <span class=\"ansi-green-fg\">raise</span> StreamingQueryException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">ParseException</span>: &#34;\\nextraneous input &#39;)&#39; expecting &lt;EOF&gt;(line 1, pos 19)\\n\\n== SQL ==\\narray(Name)*Support)\\n-------------------^^^\\n&#34;</div>"]}}],"execution_count":19},{"cell_type":"code","source":["df.withColumn(\"Name\", F.expr(\"\"\"repeat(concat(Name,','),Support)\"\"\"))\\\n  .withColumn(\"Name\", F.explode(F.expr(\"\"\"split(substring(Name,1,length(Name)-1),',')\"\"\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+-------+\n ID|   Name|Support|\n+---+-------+-------+\n  1|   John|      2|\n  1|   John|      2|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  2|  Maria|      4|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n  3|Charles|      6|\n+---+-------+-------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.withColumn(\"Name\", F.expr(\"\"\"repeat(concat(Name,','),Support)\"\"\"))\\\n  .withColumn(\"Name\", F.explode(F.expr(\"\"\"split(substring(Name,1,length(Name)-1),',')\"\"\"))).show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["list=[['1'],\n      ['2'],\n      ['\", Beatrice'],\n      ['123yo']]\ndf=spark.createDataFrame(list,['_c0'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n        _c0|\n+-----------+\n          1|\n          2|\n&#34;, Beatrice|\n      123yo|\n+-----------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df.withColumn(\"yo\",F.regexp_extract(\"_c0\",\"\\D+\",0)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+\n        _c0|         yo|\n+-----------+-----------+\n          1|           |\n          2|           |\n&#34;, Beatrice|&#34;, Beatrice|\n      123yo|         yo|\n+-----------+-----------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["df.filter(F.regexp_extract(\"_c0\",\"\\D+\",0)=='').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n_c0|\n+---+\n  1|\n  2|\n+---+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["import re as r\ndf.filter((F.col('_c0')).isin([x for x in range(1,r.count()+1)]))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-285087407581625&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">import</span> re <span class=\"ansi-green-fg\">as</span> r\n<span class=\"ansi-green-fg\">----&gt; 2</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;_c0&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>isin<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span>x <span class=\"ansi-green-fg\">for</span> x <span class=\"ansi-green-fg\">in</span> range<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span>r<span class=\"ansi-blue-fg\">.</span>count<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">+</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: module &#39;re&#39; has no attribute &#39;count&#39;</div>"]}}],"execution_count":25},{"cell_type":"code","source":["df.filter((F.col('_c0')).isin([x for x in range(1,df.count()+1)])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n_c0|\n+---+\n  1|\n  2|\n+---+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["[x for x in range(1,10)]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: [1, 2, 3, 4, 5, 6, 7, 8, 9]</div>"]}}],"execution_count":27},{"cell_type":"code","source":["df.filter((F.split(\"_c0\",\"\")[0]).isin([x for x in range(1,10)])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n_c0|\n+---+\n  1|\n  2|\n+---+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":[" df.withColumn(\"Name\", F.explode(F.array(F.col(\"Name\"),F.col(\"Name\"),F.col(\"Name\"))))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["list=[[1,'John'],\n      [2,'Maria'],\n      [3,'Charles']]\n\ndf=spark.createDataFrame(list,['ID','Name'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+\n ID|   Name|\n+---+-------+\n  1|   John|\n  2|  Maria|\n  3|Charles|\n+---+-------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["df.withColumn(\"ID\", F.explode(F.array(*[['ID']*3]))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+\n ID|   Name|\n+---+-------+\n  1|   John|\n  1|   John|\n  1|   John|\n  2|  Maria|\n  2|  Maria|\n  2|  Maria|\n  3|Charles|\n  3|Charles|\n  3|Charles|\n+---+-------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["df.withColumn(\"Name\", F.explode(F.array(*[['Name']*3]))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+\n ID|   Name|\n+---+-------+\n  1|   John|\n  1|   John|\n  1|   John|\n  2|  Maria|\n  2|  Maria|\n  2|  Maria|\n  3|Charles|\n  3|Charles|\n  3|Charles|\n+---+-------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["stock_prices = pd.DataFrame({\n        'date': ['2020-05-04', '2020-05-01', '2020-05-04', '2020-05-01', '2020-05-04', '2020-05-01'],\n        'stock':['AAPL', 'AAPL', 'FB', 'FB', 'MSFT', 'MSFT'],\n        'price': [290, 289, 204, 202, 177, 174]\n})\ndf=spark.createDataFrame(stock_prices)\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----+-----+\n      date|stock|price|\n+----------+-----+-----+\n2020-05-04| AAPL|  290|\n2020-05-01| AAPL|  289|\n2020-05-04|   FB|  204|\n2020-05-01|   FB|  202|\n2020-05-04| MSFT|  177|\n2020-05-01| MSFT|  174|\n+----------+-----+-----+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["list1=[['Andorra',42.506,1.5218,2,1],\n      ['Australia',-31.81,115.86,1,6],\n      ['Austria',41.597,12.580,4,9],\n      ['Belgium',21.782,1.286,2,3],\n      ['India',78.389,12.972,1,7],\n      ['Ireland',9.281,9.286,9,8],\n      ['USA',69.371,21.819,7,2]]\n\ndf1=spark.createDataFrame(list1,['country','Latitude','Longitude','col1','col2'])\n\ndf1.show()\n\nlist2=[['Australia',-31.81,115.86,2,6],\n       ['Belgium',21.782,1.286,1,6],\n       ['India',78.389,12.972,3,5],\n       ['USA',69.371,21.819,2,5]]\n\ndf2=spark.createDataFrame(list2,['country','Latitude','Longitude','col1','col2'])\n\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+---------+----+----+\n  country|Latitude|Longitude|col1|col2|\n+---------+--------+---------+----+----+\n  Andorra|  42.506|   1.5218|   2|   1|\nAustralia|  -31.81|   115.86|   1|   6|\n  Austria|  41.597|    12.58|   4|   9|\n  Belgium|  21.782|    1.286|   2|   3|\n    India|  78.389|   12.972|   1|   7|\n  Ireland|   9.281|    9.286|   9|   8|\n      USA|  69.371|   21.819|   7|   2|\n+---------+--------+---------+----+----+\n\n+---------+--------+---------+----+----+\n  country|Latitude|Longitude|col1|col2|\n+---------+--------+---------+----+----+\nAustralia|  -31.81|   115.86|   2|   6|\n  Belgium|  21.782|    1.286|   1|   6|\n    India|  78.389|   12.972|   3|   5|\n      USA|  69.371|   21.819|   2|   5|\n+---------+--------+---------+----+----+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["df1.join(df2,['Latitude','Longitude'],'left_semi').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+---------+----+----+\nLatitude|Longitude|  country|col1|col2|\n+--------+---------+---------+----+----+\n  78.389|   12.972|    India|   1|   7|\n  69.371|   21.819|      USA|   7|   2|\n  -31.81|   115.86|Australia|   1|   6|\n  21.782|    1.286|  Belgium|   2|   3|\n+--------+---------+---------+----+----+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["df2.alias(\"t2\").join(df1.alias(\"t1\"),['Latitude','Longitude'],'left').select(\"t2.*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+---------+---------+----+----+\nLatitude|Longitude|  country|col1|col2|\n+--------+---------+---------+----+----+\n  78.389|   12.972|    India|   3|   5|\n  69.371|   21.819|      USA|   2|   5|\n  -31.81|   115.86|Australia|   2|   6|\n  21.782|    1.286|  Belgium|   1|   6|\n+--------+---------+---------+----+----+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["list= [['Bob1',16,['x1','x2'],['x1','x3']],\n      ['Bob2',16,['x1','x2','x3'],[]]]\n\ndf=spark.createDataFrame(list,['Name','Age','P_Attribute','S_Attributes'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+------------+------------+\nName|Age| P_Attribute|S_Attributes|\n+----+---+------------+------------+\nBob1| 16|    [x1, x2]|    [x1, x3]|\nBob2| 16|[x1, x2, x3]|          []|\n+----+---+------------+------------+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"Attributes\", F.explode(F.array_union(F.expr(\"\"\"transform(P_Attribute,x-> struct(x as Attribute,1 as Type))\"\"\"),\\\n              F.expr(\"\"\"transform(S_Attributes,x-> struct(x as Attribute,2 as Type))\"\"\"))))\\\n   .select(\"Name\", \"Age\", \"Attributes.*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+----+\nName|Age|Attribute|Type|\n+----+---+---------+----+\nBob1| 16|       x1|   1|\nBob1| 16|       x2|   1|\nBob1| 16|       x1|   2|\nBob1| 16|       x3|   2|\nBob2| 16|       x1|   1|\nBob2| 16|       x2|   1|\nBob2| 16|       x3|   1|\n+----+---+---------+----+\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["df.withColumn(\"Attributes\", F.array_union(F.expr(\"\"\"transform(P_Attribute,x-> struct(1,x))\"\"\"),\\\n              F.expr(\"\"\"transform(S_Attributes,x-> struct(2,x))\"\"\")))\\\n   .withColumn(\"Attributes\", F.explode(\"Attributes\"))\\\n   .select(\"Name\",\"Age\", F.col(\"Attributes.x\").alias(\"Attribute\"),\\\n           F.col(\"Attributes.col1\").alias(\"Type\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+----+\nName|Age|Attribute|Type|\n+----+---+---------+----+\nBob1| 16|       x1|   1|\nBob1| 16|       x2|   1|\nBob1| 16|       x1|   2|\nBob1| 16|       x3|   2|\nBob2| 16|       x1|   1|\nBob2| 16|       x2|   1|\nBob2| 16|       x3|   1|\n+----+---+---------+----+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":["list=[['Bob1',16,['x1','x2'],['x1','x3'],[\"ab\",\"1\"],[1,2]],\n      ['Bob2',16,['x1','x2','x3'],[],[\"a\",\"b\",\"c\"],[]]]\n\ndf=spark.createDataFrame(list,['Name','Age','P_Attribute','S_Attributes','P_Values','S_values'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+------------+------------+---------+--------+\nName|Age| P_Attribute|S_Attributes| P_Values|S_values|\n+----+---+------------+------------+---------+--------+\nBob1| 16|    [x1, x2]|    [x1, x3]|  [ab, 1]|  [1, 2]|\nBob2| 16|[x1, x2, x3]|          []|[a, b, c]|      []|\n+----+---+------------+------------+---------+--------+\n\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"Attributes\", F.explode(F.array_union\\\n               (F.expr(\"\"\"transform(arrays_zip(P_Attribute,P_Values),x->\\\n                          struct(x.P_Attribute as Attribute,1 as Type,string(x.P_Values) as Value))\"\"\"),\\\n                F.expr(\"\"\"transform(arrays_zip(S_Attributes,S_Values),x->\\\n                          struct(x.S_Attributes as Attribute,2 as Type,string(x.S_Values) as Value))\"\"\"))))\\\n   .select(\"Name\", \"Age\", \"Attributes.*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+----+-----+\nName|Age|Attribute|Type|Value|\n+----+---+---------+----+-----+\nBob1| 16|       x1|   1|   ab|\nBob1| 16|       x2|   1|    1|\nBob1| 16|       x1|   2|    1|\nBob1| 16|       x3|   2|    2|\nBob2| 16|       x1|   1|    a|\nBob2| 16|       x2|   1|    b|\nBob2| 16|       x3|   1|    c|\n+----+---+---------+----+-----+\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"Attributes\", F.explode(F.array_union\\\n               (F.expr(\"\"\"transform(arrays_zip(P_Attribute,P_Values),x->\\\n                          struct(x.P_Attribute as Attribute,1 as Type,string(x.P_Values) as Value))\"\"\"),\\\n                F.expr(\"\"\"transform(arrays_zip(S_Attributes,S_Values),x->\\\n                          struct(x.S_Attributes as Attribute,2 as Type,string(x.S_Values) as Value))\"\"\"))))\\\n   .select(\"Name\", \"Age\", \"Attributes.*\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---------+----+-----+\nName|Age|Attribute|Type|Value|\n+----+---+---------+----+-----+\nBob1| 16|       x1|   1|   ab|\nBob1| 16|       x2|   1|    1|\nBob1| 16|       x1|   2|    1|\nBob1| 16|       x3|   2|    2|\nBob2| 16|       x1|   1|    a|\nBob2| 16|       x2|   1|    b|\nBob2| 16|       x3|   1|    c|\n+----+---+---------+----+-----+\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"Attributes\", F.explode(F.array_union(F.expr(\"\"\"transform(P_Attribute,x-> struct(x as Attribute,1 as Type))\"\"\"),\\\n              F.expr(\"\"\"transform(S_Attributes,x-> struct(x as Attribute,2 as Type))\"\"\"))))\\\n   .select(\"Name\", \"Age\", \"Attributes.*\").show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["df.withColumn(\"yo\", F.struct(F.lit(1),F.lit(2),F.lit('yo'))).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Name: string (nullable = true)\n-- Age: long (nullable = true)\n-- P_Attribute: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- S_Attributes: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- P_Values: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- S_values: array (nullable = true)\n    |-- element: long (containsNull = true)\n-- yo: struct (nullable = false)\n    |-- col1: integer (nullable = false)\n    |-- col2: integer (nullable = false)\n    |-- col3: string (nullable = false)\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["list= [['Bob1',16,['x1','x2'],['1','3']],\n      ['Bob2',16,['x1','x2','x3'],['4','5','7']]]\n\ndf=spark.createDataFrame(list,['Name','Age','S_Attributes','S_value'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+------------+---------+\nName|Age|S_Attributes|  S_value|\n+----+---+------------+---------+\nBob1| 16|    [x1, x2]|   [1, 3]|\nBob2| 16|[x1, x2, x3]|[4, 5, 7]|\n+----+---+------------+---------+\n\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["df.withColumn(\"yo\",F.expr(\"\"\"transform(arrays_zip(S_Attributes,S_value), x -> struct(x.S_Attributes as Attribute, x.S_value as Value, 2 as Type))\"\"\")).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Name: string (nullable = true)\n-- Age: long (nullable = true)\n-- S_Attributes: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- S_value: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- yo: array (nullable = true)\n    |-- element: struct (containsNull = false)\n    |    |-- Attribute: string (nullable = true)\n    |    |-- Value: string (nullable = true)\n    |    |-- Type: integer (nullable = false)\n\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["F.expr(\"\"\"transform(S_Attributes,S_value, x, y -> struct(x as Attribute, y as Vaue, 2 as Type))\"\"\")))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["df.withColumn(\"Attributes\", F.array_union(F.expr(\"\"\"transform(P_Attribute,x-> struct(x as Attribute,1 as Type))\"\"\"),\\\n              F.expr(\"\"\"transform(S_Attributes,x-> struct(x as Attribute,2 as Type))\"\"\")))\\\n   .withColumn(\"Attributes\", F.explode(\"Attributes\"))\\\n   .select(\"Name\", \"Age\", \"Attributes.*\").show()"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"stackhelp53","notebookId":1081392016075286},"nbformat":4,"nbformat_minor":0}
