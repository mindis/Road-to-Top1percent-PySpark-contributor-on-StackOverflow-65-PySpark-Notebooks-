{"cells":[{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql import functions as F"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[3,'B'],\n     [1,'A'],\n     [2,'A'],\n     [5,'C'],\n     [4,'A']]\n\n\nMain=spark.createDataFrame(list,['H1','H2'])\nMain.show()\n\nlist1=[[1,'A',0.5],\n      [4,'A',0.3],\n      [2,'D',0.4],\n      [3,'A',0.6]]\n\nTable2= spark.createDataFrame(list1,['H1','H2','R'])\nTable2.show()\n\n\nlist2=[['A',0.4],\n      ['B',0.6],\n      ['C',0.2],\n      ['D',0.4],\n      ['E',0.9],\n      ['F',0.1],\n      ['G',0.3]]\n\nTable3= spark.createDataFrame(list2,['H2','R'])\nTable3.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n H1| H2|\n+---+---+\n  3|  B|\n  1|  A|\n  2|  A|\n  5|  C|\n  4|  A|\n+---+---+\n\n+---+---+---+\n H1| H2|  R|\n+---+---+---+\n  1|  A|0.5|\n  4|  A|0.3|\n  2|  D|0.4|\n  3|  A|0.6|\n+---+---+---+\n\n+---+---+\n H2|  R|\n+---+---+\n  A|0.4|\n  B|0.6|\n  C|0.2|\n  D|0.4|\n  E|0.9|\n  F|0.1|\n  G|0.3|\n+---+---+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["Main.join(Table2.withColumnRenamed(\"H2\",\"H22\"), ['H1'],'left').drop(\"H22\")\\\n    .join(Table3.withColumnRenamed(\"R\",\"R2\"),['H2'])\\\n    .withColumn(\"R\", F.when(F.col(\"R\").isNull(), F.col(\"R2\")).otherwise(F.col(\"R\"))).drop(\"R2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+\n H2| H1|  R|\n+---+---+---+\n  B|  3|0.6|\n  C|  5|0.2|\n  A|  1|0.5|\n  A|  2|0.4|\n  A|  4|0.3|\n+---+---+---+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["list=[['grass','poison'],\n      ['grass','poison'],\n      ['grass','poison'],\n      ['fire',None],\n      ['fire',None],\n      ['fire','flying'],\n      ['water',None],\n      ['water',None],\n      ['water',None],\n      ['bug',None],\n      ['bug',None],\n      ['bug',None],\n      ['flying','fire']]\n\ndf=spark.createDataFrame(list,['type1','type2'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+\n type1| type2|\n+------+------+\n grass|poison|\n grass|poison|\n grass|poison|\n  fire|  null|\n  fire|  null|\n  fire|flying|\n water|  null|\n water|  null|\n water|  null|\n   bug|  null|\n   bug|  null|\n   bug|  null|\nflying|  fire|\n+------+------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.dropna().distinct().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+\n type1| type2|\n+------+------+\nflying|  fire|\n  fire|flying|\n grass|poison|\n+------+------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["list=[['PT381201021','2019-08-22','Albumin','2019-08-14','4.3',8],\n      ['PT381201021','2019-05-17','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-18','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-21','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-23','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-16','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-19','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-22','Albumin','NA','NA',0],\n      ['PT381201021','2019-05-20','Albumin','NA','NA',0]]\n\n\ndf=spark.createDataFrame(list,['ptid','blast_date','test_name','result_date','test_result','date_diff'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+---------+-----------+-----------+---------+\n       ptid|blast_date|test_name|result_date|test_result|date_diff|\n+-----------+----------+---------+-----------+-----------+---------+\nPT381201021|2019-08-22|  Albumin| 2019-08-14|        4.3|        8|\nPT381201021|2019-05-17|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-18|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-21|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-23|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-16|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-19|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-22|  Albumin|         NA|         NA|        0|\nPT381201021|2019-05-20|  Albumin|         NA|         NA|        0|\n+-----------+----------+---------+-----------+-----------+---------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["w=Window().partitionBy(\"ptid\",\"test_name\").orderBy(F.to_timestamp(\"blast_date\",\"yyyy-MM-dd\").cast(\"long\")).rangeBetween(Window.currentRow,86400*91)\n\ndf.withColumn(\"collect\", F.collect_list(F.array(\"result_date\",\"test_result\")).over(w))\\\n  .withColumn(\"collect\", F.expr(\"\"\"filter(collect,x-> array_contains(x,'NA')!=True)\"\"\")[0])\\\n  .withColumn(\"result_date\", F.when((F.col(\"result_date\")=='NA')&(F.col(\"collect\").isNotNull()),F.col(\"collect\")[0]).otherwise(F.col(\"result_date\")))\\\n  .withColumn(\"test_result\", F.when((F.col(\"test_result\")=='NA')&(F.col(\"collect\").isNotNull()),F.col(\"collect\")[1]).otherwise(F.col(\"test_result\"))).drop(\"timestamp\",\"collect\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+---------+-----------+-----------+---------+\nptid       |blast_date|test_name|result_date|test_result|date_diff|\n+-----------+----------+---------+-----------+-----------+---------+\nPT381201021|2019-05-16|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-17|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-18|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-19|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-20|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-21|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-22|Albumin  |NA         |NA         |0        |\nPT381201021|2019-05-23|Albumin  |2019-08-14 |4.3        |0        |\nPT381201021|2019-08-22|Albumin  |2019-08-14 |4.3        |8        |\n+-----------+----------+---------+-----------+-----------+---------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["TRAIT_0 = 0\nTRAIT_1 = 1\nTRAIT_2 = 2\ndef flag_to_list(flag):\n    trait_list = []\n    if flag & (1 << TRAIT_0 ):\n        trait_list.append(\"TRAIT_0\")\n    elif flag & (1 << TRAIT_1):\n        trait_list.append(\"TRAIT_1\")\n    elif flag & (1 << TRAIT_2):\n        trait_list.append(\"TRAIT_2\")\n\n    return trait_list\n  \nflag_to_list(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[63]: [&#39;TRAIT_0&#39;]</div>"]}}],"execution_count":8},{"cell_type":"code","source":["1<<TRAIT_1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[65]: 2</div>"]}}],"execution_count":9},{"cell_type":"code","source":["list=[['val1','val2','val3','val4','val5']]\n\ndf=spark.createDataFrame(list,['col1','col2','col3','col4','col5'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+----+\ncol1|col2|col3|col4|col5|\n+----+----+----+----+----+\nval1|val2|val3|val4|val5|\n+----+----+----+----+----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["[(F.lit(x),F.col(x)) for x in df.columns]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: [(Column&lt;b&#39;col1&#39;&gt;, Column&lt;b&#39;col1&#39;&gt;),\n (Column&lt;b&#39;col2&#39;&gt;, Column&lt;b&#39;col2&#39;&gt;),\n (Column&lt;b&#39;col3&#39;&gt;, Column&lt;b&#39;col3&#39;&gt;),\n (Column&lt;b&#39;col4&#39;&gt;, Column&lt;b&#39;col4&#39;&gt;),\n (Column&lt;b&#39;col5&#39;&gt;, Column&lt;b&#39;col5&#39;&gt;)]</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df.withColumn(\"mapped\", F.create_map(*[F.array(F.lit(x),F.col(x)) for x in df.columns]))\\\n  .select(F.explode(\"mapped\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o2312.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;map(array(&#39;col1&#39;, `col1`), array(&#39;col2&#39;, `col2`), array(&#39;col3&#39;, `col3`), array(&#39;col4&#39;, `col4`), array(&#39;col5&#39;, `col5`))&#39; due to data type mismatch: map expects a positive even number of arguments.;;\n&#39;Project [col1#2964, col2#2965, col3#2966, col4#2967, col5#2968, map(array(col1, col1#2964), array(col2, col2#2965), array(col3, col3#2966), array(col4, col4#2967), array(col5, col5#2968)) AS mapped#3017]\n+- LogicalRDD [col1#2964, col2#2965, col3#2966, col4#2967, col5#2968], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.GeneratedMethodAccessor378.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2216916248524503&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;mapped&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>create_map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span>F<span class=\"ansi-blue-fg\">.</span>array<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>lit<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> x <span class=\"ansi-green-fg\">in</span> df<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;mapped&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;map(array(&#39;col1&#39;, `col1`), array(&#39;col2&#39;, `col2`), array(&#39;col3&#39;, `col3`), array(&#39;col4&#39;, `col4`), array(&#39;col5&#39;, `col5`))&#39; due to data type mismatch: map expects a positive even number of arguments.;;\\n&#39;Project [col1#2964, col2#2965, col3#2966, col4#2967, col5#2968, map(array(col1, col1#2964), array(col2, col2#2965), array(col3, col3#2966), array(col4, col4#2967), array(col5, col5#2968)) AS mapped#3017]\\n+- LogicalRDD [col1#2964, col2#2965, col3#2966, col4#2967, col5#2968], false\\n&#34;</div>"]}}],"execution_count":12},{"cell_type":"code","source":["list=[[1, 'A', '01/20/2020'],\n       [1, 'B', '01/25/2020'],\n       [1, 'C', '01/25/2020'],\n       [1, 'A', '02/20/2020'],\n       [1, 'B', '02/25/2020'],\n       [1, 'C', 'NA']]\n\ndf=spark.createDataFrame(list,['id','test','date'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----------+\n id|test|      date|\n+---+----+----------+\n  1|   A|01/20/2020|\n  1|   B|01/25/2020|\n  1|   C|01/25/2020|\n  1|   A|02/20/2020|\n  1|   B|02/25/2020|\n  1|   C|        NA|\n+---+----+----------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["w=Window().partitionBy(\"id\",\"test\").orderBy(F.to_date(\"date\",\"MM/dd/yyyy\").desc())\n\ndf.withColumn(\"lag\", F.lag(\"date\").over(w)).show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----------+----------+\n id|test|      date|       lag|\n+---+----+----------+----------+\n  1|   C|01/25/2020|      null|\n  1|   C|        NA|01/25/2020|\n  1|   B|02/25/2020|      null|\n  1|   B|01/25/2020|02/25/2020|\n  1|   A|02/20/2020|      null|\n  1|   A|01/20/2020|02/20/2020|\n+---+----+----------+----------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["list=[['Bulbasaur','grass',49],\n      ['Ivysaur','grass',62],\n      ['Venusaur','grass',100],\n      ['Charmander','fire',52],\n      ['Charmeleon','fire',64],\n      ['Charizard','fire',104],\n      ['Squirtle','water',48],\n      ['Wartortle','water',63]]\n\ndf=spark.createDataFrame(list,['name','type1','attack'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-----+------+\n      name|type1|attack|\n+----------+-----+------+\n Bulbasaur|grass|    49|\n   Ivysaur|grass|    62|\n  Venusaur|grass|   100|\nCharmander| fire|    52|\nCharmeleon| fire|    64|\n Charizard| fire|   104|\n  Squirtle|water|    48|\n Wartortle|water|    63|\n+----------+-----+------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nw=Window().partitionBy(\"type1\")\ndf.withColumn(\"max\",F.max(\"attack\").over(w)).filter('attack=max').select(\"name\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+\n     name|\n+---------+\n Venusaur|\nWartortle|\nCharizard|\n+---------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["df.groupBy(\"type1\").agg(F.max(\"attack\").alias(\"attack\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+\ntype1|attack|\n+-----+------+\ngrass|   100|\nwater|    63|\n fire|   104|\n+-----+------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["import pandas as pd\nimport datetime\n\ndata = {'date': ['2014-01-01', '2014-01-02', '2014-01-03', '2014-01-04', '2014-01-05', '2014-01-06'],\n     'customerid': [2, 2, 2, 3, 4, 3], 'names': ['Andrew', 'Pete', 'Sean', 'Steve', 'Ray', 'Stef'], 'PaymentType': ['OI', 'CC', 'CC', 'OI', 'OI', 'OI']}\ndata = pd.DataFrame(data)\ndata['date'] = pd.to_datetime(data['date'])\nspark_data= spark.createDataFrame(data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["win = Window().partitionBy('customerid').orderBy((F.col('date')).cast(\"long\")).rangeBetween(-(2*86400), Window.currentRow)\n\nspark_data.withColumn(\"names_array\",\\\n                      F.collect_list(F.when(F.col(\"PaymentType\")=='OI',F.col(\"names\"))\\\n                      .otherwise(F.lit(None))).over(win)).sort(F.col(\"date\").asc()).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----------+------+-----------+-------------+\n               date|customerid| names|PaymentType|  names_array|\n+-------------------+----------+------+-----------+-------------+\n2014-01-01 00:00:00|         2|Andrew|         OI|     [Andrew]|\n2014-01-02 00:00:00|         2|  Pete|         CC|     [Andrew]|\n2014-01-03 00:00:00|         2|  Sean|         CC|     [Andrew]|\n2014-01-04 00:00:00|         3| Steve|         OI|      [Steve]|\n2014-01-05 00:00:00|         4|   Ray|         OI|        [Ray]|\n2014-01-06 00:00:00|         3|  Stef|         OI|[Steve, Stef]|\n+-------------------+----------+------+-----------+-------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["date      |customerid|PaymentType|names_array|\n2014-01-01|2         |OI         |['Andrew']\n2014-01-02|2         |CC         |['Andrew'] \n2014-01-03|2         |CC         |['Andrew']\n2014-01-04|3         |OI         |['Steve']\n2014-01-05|4         |OI         |['Ray']\n2014-01-06|3         |OI         |['Steve', 'Stef']"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["list=[[1, 'A', '01/20/2020'],\n       [1, 'B', '01/25/2020'],\n       [2, 'A', '02/20/2020'],\n       [2, 'B', '02/25/2020'],\n       [2, 'C', '02/25/2020']]\n\n\ndf=spark.createDataFrame(list,['id','test','date'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----------+\n id|test|      date|\n+---+----+----------+\n  1|   A|01/20/2020|\n  1|   B|01/25/2020|\n  2|   A|02/20/2020|\n  2|   B|02/25/2020|\n  2|   C|02/25/2020|\n+---+----+----------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["list=[[2, 'A', '02/20/2020'],\n       [2, 'B', '02/25/2020'],\n       [2, 'C', '02/25/2020']]\n\n\ndf=spark.createDataFrame(list,['id','test','date'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----------+\n id|test|      date|\n+---+----+----------+\n  2|   A|02/20/2020|\n  2|   B|02/25/2020|\n  2|   C|02/25/2020|\n+---+----+----------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["uniq_ids = df.select('id').distinct().coalesce(1)\nuniq_tests = df.select('test').distinct().coalesce(1)\nskeleton = (\n    uniq_ids.\n        crossJoin(\n            uniq_tests\n        )\n)\n(\n    skeleton.\n        join(\n            df,\n            ['id', 'test'],\n            'left'\n        ).\n        orderBy('id', 'test', 'date').\n        explain()\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nSort [id#1206L ASC NULLS FIRST, test#1207 ASC NULLS FIRST, date#1339 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(id#1206L ASC NULLS FIRST, test#1207 ASC NULLS FIRST, date#1339 ASC NULLS FIRST, 200), [id=#1110]\n   +- *(6) Project [id#1206L, test#1207, date#1339]\n      +- SortMergeJoin [id#1206L, test#1207], [id#1337L, test#1338], LeftOuter\n         :- Sort [id#1206L ASC NULLS FIRST, test#1207 ASC NULLS FIRST], false, 0\n         :  +- Exchange hashpartitioning(id#1206L, test#1207, 200), [id=#1101]\n         :     +- CartesianProduct\n         :        :- Coalesce 1\n         :        :  +- *(2) HashAggregate(keys=[id#1206L], functions=[])\n         :        :     +- Exchange hashpartitioning(id#1206L, 200), [id=#1089]\n         :        :        +- *(1) HashAggregate(keys=[id#1206L], functions=[])\n         :        :           +- *(1) Project [id#1206L]\n         :        :              +- *(1) Scan ExistingRDD[id#1206L,test#1207,date#1208]\n         :        +- Coalesce 1\n         :           +- *(4) HashAggregate(keys=[test#1207], functions=[])\n         :              +- Exchange hashpartitioning(test#1207, 200), [id=#1095]\n         :                 +- *(3) HashAggregate(keys=[test#1207], functions=[])\n         :                    +- *(3) Project [test#1207]\n         :                       +- *(3) Scan ExistingRDD[id#1206L,test#1207,date#1208]\n         +- Sort [id#1337L ASC NULLS FIRST, test#1338 ASC NULLS FIRST], false, 0\n            +- Exchange hashpartitioning(id#1337L, test#1338, 200), [id=#1104]\n               +- *(5) Filter (isnotnull(id#1337L) &amp;&amp; isnotnull(test#1338))\n                  +- *(5) Scan ExistingRDD[id#1337L,test#1338,date#1339]\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["df.groupBy(\"id\").agg(F.collect_list(\"test\").alias(\"x\"),F.collect_list(\"date\").alias(\"col2\"))\\\n               .withColumn(\"zip\", F.arrays_zip(F.col(\"x\"),F.col(\"col2\")))\\\n               .withColumn(\"except\", F.array_except(F.array(*(F.lit(x) for x in ['A','B','C'])),\"x\")).drop(\"x\",\"col2\")\\\n               .withColumn(\"except\", F.expr(\"\"\"transform(except,x-> struct(x,'NA'))\"\"\"))\\\n               .withColumn(\"zipped\", F.explode(F.array_union(\"zip\",\"except\")))\\\n               .select(\"id\",F.col(\"zipped.x\").alias(\"test\"),F.col(\"zipped.col2\").alias(\"date\"))\\\n               .explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Project [id#1206L, zipped#1400.x AS test#1405, zipped#1400.col2 AS date#1406]\n+- *(2) Generate explode(array_union(zip#1380, except#1394)), [id#1206L], false, [zipped#1400]\n   +- *(2) Project [id#1206L, arrays_zip(x#1374, col2#1376) AS zip#1380, transform(array_except([A,B,C], x#1374), lambdafunction(named_struct(x, lambda x#1395, col2, NA), lambda x#1395, false)) AS except#1394]\n      +- ObjectHashAggregate(keys=[id#1206L], functions=[collect_list(test#1207, 0, 0), collect_list(date#1208, 0, 0)])\n         +- Exchange hashpartitioning(id#1206L, 200), [id=#1192]\n            +- *(1) Project [id#1206L, date#1208, test#1207]\n               +- *(1) Scan ExistingRDD[id#1206L,test#1207,date#1208]\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["list=[['B', 'UES', 5,1],\n       ['B', 'MID', 10,2],\n       ['B', 'UWS', 4,3],\n       ['BR', 'EV', 1,1],\n       ['BR', 'WB', 4,2],\n       ['BR', 'MID', 5,3]]\n\ndf=spark.createDataFrame(list,['Borough','Neighborhood','Count','Row_Number'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------+-----+----------+\nBorough|Neighborhood|Count|Row_Number|\n+-------+------------+-----+----------+\n      B|         UES|    5|         1|\n      B|         MID|   10|         2|\n      B|         UWS|    4|         3|\n     BR|          EV|    1|         1|\n     BR|          WB|    4|         2|\n     BR|         MID|    5|         3|\n+-------+------------+-----+----------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["df.groupBy(\"Borough\").pivot(\"Neighborhood\").agg(F.first(\"Count\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----+---+----+----+----+\nBorough|  EV|MID| UES| UWS|  WB|\n+-------+----+---+----+----+----+\n      B|null| 10|   5|   4|null|\n     BR|   1|  5|null|null|   4|\n+-------+----+---+----+----+----+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf.withColumn(\"neighbour_dummy\", F.concat_ws(\"_\", F.lit(\"Neighbourhood\"), F.col(\"Row_Number\")))\\\n  .groupBy(\"Borough\").pivot(\"neighbour_dummy\").agg(F.first(\"Neighborhood\").alias(\"\"),\\\n                                                  F.first(\"count\").alias(\"count\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----------------+---------------------+----------------+---------------------+----------------+---------------------+\nBorough|Neighbourhood_1_|Neighbourhood_1_count|Neighbourhood_2_|Neighbourhood_2_count|Neighbourhood_3_|Neighbourhood_3_count|\n+-------+----------------+---------------------+----------------+---------------------+----------------+---------------------+\n      B|             UES|                    5|             MID|                   10|             UWS|                    4|\n     BR|              EV|                    1|              WB|                    4|             MID|                    5|\n+-------+----------------+---------------------+----------------+---------------------+----------------+---------------------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndf.withColumn(\"row_num\", F.row_number().over(Window.partitionBy(\"Borough\").orderBy(\"Count\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------+-----+-------+---------------+\nBorough|Neighborhood|Count|row_num|neighbour_dummy|\n+-------+------------+-----+-------+---------------+\n      B|         UWS|    4|      1|    Neighbour_1|\n      B|         UES|    5|      2|    Neighbour_2|\n      B|         MID|   10|      3|    Neighbour_3|\n     BR|          EV|    1|      1|    Neighbour_1|\n     BR|          WB|    4|      2|    Neighbour_2|\n     BR|         MID|    5|      3|    Neighbour_3|\n+-------+------------+-----+-------+---------------+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["pivot_neigh.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------+-----------+-----------+\nBorough|Neighbour_1|Neighbour_2|Neighbour_3|\n+-------+-----------+-----------+-----------+\n      B|        UWS|        UES|        MID|\n     BR|         EV|         WB|        MID|\n+-------+-----------+-----------+-----------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["list=[['Jan-17']]\n\ndf=spark.createDataFrame(list,['t'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\n     t|\n+------+\nJan-17|\n+------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndf.select(to_timestamp('t',\"MMM-yy HH:mm:ss\").alias(\"dt\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\n  dt|\n+----+\nnull|\n+----+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["list=[[0,'Town Street','01-07-2017 08:08:00'],\n      [0,'Town Street','01-07-2017 08:13:00'],\n      [0,'Town Street','01-07-2017 08:18:00'],\n      [0,'Town Street','01-07-2017 08:27:00'],\n      [0,'Town Street','01-07-2017 08:45:00'],\n      [0,'Town Street','01-07-2017 08:49:00'],\n      [0,'Town Street','01-08-2017 08:10:00'],\n      [0,'Town Street','01-08-2017 08:15:00']]\n\ndf=spark.createDataFrame(list,['col1','col2','timestamp'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----------+-------------------+\ncol1|       col2|          timestamp|\n+----+-----------+-------------------+\n   0|Town Street|01-07-2017 08:08:00|\n   0|Town Street|01-07-2017 08:13:00|\n   0|Town Street|01-07-2017 08:18:00|\n   0|Town Street|01-07-2017 08:27:00|\n   0|Town Street|01-07-2017 08:45:00|\n   0|Town Street|01-07-2017 08:49:00|\n   0|Town Street|01-08-2017 08:10:00|\n   0|Town Street|01-08-2017 08:15:00|\n+----+-----------+-------------------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["w=Window().partitionBy(\"date\").orderBy(F.col(\"timestamp\"))\nw2=Window().partitionBy(\"date\",\"interval\")\nw3=Window().partitionBy(\"date\",\"interval\").orderBy(F.col(\"timestamp\"))\n\ndf.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\",'MM-dd-yyyy HH:mm:ss'))\\\n  .withColumn(\"date\", F.to_date(\"timestamp\"))\\\n  .withColumn(\"lag\", F.lag(\"timestamp\").over(w))\\\n  .withColumn(\"diff\", F.when(F.col(\"lag\").isNotNull(),F.col(\"timestamp\").cast(\"long\")-F.col(\"lag\").cast(\"long\"))\\\n                       .otherwise(F.lit(None)))\\\n  .withColumn(\"interval\",F.sum(F.when(F.col(\"diff\")/300>1, F.lit(1)).otherwise(F.lit(0))).over(w))\\\n  .withColumn(\"collect\", F.collect_list(\"timestamp\").over(w2)).drop(\"lag\",\"diff\")\\\n  .withColumn(\"rownum\",F.row_number().over(w3)).filter('rownum=1 and size(collect)>1')\\\n  .select(\"date\", F.array(F.date_format(\"timestamp\", \"HH:mm:ss\"),F.size(\"collect\")).alias(\"Time\"))\\\n  .orderBy(\"date\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+\ndate      |Time         |\n+----------+-------------+\n2017-01-07|[08:08:00, 3]|\n2017-01-07|[08:45:00, 2]|\n2017-01-08|[08:10:00, 2]|\n+----------+-------------+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["w=Window().partitionBy(\"date\").orderBy(F.col(\"timestamp\"))\nw2=Window().partitionBy(\"date\",\"interval\")\nw3=Window().partitionBy(\"date\",\"interval\").orderBy(F.col(\"timestamp\"))\n\ndf.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\",'MM-dd-yyyy HH:mm:ss'))\\\n  .withColumn(\"date\", F.to_date(\"timestamp\"))\\\n  .withColumn(\"lag\", F.lag(\"timestamp\").over(w))\\\n  .withColumn(\"diff\", F.when(F.col(\"lag\").isNotNull(),F.col(\"timestamp\").cast(\"long\")-F.col(\"lag\").cast(\"long\"))\\\n                       .otherwise(F.lit(None)))\\\n  .withColumn(\"interval\",F.sum(F.when(F.col(\"diff\")/300>1, F.lit(1)).otherwise(F.lit(0))).over(w))\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----------+-------------------+----------+-------------------+----+--------+\ncol1|       col2|          timestamp|      date|                lag|diff|interval|\n+----+-----------+-------------------+----------+-------------------+----+--------+\n   0|Town Street|2017-01-08 08:10:00|2017-01-08|               null|null|       0|\n   0|Town Street|2017-01-08 08:15:00|2017-01-08|2017-01-08 08:10:00| 300|       0|\n   0|Town Street|2017-01-07 08:08:00|2017-01-07|               null|null|       0|\n   0|Town Street|2017-01-07 08:13:00|2017-01-07|2017-01-07 08:08:00| 300|       0|\n   0|Town Street|2017-01-07 08:18:00|2017-01-07|2017-01-07 08:13:00| 300|       0|\n   0|Town Street|2017-01-07 08:27:00|2017-01-07|2017-01-07 08:18:00| 540|       1|\n   0|Town Street|2017-01-07 08:45:00|2017-01-07|2017-01-07 08:27:00|1080|       2|\n+----+-----------+-------------------+----------+-------------------+----+--------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"stackhelp51","notebookId":3340375955522295},"nbformat":4,"nbformat_minor":0}
