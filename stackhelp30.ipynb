{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nlist=[[1, 2019, 1, 4],\n      [1, 2019, 2, 6],\n      [1, 2019, 3, 4],\n      [1, 2019, 4, 2],\n      [2, 2019, 1, 2],\n      [2, 2019, 5, 3],\n      [3, 2019, 1, 9]]\n\n\ndf=spark.createDataFrame(list,['Customer_ID', 'Purchase_Year', 'Purchase_Month', 'Purchases'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------------+--------------+---------+\nCustomer_ID|Purchase_Year|Purchase_Month|Purchases|\n+-----------+-------------+--------------+---------+\n          1|         2019|             1|        4|\n          1|         2019|             2|        6|\n          1|         2019|             3|        4|\n          1|         2019|             4|        2|\n          2|         2019|             1|        2|\n          2|         2019|             5|        3|\n          3|         2019|             1|        9|\n+-----------+-------------+--------------+---------+\n\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\ndays= lambda i: i * 86400\nw=Window().partitionBy(\"Customer_ID\").orderBy(\"sec\").rangeBetween(-days(89),0)\ndf.withColumn(\"sec\", F.to_timestamp(F.concat(\"Purchase_Year\",\"Purchase_Month\"),\"yyyyM\").cast(\"long\"))\\\n  .withColumn(\"L3\", F.sum(\"Purchases\").over(w)).orderBy(\"Customer_ID\",\"Purchase_Month\").drop(\"sec\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------------+--------------+---------+---+\nCustomer_ID|Purchase_Year|Purchase_Month|Purchases| L3|\n+-----------+-------------+--------------+---------+---+\n          1|         2019|             1|        4|  4|\n          1|         2019|             2|        6| 10|\n          1|         2019|             3|        4| 14|\n          1|         2019|             4|        2| 12|\n          2|         2019|             1|        2|  2|\n          2|         2019|             5|        3|  3|\n          3|         2019|             1|        9|  9|\n+-----------+-------------+--------------+---------+---+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["Customer_ID, Purchase_Year, Purchase_Month, Purchases, L3M\n   1             2019             1              4     4\n   1             2019             2              6     10\n   1             2019             3              4     14\n   1             2019             4              2     12\n   2             2019             1              2     2\n   2             2019             5              3     3\n   3             2019             1              9     9"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["cschema = StructType([StructField(\"items\",ArrayType(StructType([StructField(\"name\", StringType()),\n                      StructField(\"quantity\", StringType())])))])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\ncschema = StructType([StructField(\"items\",ArrayType(StructType([StructField(\"name\", StringType()),\n                      StructField(\"quantity\", StringType())])))])\nlist=[[[['A','1'], ['B', '1'], ['C', '2']]]]\ndf=spark.createDataFrame(list,cschema)\ndf.show(truncate=False)\ndf.printSchema()\ndf.coalesce(1).rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------+\nitems                   |\n+------------------------+\n[[A, 1], [B, 1], [C, 2]]|\n+------------------------+\n\nroot\n-- items: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- name: string (nullable = true)\n    |    |-- quantity: string (nullable = true)\n\nOut[16]: 1</div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.withColumn(\"items\", F.regexp_replace(F.expr(\"\"\"transform(items, x-> concat_ws(', ',x.name,x.quantity))\"\"\").cast(\"string\"),\"\\[|]\",\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\n           items|\n+----------------+\nA, 1, B, 1, C, 2|\n+----------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.withColumn(\"item2\", F.array_join(F.expr(\"\"\"transform(items, x-> concat_ws(',',x.name,x.quantity))\"\"\"),', ')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------+\n               items|        item2|\n+--------------------+-------------+\n[[A, 1], [B, 1], ...|A,1, B,1, C,2|\n+--------------------+-------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["df.withColumn(\"items\", F.expr(\"array_join(transform(items, \\\n                                i -> concat_ws(',', i.name, i.quantity)), ',')\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n      items|\n+-----------+\nA,1,B,1,C,2|\n+-----------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["list=[['a','s3://path1/path6/yyy.json.gz'],\n      ['b','s3://path3/path7/xxx.json.gz'],\n      ['c','s3://path3/path8/aaa.json.gz'],\n      ['c','s3://path4/path9/bbb.json.gz']]\ndf=spark.createDataFrame(list,['id','S3location'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------------------+\n id|          S3location|\n+---+--------------------+\n  a|s3://path1/path6/...|\n  b|s3://path3/path7/...|\n  c|s3://path3/path8/...|\n  c|s3://path4/path9/...|\n+---+--------------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["paths=[]\nfor i in df.collect():\n  paths.append(i[1])\npaths"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: [&#39;s3://path1/path6/yyy.json.gz&#39;,\n &#39;s3://path3/path7/xxx.json.gz&#39;,\n &#39;s3://path3/path8/aaa.json.gz&#39;,\n &#39;s3://path4/path9/bbb.json.gz&#39;]</div>"]}}],"execution_count":11},{"cell_type":"code","source":["list=[['1,val1, val4'],\n        ['2,val1'],\n        ['3,val1, val2, val3'],\n        ['4,val1, val2, val3, val4']]\n\ndf=spark.createDataFrame(list,['col1'])\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------+\ncol1                    |\n+------------------------+\n1,val1, val4            |\n2,val1                  |\n3,val1, val2, val3      |\n4,val1, val2, val3, val4|\n+------------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df.withColumn(\"col3\", F.split(\"col1\",',')).withColumn(\"col1\", F.element_at(\"col3\",1)).withColumn(\"col2\", F.expr(\"\"\"filter(col3, x-> x!=col1)\"\"\")).drop(\"col3\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------------------------+\ncol1|col2                       |\n+----+---------------------------+\n1   |[val1,  val4]              |\n2   |[val1]                     |\n3   |[val1,  val2,  val3]       |\n4   |[val1,  val2,  val3,  val4]|\n+----+---------------------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.types import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["df.withColumn(\"col3\", F.split(\"col1\",',')).withColumn(\"col1\", F.element_at(\"col3\",1)).withColumn(\"col2\", F.array_except(\"col3\",(F.array(F.col(\"col1\"))))).drop(\"col3\").printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- col1: string (nullable = true)\n-- col2: array (nullable = true)\n    |-- element: string (containsNull = true)\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["+----+------------------------+\n|col1|col2                    |\n+----+------------------------+\n|   1|[val1, val4]            |\n|   2|[val1]                  |\n|   3|[val1, val2, val3]      |\n|   4|[val1, val2, val3, val4]|\n+----+------------------------+"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["df.withColumn(\"col3\", F.regexp_extract(\"col1\",r'(\\d+)',1)).show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1440734316831576&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col3&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>regexp_extract<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;col1&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">r&#39;(\\d+)&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> __repr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3215.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 112.0 failed 1 times, most recent failure: Lost task 2.0 in stage 112.0 (TID 308, localhost, executor driver): java.lang.IndexOutOfBoundsException: No group 2\n\tat java.util.regex.Matcher.group(Matcher.java:538)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:159)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:158)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2581)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:270)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:57)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2890)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3508)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:112)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:241)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:171)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2833)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:266)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:303)\n\tat sun.reflect.GeneratedMethodAccessor378.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IndexOutOfBoundsException: No group 2\n\tat java.util.regex.Matcher.group(Matcher.java:538)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:640)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:159)\n\tat org.apache.spark.sql.execution.collect.Collector$$anonfun$2.apply(Collector.scala:158)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:533)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:539)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.withColumn(\"col3\", F.split(\"col1\",',')).withColumn(\"col1\", F.regexp_extract(\"col1\",r'(\\d+)',1)).withColumn(\"col2\", F.expr(\"\"\"filter(col3,x-> x!=col1)\"\"\")).drop(\"col3\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------------------------+\ncol1|col2                       |\n+----+---------------------------+\n1   |[val1,  val4]              |\n2   |[val1]                     |\n3   |[val1,  val2,  val3]       |\n4   |[val1,  val2,  val3,  val4]|\n+----+---------------------------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["list=[[1, '20180101', 'A', 1],\n       [1, '20180102', 'A', 2],\n       [1, '20180103', 'B', 1],\n       [1, '20180104', 'A', 1],\n       [1, '20180105', 'A',1],\n       [1, '20180106', 'A',1],\n       [2, '20180101', 'C', 1],\n       [2, '20180102', 'D', 1],\n       [2, '20180103', 'D', 2],\n       [2, '20180103', 'D', 3]]\ndf= spark.createDataFrame(list,['id', 'date', 'item', 'streak'])\ndf=df.drop(\"streak\")\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+----+\n id|    date|item|\n+---+--------+----+\n  1|20180101|   A|\n  1|20180102|   A|\n  1|20180103|   B|\n  1|20180104|   A|\n  1|20180105|   A|\n  1|20180106|   A|\n  2|20180101|   C|\n  2|20180102|   D|\n  2|20180103|   D|\n  2|20180103|   D|\n+---+--------+----+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["w=Window().partitionBy(\"id\",\"item\").orderBy(\"date\")\ndf.withColumn(\"rownum\",F.row_number().over(w)).withColumn(\"lag\", F.lag(\"date\").over(w))\\\n.withColumn(\"lag2\", F.lag(\"rownum\").over(w))\\\n.withColumn(\"almost\", F.when((F.col(\"date\")-F.col(\"lag\"))>1,F.lit(1)).otherwise(F.col(\"rownum\")))\\\n.withColumn(\"almost2\", F.lag(\"almost\").over(w)).orderBy(\"id\",\"date\")\\\n.withColumn(\"almost2\", F.when(F.col(\"almost2\").isNull(),F.lit(0)).otherwise(F.col(\"almost2\")))\\\n.withColumn(\"streak\", F.when(F.col(\"almost\")-F.col(\"almost2\")>1,F.col(\"almost2\")+1).otherwise(F.col(\"almost\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+----+------+--------+----+------+-------+------+\n id|    date|item|rownum|     lag|lag2|almost|almost2|streak|\n+---+--------+----+------+--------+----+------+-------+------+\n  1|20180101|   A|     1|    null|null|     1|      0|     1|\n  1|20180102|   A|     2|20180101|   1|     2|      1|     2|\n  1|20180103|   B|     1|    null|null|     1|      0|     1|\n  1|20180104|   A|     3|20180102|   2|     1|      2|     1|\n  1|20180105|   A|     4|20180104|   3|     4|      1|     2|\n  1|20180106|   A|     5|20180105|   4|     5|      4|     5|\n  2|20180101|   C|     1|    null|null|     1|      0|     1|\n  2|20180102|   D|     1|    null|null|     1|      0|     1|\n  2|20180103|   D|     2|20180102|   1|     2|      1|     2|\n  2|20180103|   D|     3|20180103|   2|     3|      2|     3|\n+---+--------+----+------+--------+----+------+-------+------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["w=Window().partitionBy(\"id\",\"item\").orderBy(\"date\")\nw2=Window().partitionBy(\"id\",\"item\").orderBy(F.col(\"date\").cast(\"long\")).rangeBetween(-1,0)\n                                                        \ndf.withColumn(\"d\", F.count(\"item\").over(w2))\\\n.withColumn(\"lag\", F.lag(\"date\").over(w))\\\n.orderBy(\"id\",\"date\").show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+----+---+--------+\n id|    date|item|  d|     lag|\n+---+--------+----+---+--------+\n  1|20180101|   A|  1|    null|\n  1|20180102|   A|  2|20180101|\n  1|20180103|   B|  1|    null|\n  1|20180104|   A|  1|20180102|\n  1|20180105|   A|  2|20180104|\n  1|20180106|   A|  2|20180105|\n  2|20180101|   C|  1|    null|\n  2|20180102|   D|  1|    null|\n  2|20180103|   D|  3|20180102|\n  2|20180103|   D|  3|20180103|\n+---+--------+----+---+--------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["id, date, item, streak\n\n1, 20180101, A, 1\n1, 20180102, A, 2\n1, 20180103, B, 1\n1, 20180104, A, 1\n2, 20180101, C, 1\n2, 20180102, D, 1\n2, 20180103, D, 2\n2, 20180103, D, 3"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"stackhelp30","notebookId":198294997913462},"nbformat":4,"nbformat_minor":0}
