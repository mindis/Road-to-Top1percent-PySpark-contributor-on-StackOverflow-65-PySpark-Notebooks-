{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list1=[[1],[2],[3],[4],[5],[6],[7],[8],[9]]\ndf1=spark.createDataFrame(list1, ['customer_id'])\n\n\nlist2=[[1, '2019-04-08',20],\n       [1, '2019-02-04',25],\n       [1, '2019-01-04',26],\n       [2, '2020-02-04',27],\n       [2,'2019-01-08',30],\n       [2, '2019-01-03',31],\n       [3, '2020-08-01',32],\n       [3, '2019-01-04',41],\n       [4, '2020-01-08',45],\n       [5,'2021-04-01',49],\n       [6,'2019-04-01',51],\n       [6,'2019-02-01',62]]\ndf2=spark.createDataFrame(list2, ['customer_id', 'created_at','amount'])\n\ndf1.show()\ndf2.show()\n\ndf1.createOrReplaceTempView(\"customers\")\ndf2.withColumn(\"created_at\", F.to_timestamp(\"created_at\")).createOrReplaceTempView(\"loans\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\ncustomer_id|\n+-----------+\n          1|\n          2|\n          3|\n          4|\n          5|\n          6|\n          7|\n          8|\n          9|\n+-----------+\n\n+-----------+----------+------+\ncustomer_id|created_at|amount|\n+-----------+----------+------+\n          1|2019-04-08|    20|\n          1|2019-02-04|    25|\n          1|2019-01-04|    26|\n          2|2020-02-04|    27|\n          2|2019-01-08|    30|\n          2|2019-01-03|    31|\n          3|2020-08-01|    32|\n          3|2019-01-04|    41|\n          4|2020-01-08|    45|\n          5|2021-04-01|    49|\n          6|2019-04-01|    51|\n          6|2019-02-01|    62|\n+-----------+----------+------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["spark.sql(\"\"\"select a.customer_id from customers a join (select customer_id, count(*) from loans group by customer_id having count(*)>1) b on a.customer_id=b.customer_id\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\ncustomer_id|\n+-----------+\n          6|\n          1|\n          3|\n          2|\n+-----------+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["spark.sql(\"\"\"select a.customer_id from customers a join (select customer_id, count(*) from loans group by customer_id having count(*)>1) b on a.customer_id=b.customer_id\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\ncustomer_id|\n+-----------+\n          6|\n          1|\n          3|\n          2|\n+-----------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["spark.sql(\"\"\"select a.customer_id, b.amount from customers a join (select amount, customer_id, year(created_at) as YEAR, (row_number() over (partition by customer_id order by created_at)) as RN from loans) b on a.customer_id=b.customer_id where YEAR=2019 and RN=1 \"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+------+\ncustomer_id|amount|\n+-----------+------+\n          6|    62|\n          1|    26|\n          3|    41|\n          2|    31|\n+-----------+------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["spark.sql(\"\"\"select * from loans\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------------------+------+\ncustomer_id|         created_at|amount|\n+-----------+-------------------+------+\n          1|2019-04-08 00:00:00|    20|\n          1|2019-02-04 00:00:00|    25|\n          1|2019-01-04 00:00:00|    26|\n          2|2020-02-04 00:00:00|    27|\n          2|2019-01-08 00:00:00|    30|\n          2|2019-01-03 00:00:00|    31|\n          3|2020-08-01 00:00:00|    32|\n          3|2019-01-04 00:00:00|    41|\n          4|2020-01-08 00:00:00|    45|\n          5|2021-04-01 00:00:00|    49|\n          6|2019-04-01 00:00:00|    51|\n          6|2019-02-01 00:00:00|    62|\n+-----------+-------------------+------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["list=[[[1,1,1]],\n      [[3,4,5]],\n      [[1,2,1,2]]]\n\ndf=spark.createDataFrame(list, [\"col1\"])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\n        col1|\n+------------+\n   [1, 1, 1]|\n   [3, 4, 5]|\n[1, 2, 1, 2]|\n+------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["df.withColumn(\"count\", F.size(F.array_distinct(\"col1\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+-----+\n        col1|count|\n+------------+-----+\n   [1, 1, 1]|    1|\n   [3, 4, 5]|    3|\n[1, 2, 1, 2]|    2|\n+------------+-----+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nimport pandas as pd\nPage =   ['H','H','H','H','H','H','H','H','H']\nArticle = ['A','B','C','A','B','C','D','C','B']\nRank =    [1,1,1,2,2,2,3,3,3]\nCountOfRank = [50,30,10,40,30,10,40,30,50]\n\ndf = spark.createDataFrame(pd.DataFrame([Page,Article,Rank,CountOfRank]).T, schema=[\"Group\",'Article', 'Rank','CountOfRank'])\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+----+-----------+\nGroup|Article|Rank|CountOfRank|\n+-----+-------+----+-----------+\n    H|      A|   1|         50|\n    H|      B|   1|         30|\n    H|      C|   1|         10|\n    H|      A|   2|         40|\n    H|      B|   2|         30|\n    H|      C|   2|         10|\n    H|      D|   3|         40|\n    H|      C|   3|         30|\n    H|      B|   3|         50|\n+-----+-------+----+-----------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["w=Window().partitionBy(\"Group\",\"Rank\").orderBy(F.col(\"CountOfRank\").desc())\nw2=Window().partitionBy(\"Group\",\"Article\",\"max\").orderBy(F.col(\"Rank\").asc())\ndf.withColumn(\"max\", F.row_number().over(w))\\\n.withColumn(\"max1\", F.dense_rank().over(w2)).show()\n\n\n\n\n\n\n\n\n'''.filter(F.col(\"CountOfRank\")==F.col(\"first\"))\\\n.withColumn(\"max2\", F.row_number().over(w)).filter(F.col(\"max2\")==1)\\\n.drop(\"max\",\"first\",\"max2\").orderBy(\"Article\").show()\n\n'''"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------+----+-----------+---+----+\nGroup|Article|Rank|CountOfRank|max|max1|\n+-----+-------+----+-----------+---+----+\n    H|      B|   1|         30|  2|   1|\n    H|      B|   2|         30|  2|   2|\n    H|      A|   1|         50|  1|   1|\n    H|      A|   2|         40|  1|   2|\n    H|      D|   3|         40|  2|   1|\n    H|      B|   3|         50|  1|   1|\n    H|      C|   1|         10|  3|   1|\n    H|      C|   2|         10|  3|   2|\n    H|      C|   3|         30|  3|   3|\n+-----+-------+----+-----------+---+----+\n\nOut[3]: &#39;.filter(F.col(&#34;CountOfRank&#34;)==F.col(&#34;first&#34;)).withColumn(&#34;max2&#34;, F.row_number().over(w)).filter(F.col(&#34;max2&#34;)==1).drop(&#34;max&#34;,&#34;first&#34;,&#34;max2&#34;).orderBy(&#34;Article&#34;).show()\\n\\n&#39;</div>"]}}],"execution_count":11},{"cell_type":"code","source":["w=Window().partitionBy(\"Group\",\"Rank\").orderBy(F.col(\"CountOfRank\").desc())\nw2=Window().partitionBy(\"Group\",\"Article\",\"max\").orderBy(\"max\")\ndf.withColumn(\"max\", F.row_number().over(w))\\\n.withColumn(\"first\", F.first(\"CountOfRank\").over(w2))\\\n.orderBy(\"Article\",\"max\").filter(F.col(\"CountOfRank\")==F.col(\"first\"))\\\n.withColumn(\"max2\", F.row_number().over(w)).filter(F.col(\"max2\")==1)\\\n.drop(\"max\",\"first\",\"max2\").orderBy(\"Article\").show()\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["+-----+-------+----+-----------+\n|Group|Article|Rank|CountOfRank|\n+-----+-------+----+-----------+\n|    H|      A|   1|         50|\n|    H|      B|   2|         30|\n|    H|      D|   3|         40|\n+-----+-------+----+-----------+"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["+--------+----------+-----------+-------+\n|Customer|FlightDate|IssuingDate|Revenue|\n+--------+----------+-----------+-------+\n|       H|         A|          1|     50|\n|       H|         B|          2|     30|\n|       H|         D|          3|     40|\n+--------+----------+-----------+-------+"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\nlist=[['two',0.6,1.2,1.7,1.5,1.4,2.0,1],\n      ['one',0.3,1.2,1.3,1.5,1.4,1.0,1],\n      ['three',0.4,1.2,1.5,1.9,1.2,0.9,1],\n      ['four',0.9,1.2,1.4,1.9,2.5,1.2,1],\n      ['five',0.9,1.2,1.4,1.9,2.5,1.2,1]]\n\ndf=spark.createDataFrame(list, ['num','col1','col2','col3','col4','col5','col6'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["def transpose(df):\n\n   return  df.withColumn('concat',F.split((F.concat_ws(',',(*(x for x in df.columns[1:])))),','))\\\n             .drop(*(x for x in df.columns[1:])).groupBy().pivot(df.columns[0]).agg(F.first(\"concat\"))\\\n             .withColumn(\"met\", F.explode(F.arrays_zip(*(x[0] for x in df.collect()))))\\\n             .select(\"met.*\")\n  \ndf.show()\ntranspose(df).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+----+----+----+----+---+\n  num|col1|col2|col3|col4|col5|col6| _8|\n+-----+----+----+----+----+----+----+---+\n  two| 0.6| 1.2| 1.7| 1.5| 1.4| 2.0|  1|\n  one| 0.3| 1.2| 1.3| 1.5| 1.4| 1.0|  1|\nthree| 0.4| 1.2| 1.5| 1.9| 1.2| 0.9|  1|\n four| 0.9| 1.2| 1.4| 1.9| 2.5| 1.2|  1|\n five| 0.9| 1.2| 1.4| 1.9| 2.5| 1.2|  1|\n+-----+----+----+----+----+----+----+---+\n\n+---+---+-----+----+----+\ntwo|one|three|four|five|\n+---+---+-----+----+----+\n0.6|0.3|  0.4| 0.9| 0.9|\n1.2|1.2|  1.2| 1.2| 1.2|\n1.7|1.3|  1.5| 1.4| 1.4|\n1.5|1.5|  1.9| 1.9| 1.9|\n1.4|1.4|  1.2| 2.5| 2.5|\n2.0|1.0|  0.9| 1.2| 1.2|\n  1|  1|    1|   1|   1|\n+---+---+-----+----+----+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["def transpose(df):\n           df1= df.withColumn('concat',F.split((F.concat_ws(',',(*(x for x in df.columns[1:])))),','))\\\n                 .drop(*(x for x in df.columns[1:])).groupBy().pivot(df.columns[0]).agg(F.first(\"concat\"))\n           return df1.withColumn(\"met\", F.explode(F.arrays_zip(*(df1.columns))))\\\n                 .select(\"met.*\")\ndf.show()\ntranspose(df).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+----+----+----+----+---+\n  num|col1|col2|col3|col4|col5|col6| _8|\n+-----+----+----+----+----+----+----+---+\n  two| 0.6| 1.2| 1.7| 1.5| 1.4| 2.0|  1|\n  one| 0.3| 1.2| 1.3| 1.5| 1.4| 1.0|  1|\nthree| 0.4| 1.2| 1.5| 1.9| 1.2| 0.9|  1|\n four| 0.9| 1.2| 1.4| 1.9| 2.5| 1.2|  1|\n five| 0.9| 1.2| 1.4| 1.9| 2.5| 1.2|  1|\n+-----+----+----+----+----+----+----+---+\n\n+----+----+---+-----+---+\nfive|four|one|three|two|\n+----+----+---+-----+---+\n 0.9| 0.9|0.3|  0.4|0.6|\n 1.2| 1.2|1.2|  1.2|1.2|\n 1.4| 1.4|1.3|  1.5|1.7|\n 1.9| 1.9|1.5|  1.9|1.5|\n 2.5| 2.5|1.4|  1.2|1.4|\n 1.2| 1.2|1.0|  0.9|2.0|\n   1|   1|  1|    1|  1|\n+----+----+---+-----+---+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql import functions as F\nlist=[['24-MAY-2019']]\n\n\ndf=spark.createDataFrame(list, ['date'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+\n       date|\n+-----------+\n24-MAY-2019|\n+-----------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["df.withColumn(\"date1\", F.to_timestamp(\"date\",'dd-MMM-yyyy')).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-------------------+\n       date|              date1|\n+-----------+-------------------+\n24-MAY-2019|2019-05-24 00:00:00|\n+-----------+-------------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql import functions as F\nlist=[[1,'0','1','0','2','tre',100],\n     [2,'1','0','0','0','tre',200],\n     [3,'2','0','1','1','abc',300]]\n\ndf=spark.createDataFrame(list, ['id','pt_1','pt_2','pt_3','pt_5','name','amt'])\ndf.show()\n\ndf.select((*(x for x in df.columns if not x.startswith('pt'))),*(F.col(x).cast(\"long\").alias(x) for x in df.columns if x.startswith('pt'))).withColumn(\"pt\", F.greatest(*[x for x in df.columns if x.startswith('pt')])).show()\n\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+----+----+----+----+---+\n id|pt_1|pt_2|pt_3|pt_5|name|amt|\n+---+----+----+----+----+----+---+\n  1|   0|   1|   0|   2| tre|100|\n  2|   1|   0|   0|   0| tre|200|\n  3|   2|   0|   1|   1| abc|300|\n+---+----+----+----+----+----+---+\n\n+---+----+---+----+----+----+----+---+\n id|name|amt|pt_1|pt_2|pt_3|pt_5| pt|\n+---+----+---+----+----+----+----+---+\n  1| tre|100|   0|   1|   0|   2|  2|\n  2| tre|200|   1|   0|   0|   0|  1|\n  3| abc|300|   2|   0|   1|   1|  2|\n+---+----+---+----+----+----+----+---+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"stackhelp24","notebookId":160439156022129},"nbformat":4,"nbformat_minor":0}
