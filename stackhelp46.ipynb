{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["\n\n#I want to join two dataframes by column timestamp df2.join(df1, how='left'). The next timestamp column df1 is the stop condition\n\n#Dataframes to join\n\ndf1 = spark.createDataFrame([(1,  110, 'walk',  'work',  '2019-09-28 13:40:00'),\n                         (2,  110, 'metro', 'work',  '2019-09-28 14:00:00'),\n                         (3,  110, 'walk',  'work',  '2019-09-28 14:02:00'),\n                         (4,  120, 'bus',   'home',  '2019-09-28 17:00:00'),\n                         (5,  120, 'metro', 'home',  '2019-09-28 17:20:00'),\n                         (6,  120, 'walk',  'home',  '2019-09-28 17:45:00')],\n                        ['id', 'u_uuid', 'mode', 'place', 'timestamp'])\n\ndf2 = spark.createDataFrame([(1,  '2019-09-28 13:30:00'),\n                         (2,  '2019-09-28 13:35:00'),\n                         (3,  '2019-09-28 13:39:00'),\n                         (4,  '2019-09-28 13:50:00'),\n                         (5,  '2019-09-28 13:55:00'),\n                         (6,  '2019-09-28 14:01:00'),\n                         (7,  '2019-09-28 16:30:00'),\n                         (8,  '2019-09-28 16:40:00'),\n                         (9,  '2019-09-28 16:50:00'),\n                         (10, '2019-09-28 17:25:00'),\n                         (11, '2019-09-28 17:30:00'),\n                         (12, '2019-09-28 17:35:00')],\n                         ['id', 'timestamp'])\n  \ndf1.show()\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+-----+-----+-------------------+\n id|u_uuid| mode|place|          timestamp|\n+---+------+-----+-----+-------------------+\n  1|   110| walk| work|2019-09-28 13:40:00|\n  2|   110|metro| work|2019-09-28 14:00:00|\n  3|   110| walk| work|2019-09-28 14:02:00|\n  4|   120|  bus| home|2019-09-28 17:00:00|\n  5|   120|metro| home|2019-09-28 17:20:00|\n  6|   120| walk| home|2019-09-28 17:45:00|\n+---+------+-----+-----+-------------------+\n\n+---+-------------------+\n id|          timestamp|\n+---+-------------------+\n  1|2019-09-28 13:30:00|\n  2|2019-09-28 13:35:00|\n  3|2019-09-28 13:39:00|\n  4|2019-09-28 13:50:00|\n  5|2019-09-28 13:55:00|\n  6|2019-09-28 14:01:00|\n  7|2019-09-28 16:30:00|\n  8|2019-09-28 16:40:00|\n  9|2019-09-28 16:50:00|\n 10|2019-09-28 17:25:00|\n 11|2019-09-28 17:30:00|\n 12|2019-09-28 17:35:00|\n+---+-------------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["display(df1.join(df2, df1.timestamp>df2.timestamp))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>u_uuid</th><th>mode</th><th>place</th><th>timestamp</th><th>id</th><th>timestamp</th></tr></thead><tbody><tr><td>1</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 13:40:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>1</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 13:40:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>1</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 13:40:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>2</td><td>110</td><td>metro</td><td>work</td><td>2019-09-28 14:00:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>2</td><td>110</td><td>metro</td><td>work</td><td>2019-09-28 14:00:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>2</td><td>110</td><td>metro</td><td>work</td><td>2019-09-28 14:00:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>2</td><td>110</td><td>metro</td><td>work</td><td>2019-09-28 14:00:00</td><td>4</td><td>2019-09-28 13:50:00</td></tr><tr><td>2</td><td>110</td><td>metro</td><td>work</td><td>2019-09-28 14:00:00</td><td>5</td><td>2019-09-28 13:55:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>4</td><td>2019-09-28 13:50:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>5</td><td>2019-09-28 13:55:00</td></tr><tr><td>3</td><td>110</td><td>walk</td><td>work</td><td>2019-09-28 14:02:00</td><td>6</td><td>2019-09-28 14:01:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>4</td><td>2019-09-28 13:50:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>5</td><td>2019-09-28 13:55:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>6</td><td>2019-09-28 14:01:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>7</td><td>2019-09-28 16:30:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>8</td><td>2019-09-28 16:40:00</td></tr><tr><td>4</td><td>120</td><td>bus</td><td>home</td><td>2019-09-28 17:00:00</td><td>9</td><td>2019-09-28 16:50:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>4</td><td>2019-09-28 13:50:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>5</td><td>2019-09-28 13:55:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>6</td><td>2019-09-28 14:01:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>7</td><td>2019-09-28 16:30:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>8</td><td>2019-09-28 16:40:00</td></tr><tr><td>5</td><td>120</td><td>metro</td><td>home</td><td>2019-09-28 17:20:00</td><td>9</td><td>2019-09-28 16:50:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>1</td><td>2019-09-28 13:30:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>2</td><td>2019-09-28 13:35:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>3</td><td>2019-09-28 13:39:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>4</td><td>2019-09-28 13:50:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>5</td><td>2019-09-28 13:55:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>6</td><td>2019-09-28 14:01:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>7</td><td>2019-09-28 16:30:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>8</td><td>2019-09-28 16:40:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>9</td><td>2019-09-28 16:50:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>10</td><td>2019-09-28 17:25:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>11</td><td>2019-09-28 17:30:00</td></tr><tr><td>6</td><td>120</td><td>walk</td><td>home</td><td>2019-09-28 17:45:00</td><td>12</td><td>2019-09-28 17:35:00</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["list=[[\"08.27.18\"]]\n\ndf=spark.createDataFrame(list,['date'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+\n    date|\n+--------+\n08.27.18|\n+--------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["list=[[1,'Hello','Repeat'],\n      [2,'Word','Repeat'],\n      [3,'Aux','No repeat'],\n      [4,'Test','Repeat']]\n\ndf=spark.createDataFrame(list,['id','col1','col2'])\n\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+---------+\n id| col1|     col2|\n+---+-----+---------+\n  1|Hello|   Repeat|\n  2| Word|   Repeat|\n  3|  Aux|No repeat|\n  4| Test|   Repeat|\n+---+-----+---------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\n(df.withColumn(\"test\",F.when(df['col2']=='Repeat',\n       F.array([F.lit(str(i)) for i in range(1,4)])).otherwise(F.array(F.lit(''))))\n  .withColumn(\"col3\",F.explode(F.col(\"test\"))).drop(\"test\")\n  .withColumn(\"col3\",F.concat(F.col(\"col1\"),F.col(\"col3\")))).show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["df.withColumn(\"test\",F.when(df['col2']=='Repeat',\\ F.array([F.concat(F.col(\"col1\"),F.lit(str(i))) for i in range(1,4)])).otherwise(F.array(F.col(\"col1\"))))\\ .select(\"id\",\"col1\",\"col2\", F.explode(\"test\").alias(\"test\"))\\ .show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndf.withColumn(\"test\",F.when(df['col2']=='Repeat',\\\n       F.array([F.concat(F.col(\"col1\"),F.lit(str(i))) for i in range(1,4)])).otherwise(F.array(F.col(\"col1\"))))\\\n       .select(\"id\",\"col1\",\"col2\", F.explode(\"test\"))\\\n       .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+---------+------+\n id| col1|     col2|   col|\n+---+-----+---------+------+\n  1|Hello|   Repeat|Hello1|\n  1|Hello|   Repeat|Hello2|\n  1|Hello|   Repeat|Hello3|\n  2| Word|   Repeat| Word1|\n  2| Word|   Repeat| Word2|\n  2| Word|   Repeat| Word3|\n  3|  Aux|No repeat|   Aux|\n  4| Test|   Repeat| Test1|\n  4| Test|   Repeat| Test2|\n  4| Test|   Repeat| Test3|\n+---+-----+---------+------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"date\", F.date_format(F.to_date(\"date\", \"MM.dd.yy\"),\"MM.dd.yyyy\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\n      date|\n+----------+\n08.27.2018|\n+----------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["list=[[1,2,1],\n      [None,3,None],\n      [5,3,None]]\n\ndf=spark.createDataFrame(list,['col1','col2','col3'])\n\ndf.show()\n\ndf1=df.withColumn(\"col3\", F.when(F.col(\"col3\").isNotNull(), F.lit(None)).otherwise(F.col(\"col3\")))\ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   1|   2|   1|\nnull|   3|null|\n   5|   3|null|\n+----+----+----+\n\n+----+----+----+\ncol1|col2|col3|\n+----+----+----+\n   1|   2|null|\nnull|   3|null|\n   5|   3|null|\n+----+----+----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["cols = [x for x in df1.columns if df1.filter(F.col(x).isNotNull()).count() > 0]\ndf1.select(*cols).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+\ncol1|col2|\n+----+----+\n   1|   2|\nnull|   3|\n   5|   3|\n+----+----+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["df1.select(*[F.array(x) for x in df1.columns])\\\n   .drop(*[x for x in df.columns if df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+-----------+\narray(col1)|array(col2)|array(col3)|\n+-----------+-----------+-----------+\n        [1]|        [2]|         []|\n         []|        [3]|         []|\n        [5]|        [3]|         []|\n+-----------+-----------+-----------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["list\ndf=spark.createDataFrame(list,['date'])\ndf.show()\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+\n      date|\n+----------+\n2019-09-28|\n2020-03-29|\n2019-03-10|\n+----------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["import datetime"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\n\ndf.withColumn('Gender', regexp_replace(regexp_replace(regexp_replace('Gender', 'F','Female'),'M','Male'),' ','missing'))\\\n  .fillna({'Gender': 'missing'}).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+\n Gender| count|\n+-------+------+\n Female| 44015|\nmissing| 42175|\n   Male|104423|\nmissing|     1|\n+-------+------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.functions import when\n\n\ndf.withColumn(\"Gender\", F.when(F.col(\"Gender\")=='F',F.lit(\"Female\"))\\\n              .when(F.col(\"Gender\")=='M',F.lit(\"Male\"))\\\n              .when(F.col(\"Gender\")==' ',F.lit('missing')).otherwise(F.col(\"Gender\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------+\n Gender| count|\n+-------+------+\n Female| 44015|\n   null| 42175|\n   Male|104423|\nmissing|     1|\n+-------+------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["list=[['NT','*1423'],\n['ACT','2868'],\n['*SA','12242'],\n['TAS','4603'],\n['WA','35848'],\n['*NZ*','806'],\n['QLD','44410'],\n['missing','2612'],\n['VIC','40607'],\n['NSW','45195']]\n\ndf=spark.createDataFrame(list,['State1','count'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT|*1423|\n    ACT| 2868|\n    *SA|12242|\n    TAS| 4603|\n     WA|35848|\n   *NZ*|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"State1\", F.expr(\"\"\"replace(state1,'*')\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT|*1423|\n    ACT| 2868|\n     SA|12242|\n    TAS| 4603|\n     WA|35848|\n     NZ|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["df.selectExpr(\"replace(State1,'*') as State1\",\"count\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT|*1423|\n    ACT| 2868|\n     SA|12242|\n    TAS| 4603|\n     WA|35848|\n     NZ|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["df.withColumn(\"State1\", F.regexp_replace(\"State1\",\"\\*\",\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT|*1423|\n    ACT| 2868|\n     SA|12242|\n    TAS| 4603|\n     WA|35848|\n     NZ|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.withColumn(\"State1\", F.when(F.col(\"State1\").contains(\"*\"), F.expr(\"\"\"substring(State1, 2, length(State1))\"\"\"))\\\n                         .otherwise(F.col(\"State1\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT| 1423|\n    ACT| 2868|\n     SA|12242|\n    TAS| 4603|\n     WA|35848|\n     NZ|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df = df.withColumn('Gender', regexp_replace('Gender', 'F','Female'))\ndf = df.withColumn('Gender', regexp_replace('Gender', 'M','Male'))\ndf = df.withColumn('Gender', regexp_replace('Gender', ' ','missing'))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["df.replace(\"*\",\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----+\n State1|count|\n+-------+-----+\n     NT| 1423|\n    ACT| 2868|\n     SA|12242|\n    TAS| 4603|\n     WA|35848|\n    *NZ|  806|\n    QLD|44410|\nmissing| 2612|\n    VIC|40607|\n    NSW|45195|\n+-------+-----+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["list=[[True,'10%',200,\"\"\"[{\"Out_1\": \"Mean\", \"Out_2\": \"25\"}, {\"Out_1\": \"Median\", \"Out_2\": \"21\"}]\"\"\"],\n[False,'15%',150,\"\"\"[{\"Out_1\": \"Mean\", \"Out_2\": \"19\"}, {\"Out_1\": \"Median\", \"Out_2\": \"18\"}]\"\"\"],\n[True,'12%',100,\"\"\"[{\"Out_1\": \"Mean\", \"Out_2\": \"22\"}, {\"Out_1\": \"Median\", \"Out_2\": \"20\"}]\"\"\"]]\n\ndf=spark.createDataFrame(list,['Var 1','Var 2','Var 3','JSON Table' ])\n\ndf.show(truncate=False)\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+----------------------------------------------------------------------+\nVar 1|Var 2|Var 3|JSON Table                                                            |\n+-----+-----+-----+----------------------------------------------------------------------+\ntrue |10%  |200  |[{&#34;Out_1&#34;: &#34;Mean&#34;, &#34;Out_2&#34;: &#34;25&#34;}, {&#34;Out_1&#34;: &#34;Median&#34;, &#34;Out_2&#34;: &#34;21&#34;}]|\nfalse|15%  |150  |[{&#34;Out_1&#34;: &#34;Mean&#34;, &#34;Out_2&#34;: &#34;19&#34;}, {&#34;Out_1&#34;: &#34;Median&#34;, &#34;Out_2&#34;: &#34;18&#34;}]|\ntrue |12%  |100  |[{&#34;Out_1&#34;: &#34;Mean&#34;, &#34;Out_2&#34;: &#34;22&#34;}, {&#34;Out_1&#34;: &#34;Median&#34;, &#34;Out_2&#34;: &#34;20&#34;}]|\n+-----+-----+-----+----------------------------------------------------------------------+\n\nroot\n-- Var 1: boolean (nullable = true)\n-- Var 2: string (nullable = true)\n-- Var 3: long (nullable = true)\n-- JSON Table: string (nullable = true)\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["df.select(F.schema_of_json('[{\"Out_1\": \"Mean\", \"Out_2\": \"25\"}, {\"Out_1\": \"Median\", \"Out_2\": \"21\"}]').alias(\"json\")).collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: [Row(json=&#39;array&lt;struct&lt;Out_1:string,Out_2:string&gt;&gt;&#39;),\n Row(json=&#39;array&lt;struct&lt;Out_1:string,Out_2:string&gt;&gt;&#39;),\n Row(json=&#39;array&lt;struct&lt;Out_1:string,Out_2:string&gt;&gt;&#39;)]</div>"]}}],"execution_count":26},{"cell_type":"code","source":["\nschema = ArrayType(MapType(StringType(),StringType()))\nschema1= 'array<struct<Out_1:string,Out_2:string>>'\n\ndf\\\n     .withColumn(\"JSON Table\", F.explode(F.from_json(\"JSON Table\", schema)))\\\n     .select(F.col('*'), F.col('JSON Table')).select(\"Var 1\",\"Var 2\",\"Var 3\",\"JSON Table.Out_1\",\"JSON Table.Out_2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+------+-----+\nVar 1|Var 2|Var 3| Out_1|Out_2|\n+-----+-----+-----+------+-----+\n true|  10%|  200|  Mean|   25|\n true|  10%|  200|Median|   21|\nfalse|  15%|  150|  Mean|   19|\nfalse|  15%|  150|Median|   18|\n true|  12%|  100|  Mean|   22|\n true|  12%|  100|Median|   20|\n+-----+-----+-----+------+-----+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nschema = ArrayType(MapType(StringType(),StringType()))\ndf.withColumn(\"JSON Table\", F.explode(F.from_json(\"JSON Table\", schema)))\\\n  .select(\"Var 1\",\"Var 2\",\"Var 3\",\"JSON Table.Out_1\",\"JSON Table.Out_2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+------+-----+\nVar 1|Var 2|Var 3| Out_1|Out_2|\n+-----+-----+-----+------+-----+\n true|  10%|  200|  Mean|   25|\n true|  10%|  200|Median|   21|\nfalse|  15%|  150|  Mean|   19|\nfalse|  15%|  150|Median|   18|\n true|  12%|  100|  Mean|   22|\n true|  12%|  100|Median|   20|\n+-----+-----+-----+------+-----+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql import functions as F \nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom pyspark.storagelevel import StorageLevel"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["STANDARD_TENORS = [1, 2, 3]\nmydata1 = (\"A\", 100, 2) \nmydata2 = (\"A\", 200, 1) \nmydata3 = (\"B\", 300, 1) \nmydata4 = (\"B\", 400, 2) \nmydata5 = (\"B\", 500, 3) \nmydata6 = (\"C\", 600, 3)\nmyDataAll=[mydata1, mydata2, mydata3, mydata4, mydata5, mydata6]\ndf = spark.createDataFrame(myDataAll,['Name','Number','days'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----+\nName|Number|days|\n+----+------+----+\n   A|   100|   2|\n   A|   200|   1|\n   B|   300|   1|\n   B|   400|   2|\n   B|   500|   3|\n   C|   600|   3|\n+----+------+----+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"Number\")\\\n                      ,F.first(\"days2\").alias(\"days2\"),F.collect_list(\"days\").alias(\"days\"))\\\n  .withColumn(\"day5\", F.expr(\"\"\"transform(array_except(days2,days),days-> struct(bigint(-1),days))\"\"\"))\\\n  .withColumn(\"days3\", F.explode(F.expr(\"\"\"array_union(arrays_zip(col1,days),day5)\"\"\"))).select(\"Name\",\"days3.*\")\\\n  .withColumn(\"Number\", F.when(F.col(\"Number\")==-1, F.lit(None)).otherwise(F.col(\"Number\")))\\\n  .orderBy(\"Name\",\"days\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o4744.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;(`Number` = CAST(-1 AS BIGINT))&#39; due to data type mismatch: differing types in &#39;(`Number` = CAST(-1 AS BIGINT))&#39; (array&lt;bigint&gt; and bigint).; line 1 pos 49;\n&#39;Project [Name#18, Number#3802, days2#3804, days#3806, transform(array_except(cast(days2#3804 as array&lt;bigint&gt;), days#3806), lambdafunction(named_struct(col1, (Number#3802 = cast(-1 as bigint)), days, lambda days#3812L), lambda days#3812L, false)) AS day5#3811]\n+- Aggregate [Name#18], [Name#18, collect_list(Number#19L, 0, 0) AS Number#3802, first(days2#3792, false) AS days2#3804, collect_list(days#20L, 0, 0) AS days#3806]\n   +- Project [Name#18, Number#19L, days#20L, array(1, 2, 3) AS days2#3792]\n      +- LogicalRDD [Name#18, Number#19L, days#20L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:322)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8$$anonfun$apply$13.apply(TreeNode.scala:381)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.GeneratedMethodAccessor296.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-442081451508080&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   .groupBy(&#34;Name&#34;).agg(F.collect_list(&#34;Number&#34;).alias(&#34;Number&#34;)\\\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                       ,F.first(&#34;days2&#34;).alias(&#34;days2&#34;),F.collect_list(&#34;days&#34;).alias(&#34;days&#34;))\\\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;day5&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;transform(array_except(days2,days),days-&gt; struct(Number=bigint(-1),days))&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;days3&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>explode<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;array_union(arrays_zip(Number,days),day5)&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Name&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;days3.*&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Number&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Number&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">==</span><span class=\"ansi-blue-fg\">-</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>lit<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>otherwise<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Number&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;(`Number` = CAST(-1 AS BIGINT))&#39; due to data type mismatch: differing types in &#39;(`Number` = CAST(-1 AS BIGINT))&#39; (array&lt;bigint&gt; and bigint).; line 1 pos 49;\\n&#39;Project [Name#18, Number#3802, days2#3804, days#3806, transform(array_except(cast(days2#3804 as array&lt;bigint&gt;), days#3806), lambdafunction(named_struct(col1, (Number#3802 = cast(-1 as bigint)), days, lambda days#3812L), lambda days#3812L, false)) AS day5#3811]\\n+- Aggregate [Name#18], [Name#18, collect_list(Number#19L, 0, 0) AS Number#3802, first(days2#3792, false) AS days2#3804, collect_list(days#20L, 0, 0) AS days#3806]\\n   +- Project [Name#18, Number#19L, days#20L, array(1, 2, 3) AS days2#3792]\\n      +- LogicalRDD [Name#18, Number#19L, days#20L], false\\n&#34;</div>"]}}],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"col1\")\\\n                      ,F.first(\"days2\").alias(\"days2\"),F.collect_list(\"days\").alias(\"x\"))\\\n  .withColumn(\"days3\", F.arrays_zip(F.col(\"col1\"),F.col(\"x\")))\\\n  .withColumn(\"days4\", F.array_except(\"days2\",\"x\"))\\\n  .withColumn(\"day5\", F.expr(\"\"\"transform(days4,x-> struct(bigint(-1),x))\"\"\"))\\\n  .withColumn(\"days3\", F.explode(F.array_union(\"days3\",\"day5\"))).select(\"Name\",\"days3.*\")\\\n  .withColumn(\"Number\", F.when(F.col(\"col1\")==-1, F.lit(None)).otherwise(F.col(\"col1\"))).drop(\"col1\")\\\n  .withColumnRenamed(\"x\",\"days\")\\\n  .orderBy(\"Name\",\"days\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"col1\")\\\n                      ,F.first(\"days2\").alias(\"days2\"),F.collect_list(\"days\").alias(\"x\"))\\\n  .withColumn(\"days4\", F.array_except(\"days2\",\"x\"))\\\n  .withColumn(\"day5\", F.expr(\"\"\"transform(days4,x-> struct(bigint(-1),x))\"\"\"))\\\n  .withColumn(\"zip\", F.explode(F.array_union(F.arrays_zip(F.col(\"col1\"),F.col(\"x\")),\"day5\"))).select(\"Name\",\"zip.*\")\\\n  .withColumn(\"Number\", F.when(F.col(\"col1\")==-1, F.lit(None)).otherwise(F.col(\"col1\"))).drop(\"col1\")\\\n  .select(\"Name\", \"Number\", F.col(\"x\").alias(\"days\"))\\\n  .orderBy(\"Name\",\"days\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o5824.showString.\n: java.lang.IllegalArgumentException: requirement failed: All input types must be the same except nullable, containsNull, valueContainsNull flags. The input types found are\n\tArrayType(StructType(StructField(0,LongType,true), StructField(1,LongType,true)),false)\n\tArrayType(StructType(StructField(col1,LongType,false), StructField(x,LongType,false)),false)\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression$class.dataTypeCheck(Expression.scala:776)\n\tat org.apache.spark.sql.catalyst.expressions.ArrayUnion.dataTypeCheck(collectionOperations.scala:3302)\n\tat org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression$class.dataType(Expression.scala:783)\n\tat org.apache.spark.sql.catalyst.expressions.ArrayUnion.dataType(collectionOperations.scala:3302)\n\tat org.apache.spark.sql.catalyst.expressions.ExplodeBase.elementSchema(generators.scala:284)\n\tat org.apache.spark.sql.catalyst.expressions.Generator$class.dataType(generators.scala:47)\n\tat org.apache.spark.sql.catalyst.expressions.ExplodeBase.dataType(generators.scala:271)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:125)\n\tat org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$genCode$2.apply(Expression.scala:125)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:123)\n\tat org.apache.spark.sql.execution.GenerateExec.codeGenCollection(GenerateExec.scala:169)\n\tat org.apache.spark.sql.execution.GenerateExec.doConsume(GenerateExec.scala:154)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:220)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:191)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:77)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:193)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:378)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:407)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:378)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.GenerateExec.doProduce(GenerateExec.scala:141)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.GenerateExec.produce(GenerateExec.scala:58)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:51)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:94)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:89)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:548)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:602)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$5.apply(SparkPlan.scala:188)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:161)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:67)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:325)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectResult(Dataset.scala:2890)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3508)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3492)\n\tat org.apache.spark.sql.Dataset$$anonfun$54.apply(Dataset.scala:3487)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:113)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:242)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:99)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:172)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3487)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2619)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2833)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:266)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:303)\n\tat sun.reflect.GeneratedMethodAccessor381.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-442081451508084&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span>   <span class=\"ansi-blue-fg\">.</span>select<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Name&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;Number&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;x&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>alias<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;days&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     10</span>   <span class=\"ansi-blue-fg\">.</span>orderBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Name&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;days&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-fg\">---&gt; 11</span><span class=\"ansi-red-fg\">   </span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    384</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    385</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 386</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    387</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    388</span>     <span class=\"ansi-green-fg\">def</span> __repr__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     77</span>                 <span class=\"ansi-green-fg\">raise</span> QueryExecutionException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     78</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;java.lang.IllegalArgumentException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 79</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> IllegalArgumentException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     80</span>             <span class=\"ansi-green-fg\">raise</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     81</span>     <span class=\"ansi-green-fg\">return</span> deco\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: &#39;requirement failed: All input types must be the same except nullable, containsNull, valueContainsNull flags. The input types found are\\n\\tArrayType(StructType(StructField(0,LongType,true), StructField(1,LongType,true)),false)\\n\\tArrayType(StructType(StructField(col1,LongType,false), StructField(x,LongType,false)),false)&#39;</div>"]}}],"execution_count":33},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"col1\")\\\n                      ,F.first(\"days2\").alias(\"days2\"),F.collect_list(\"days\").alias(\"x\"))\\\n  .withColumn(\"days3\", F.arrays_zip(F.col(\"col1\"),F.col(\"x\")))\\\n  .withColumn(\"days4\", F.array_except(\"days2\",\"x\"))\\\n  .withColumn(\"day5\", F.expr(\"\"\"transform(days4,x-> struct(bigint(-1),x))\"\"\"))\\\n  .withColumn(\"days3\", F.explode(F.array_union(\"days3\",\"day5\"))).select(\"Name\",\"days3.*\")\\\n  .withColumn(\"Number\", F.when(F.col(\"col1\")==-1, F.lit(None)).otherwise(F.col(\"col1\"))).drop(\"col1\")\\\n  .select(\"Name\", \"Number\", F.col(\"x\").alias(\"days\"))\\\n  .orderBy(\"Name\",\"days\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----+\nName|Number|days|\n+----+------+----+\nA   |200   |1   |\nA   |100   |2   |\nA   |null  |3   |\nB   |300   |1   |\nB   |400   |2   |\nB   |500   |3   |\nC   |null  |1   |\nC   |null  |2   |\nC   |600   |3   |\n+----+------+----+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"col1\")\\\n                      ,F.first(\"days2\").alias(\"days2\"),F.collect_list(\"days\").alias(\"x\"))\\\n  .withColumn(\"days3\", F.arrays_zip(F.col(\"col1\"),F.col(\"x\")))\\\n  .withColumn(\"days4\", F.array_except(\"days2\",\"x\"))\\\n  .withColumn(\"day5\", F.expr(\"\"\"transform(days4,x-> struct(bigint(-1),x))\"\"\"))\\\n  .withColumn(\"days3\", F.explode(F.array_union(\"days3\",\"day5\"))).select(\"Name\",\"days3.*\")\\\n  .withColumn(\"Number\", F.when(F.col(\"col1\")==-1, F.lit(None)).otherwise(F.col(\"col1\"))).drop(\"col1\")\\\n  .select(\"Name\", \"Number\", F.col(\"x\").alias(\"days\"))\\\n  .orderBy(\"Name\",\"days\")\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----+\nName|Number|days|\n+----+------+----+\nA   |100   |1   |\nA   |200   |2   |\nA   |null  |3   |\nB   |300   |1   |\nB   |400   |2   |\nB   |500   |3   |\nC   |null  |1   |\nC   |null  |2   |\nC   |600   |3   |\n+----+------+----+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["mydata1 = (\"A\", 100, 1)\nmydata2 = (\"A\", 200, 2)\nmydata3 = (\"B\", 300, 1)\nmydata4 = (\"B\", 400, 2)\nmydata5 = (\"B\", 500, 3)\nmydata6 = (\"C\", 600, 1)\nmyDataAll = [mydata1, mydata2, mydata3, mydata4, mydata5, mydata6]\n\nSTANDARD_TENORS = [1, 2, 3]\n\ndf_sd = spark.createDataFrame(STANDARD_TENORS, IntegerType())\ndf_sd = df_sd.withColumnRenamed(\"value\", \"days\")\n\ndf_sd.show()\n\ndf = spark.createDataFrame(myDataAll,['Name','Number','days'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\ndays|\n+----+\n   1|\n   2|\n   3|\n+----+\n\n+----+------+----+\nName|Number|days|\n+----+------+----+\n   A|   100|   1|\n   A|   200|   2|\n   B|   300|   1|\n   B|   400|   2|\n   B|   500|   3|\n   C|   600|   1|\n+----+------+----+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf_sd.agg(F.collect_list(\"days\").alias(\"days\")).join(\\\ndf.orderBy(\"Name\",\"days\").groupBy(\"Name\")\\\n.agg(F.collect_list(\"Number\").alias(\"Number\"),F.collect_list(\"days\").alias(\"days1\")),\\\n                      F.size(\"days\")>=F.size(\"days1\")).drop(\"days1\")\\\n     .withColumn(\"zipped\", F.explode(F.arrays_zip(\"Number\",\"days\")))\\\n     .select(\"Name\",\"zipped.*\")\\\n     .orderBy(\"Name\",\"days\")\\\n     .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----+\nName|Number|days|\n+----+------+----+\n   A|   100|   1|\n   A|   200|   2|\n   A|  null|   3|\n   B|   300|   1|\n   B|   400|   2|\n   B|   500|   3|\n   C|   600|   1|\n   C|  null|   2|\n   C|  null|   3|\n+----+------+----+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .orderBy(\"Name\",\"days\")\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"Number\")\\\n                      ,F.first(\"days2\").alias(\"days\"))\\\n  .withColumn(\"zipped\", F.explode(F.arrays_zip(\"Number\",\"days\")))\\\n  .select(\"Name\",\"zipped.*\").orderBy(\"Name\",\"days\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+----+\nName|Number|days|\n+----+------+----+\n   A|   200|   1|\n   A|   100|   2|\n   A|  null|   3|\n   B|   300|   1|\n   B|   400|   2|\n   B|   500|   3|\n   C|   600|   1|\n   C|  null|   2|\n   C|  null|   3|\n+----+------+----+\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"days2\", F.array(*[F.lit(x) for x in STANDARD_TENORS]))\\\n  .orderBy(\"Name\",\"days\")\\\n  .groupBy(\"Name\").agg(F.collect_list(\"Number\").alias(\"Number\")\\\n                      ,F.first(\"days2\").alias(\"days\"))\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---------------+---------+\nName|         Number|     days|\n+----+---------------+---------+\n   B|[300, 400, 500]|[1, 2, 3]|\n   C|          [600]|[1, 2, 3]|\n   A|     [200, 100]|[1, 2, 3]|\n+----+---------------+---------+\n\n</div>"]}}],"execution_count":39},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":40}],"metadata":{"name":"stackhelp46","notebookId":3507020754336918},"nbformat":4,"nbformat_minor":0}
