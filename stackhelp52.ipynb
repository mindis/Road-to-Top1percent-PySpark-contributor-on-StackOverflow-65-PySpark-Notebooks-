{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import Row"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[['PT085087309  2013-10-03  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-04  Influenza A.H1.respiratory.qualitative  2013-10-04  not detected  0'],\n['PT085087309  2013-10-07  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  2'],\n['PT085087309  2013-10-09  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  4'],\n['PT085087309  2013-10-14  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  9'],\n['PT085087309  2013-10-15  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  10'],\n['PT085087309  2013-10-18  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  13'],\n['PT085087309  2013-10-21  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-23  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-24  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-25  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-27  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-28  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-10-31  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-01  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-04  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-06  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-08  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-11  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-14  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2013-11-15  Influenza A.H1.respiratory.qualitative  2013-11-15  not detected  0'],\n['PT085087309  2013-11-18  Influenza A.H1.respiratory.qualitative  2013-11-15  not detected  3'],\n['PT085087309  2013-11-19  Influenza A.H1.respiratory.qualitative  2013-11-15  not detected  4'],\n['PT085087309  2013-11-21  Influenza A.H1.respiratory.qualitative  2013-11-15  not detected  6'],\n['PT085087309  2014-09-29  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  8'],\n['PT085087309  2014-09-30  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  9'],\n['PT085087309  2014-10-01  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  10'],\n['PT085087309  2014-10-02  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  11'],\n['PT085087309  2014-10-03  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  12'],\n['PT085087309  2014-10-04  Influenza A.H1.respiratory.qualitative  2014-09-21  not detected  13'],\n['PT085087309  2014-10-06  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2014-10-07  Influenza A.H1.respiratory.qualitative  NA  NA  0'],\n['PT085087309  2014-10-09  Influenza A.H1.respiratory.qualitative  NA  NA  0']]\n\ndf=spark.createDataFrame(list,['col'])\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------------------------------------------------------------------------+\ncol                                                                                          |\n+---------------------------------------------------------------------------------------------+\nPT085087309  2013-10-03  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-04  Influenza A.H1.respiratory.qualitative  2013-10-04  not detected  0 |\nPT085087309  2013-10-07  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  2 |\nPT085087309  2013-10-09  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  4 |\nPT085087309  2013-10-14  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  9 |\nPT085087309  2013-10-15  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  10|\nPT085087309  2013-10-18  Influenza A.H1.respiratory.qualitative  2013-10-05  not detected  13|\nPT085087309  2013-10-21  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-23  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-24  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-25  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-27  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-28  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-10-31  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-01  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-04  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-06  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-08  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-11  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\nPT085087309  2013-11-14  Influenza A.H1.respiratory.qualitative  NA  NA  0                   |\n+---------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df=df.withColumn(\"col\", F.split('col','  '))\\\n  .withColumn(\"ptid\", F.col(\"col\")[0])\\\n  .withColumn(\"blast_date\", F.col(\"col\")[1])\\\n  .withColumn(\"test_name\", F.col(\"col\")[2])\\\n  .withColumn(\"result_date\", F.col(\"col\")[3])\\\n  .withColumn(\"test_result\", F.col(\"col\")[4])\\\n  .withColumn(\"date_diff\", F.col(\"col\")[5])\\\n  .drop(\"col\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["df.show(20)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+--------------------+-----------+------------+---------+\n       ptid|blast_date|           test_name|result_date| test_result|date_diff|\n+-----------+----------+--------------------+-----------+------------+---------+\nPT085087309|2013-10-03|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-04|Influenza A.H1.re...| 2013-10-04|not detected|        0|\nPT085087309|2013-10-07|Influenza A.H1.re...| 2013-10-05|not detected|        2|\nPT085087309|2013-10-09|Influenza A.H1.re...| 2013-10-05|not detected|        4|\nPT085087309|2013-10-14|Influenza A.H1.re...| 2013-10-05|not detected|        9|\nPT085087309|2013-10-15|Influenza A.H1.re...| 2013-10-05|not detected|       10|\nPT085087309|2013-10-18|Influenza A.H1.re...| 2013-10-05|not detected|       13|\nPT085087309|2013-10-21|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-23|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-24|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-25|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-27|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-28|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-31|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-01|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-04|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-06|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-08|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-11|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-11-14|Influenza A.H1.re...|         NA|          NA|        0|\n+-----------+----------+--------------------+-----------+------------+---------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"ptid\",\"test_name\").orderBy(\"blast_date\")\nw2=Window().partitionBy(\"ptid\",\"test_name\",\"inc_sum\").orderBy(\"blast_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\ndisplay(df.withColumn(\"inc_sum\",F.sum(F.when(F.col(\"result_date\")!='NA', F.lit(1))\\\n                                   .otherwise(F.lit(0))).over(w))\\\n  .withColumn(\"result_date\", F.when(F.col(\"result_date\")=='NA',F.first(\"result_date\").over(w2))\\\n                              .otherwise(F.col(\"result_date\")))\\\n  .withColumn(\"test_result\", F.when(F.col(\"test_result\")=='NA',F.first(\"test_result\").over(w2))\\\n                              .otherwise(F.col(\"test_result\"))).drop(\"inc_sum\"))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ptid</th><th>blast_date</th><th>test_name</th><th>result_date</th><th>test_result</th><th>date_diff</th></tr></thead><tbody><tr><td>PT085087309</td><td>2013-10-03</td><td>Influenza A.H1.respiratory.qualitative</td><td>NA</td><td>NA</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-04</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-07</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>2</td></tr><tr><td>PT085087309</td><td>2013-10-09</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>4</td></tr><tr><td>PT085087309</td><td>2013-10-14</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>9</td></tr><tr><td>PT085087309</td><td>2013-10-15</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>10</td></tr><tr><td>PT085087309</td><td>2013-10-18</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>13</td></tr><tr><td>PT085087309</td><td>2013-10-21</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-23</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-24</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-25</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-27</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-28</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-31</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-01</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-06</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-08</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-11</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-14</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-15</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-18</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td>3</td></tr><tr><td>PT085087309</td><td>2013-11-19</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td>4</td></tr><tr><td>PT085087309</td><td>2013-11-21</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td>6</td></tr><tr><td>PT085087309</td><td>2014-09-29</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>8</td></tr><tr><td>PT085087309</td><td>2014-09-30</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>9</td></tr><tr><td>PT085087309</td><td>2014-10-01</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>10</td></tr><tr><td>PT085087309</td><td>2014-10-02</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>11</td></tr><tr><td>PT085087309</td><td>2014-10-03</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>12</td></tr><tr><td>PT085087309</td><td>2014-10-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>13</td></tr><tr><td>PT085087309</td><td>2014-10-06</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2014-10-07</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2014-10-09</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["list=[['PT085087309','2013-10-18','Influenza A.H1.respiratory.qualitative','2013-10-05','not detected',13],\n      ['PT085087309','2013-10-21','Influenza A.H1.respiratory.qualitative','NA','NA',0],\n      ['PT085087309','2013-10-23','Influenza A.H1.respiratory.qualitative','NA','NA',0],\n      ['PT085087309','2013-10-24','Influenza A.H1.respiratory.qualitative','2013-10-08','detected',0],\n      ['PT085087309','2013-10-25','Influenza A.H1.respiratory.qualitative','NA','NA',0],\n      ['PT085087309','2013-10-27','Influenza A.H1.respiratory.qualitative','NA','NA',0]]\n\ndf=spark.createDataFrame(list,['ptid','blast_date','test_name','result_date','test_result','date_diff'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+--------------------+-----------+------------+---------+\n       ptid|blast_date|           test_name|result_date| test_result|date_diff|\n+-----------+----------+--------------------+-----------+------------+---------+\nPT085087309|2013-10-18|Influenza A.H1.re...| 2013-10-05|not detected|       13|\nPT085087309|2013-10-21|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-23|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-24|Influenza A.H1.re...| 2013-10-08|    detected|        0|\nPT085087309|2013-10-25|Influenza A.H1.re...|         NA|          NA|        0|\nPT085087309|2013-10-27|Influenza A.H1.re...|         NA|          NA|        0|\n+-----------+----------+--------------------+-----------+------------+---------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"ptid\",\"test_name\").orderBy(\"blast_date\")\nw2=Window().partitionBy(\"ptid\",\"test_name\",\"inc_sum\").orderBy(\"blast_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\ndisplay(df.withColumn(\"inc_sum\",F.sum(F.when(F.col(\"result_date\")!='NA', F.lit(1))\\\n                                   .otherwise(F.lit(0))).over(w))\\\n  .withColumn(\"result_date\", F.when(F.col(\"result_date\")=='NA',F.first(\"result_date\").over(w2))\\\n                              .otherwise(F.col(\"result_date\")))\\\n  .withColumn(\"test_result\", F.when(F.col(\"test_result\")=='NA',F.first(\"test_result\").over(w2))\\\n                              .otherwise(F.col(\"test_result\"))).drop(\"inc_sum\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ptid</th><th>blast_date</th><th>test_name</th><th>result_date</th><th>test_result</th><th>date_diff</th></tr></thead><tbody><tr><td>PT085087309</td><td>2013-10-03</td><td>Influenza A.H1.respiratory.qualitative</td><td>NA</td><td>NA</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-04</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-07</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>2</td></tr><tr><td>PT085087309</td><td>2013-10-09</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>4</td></tr><tr><td>PT085087309</td><td>2013-10-14</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>9</td></tr><tr><td>PT085087309</td><td>2013-10-15</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>10</td></tr><tr><td>PT085087309</td><td>2013-10-18</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>13</td></tr><tr><td>PT085087309</td><td>2013-10-21</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-23</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-24</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-25</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-27</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-28</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-10-31</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-01</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-06</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-08</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-11</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-14</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-10-05</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2013-11-15</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2013-11-18</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2013-11-19</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2013-11-21</td><td>Influenza A.H1.respiratory.qualitative</td><td>2013-11-15</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-09-29</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-09-30</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-10-01</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-10-02</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-10-03</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-10-04</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td></td></tr><tr><td>PT085087309</td><td>2014-10-06</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2014-10-07</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr><tr><td>PT085087309</td><td>2014-10-09</td><td>Influenza A.H1.respiratory.qualitative</td><td>2014-09-21</td><td>not detected</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"code","source":["\nspark.sql(\"\"\"\n           Select\n           VAR\n           From mytable\n           \"\"\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["list=[['City1',160,158,253,391,12],\n      ['City2',212,27,362,512,34],\n      ['City3',90,150,145,274,56]]\n\ndf=spark.createDataFrame(list,['City','JAN','FEB','MAR','DEC','Constant'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+---+---+---+--------+\n City|JAN|FEB|MAR|DEC|Constant|\n+-----+---+---+---+---+--------+\nCity1|160|158|253|391|      12|\nCity2|212| 27|362|512|      34|\nCity3| 90|150|145|274|      56|\n+-----+---+---+---+---+--------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["df.show()#sampledata\n#+-----+---+---+---+---+--------+\n#| City|JAN|FEB|MAR|DEC|Constant|\n#+-----+---+---+---+---+--------+\n#|City1|160|158|253|391|      12|\n#|City2|212| 27|362|512|      34|\n#|City3| 90|150|145|274|      56|\n#+-----+---+---+---+---+--------+\n\n\n\nlistofmonths=['JAN','FEB','MAR','DEC']\n\nfrom pyspark.sql import functions as F\ndf.withColumn(\"arr\", F.struct(*[(F.col(x)*F.col('Constant')).alias(x) for x in listofmonths]))\\\n  .drop(*listofmonths)\\\n  .select(\"City\",\"arr.*\")\\\n  .show()\n\n#+-----+----+----+-----+-----+\n#| City| JAN| FEB|  MAR|  DEC|\n#+-----+----+----+-----+-----+\n#|City1|1920|1896| 3036| 4692|\n#|City2|7208| 918|12308|17408|\n#|City3|5040|8400| 8120|15344|\n#+-----+----+----+-----+-----+  \n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+-----+-----+\n City| JAN| FEB|  MAR|  DEC|\n+-----+----+----+-----+-----+\nCity1|1920|1896| 3036| 4692|\nCity2|7208| 918|12308|17408|\nCity3|5040|8400| 8120|15344|\n+-----+----+----+-----+-----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"arr\", F.struct(*[(F.col(x)*F.col('Constant')).alias(x) for x in listofmonths]))\\\n  .select(\"City\",\"arr.*\")\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+-----+-----+\n City| JAN| FEB|  MAR|  DEC|\n+-----+----+----+-----+-----+\nCity1|1920|1896| 3036| 4692|\nCity2|7208| 918|12308|17408|\nCity3|5040|8400| 8120|15344|\n+-----+----+----+-----+-----+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"arr\", F.struct(*[(F.col(x)*F.col('Constant')).alias(x) for x in df.columns if x!='City' and x!='Constant']))\\\n  .select(\"City\",\"arr.*\")\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+-----+-----+\n City| JAN| FEB|  MAR|  DEC|\n+-----+----+----+-----+-----+\nCity1|1920|1896| 3036| 4692|\nCity2|7208| 918|12308|17408|\nCity3|5040|8400| 8120|15344|\n+-----+----+----+-----+-----+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"arr\", F.struct(*[(F.col(x)*F.col('Constant')).alias(x) for x in listofmonths]))\\\n  .drop(*listofmonths)\\\n  .select(\"City\",\"arr.*\")\\\n  .show()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+----+-----+-----+\n City| JAN| FEB|  MAR|  DEC|\n+-----+----+----+-----+-----+\nCity1|1920|1896| 3036| 4692|\nCity2|7208| 918|12308|17408|\nCity3|5040|8400| 8120|15344|\n+-----+----+----+-----+-----+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["df.withColumn(\"arr\", F.arrays_zip(*[F.array(F.struct(F.col(x)*F.col('Constant'))).alias(x) for x in ['JAN','FEB','MAR','DEC']]))\\\n  .drop(*['JAN','FEB','MAR','DEC'])\\\n  .printSchema()\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- City: string (nullable = true)\n-- Constant: long (nullable = true)\n-- arr: array (nullable = false)\n    |-- element: struct (containsNull = false)\n    |    |-- 0: struct (nullable = true)\n    |    |    |-- col1: long (nullable = true)\n    |    |-- 1: struct (nullable = true)\n    |    |    |-- col1: long (nullable = true)\n    |    |-- 2: struct (nullable = true)\n    |    |    |-- col1: long (nullable = true)\n    |    |-- 3: struct (nullable = true)\n    |    |    |-- col1: long (nullable = true)\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["df.withColumn(\"arr\", F.array(*[F.struct(F.col(x)) for x in ['JAN','FEB','MAR','DEC']]))\\\n  .withColumn(\"arr\", F.expr(\"\"\"transform(arr,x-> x*Constant)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o367.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;array(named_struct(&#39;JAN&#39;, `JAN`), named_struct(&#39;FEB&#39;, `FEB`), named_struct(&#39;MAR&#39;, `MAR`), named_struct(&#39;DEC&#39;, `DEC`))&#39; due to data type mismatch: input to function array should all be the same type, but it&#39;s [struct&lt;JAN:bigint&gt;, struct&lt;FEB:bigint&gt;, struct&lt;MAR:bigint&gt;, struct&lt;DEC:bigint&gt;];;\n&#39;Project [City#2, JAN#3L, FEB#4L, MAR#5L, DEC#6L, Constant#7L, array(named_struct(JAN, JAN#3L), named_struct(FEB, FEB#4L), named_struct(MAR, MAR#5L), named_struct(DEC, DEC#6L)) AS arr#103]\n+- LogicalRDD [City#2, JAN#3L, FEB#4L, MAR#5L, DEC#6L, Constant#7L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1362)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2294)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-966920025957322&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;arr&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>array<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span><span class=\"ansi-blue-fg\">[</span>F<span class=\"ansi-blue-fg\">.</span>struct<span class=\"ansi-blue-fg\">(</span>F<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span>x<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">for</span> x <span class=\"ansi-green-fg\">in</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;JAN&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;FEB&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;MAR&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;DEC&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;arr&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;transform(arr,x-&gt; x*Constant)&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2025</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2026</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2027</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;array(named_struct(&#39;JAN&#39;, `JAN`), named_struct(&#39;FEB&#39;, `FEB`), named_struct(&#39;MAR&#39;, `MAR`), named_struct(&#39;DEC&#39;, `DEC`))&#39; due to data type mismatch: input to function array should all be the same type, but it&#39;s [struct&lt;JAN:bigint&gt;, struct&lt;FEB:bigint&gt;, struct&lt;MAR:bigint&gt;, struct&lt;DEC:bigint&gt;];;\\n&#39;Project [City#2, JAN#3L, FEB#4L, MAR#5L, DEC#6L, Constant#7L, array(named_struct(JAN, JAN#3L), named_struct(FEB, FEB#4L), named_struct(MAR, MAR#5L), named_struct(DEC, DEC#6L)) AS arr#103]\\n+- LogicalRDD [City#2, JAN#3L, FEB#4L, MAR#5L, DEC#6L, Constant#7L], false\\n&#34;</div>"]}}],"execution_count":15},{"cell_type":"code","source":["list=[[1585766054000,3489692692      ,       3.0,  0.159999     ,  7.58996],\n      [1585766055000, 3489692692      ,        3.0,   0.239999    ,   11.2699], \n      [1585766058000, 3489692692      ,        3.0,   0.135489    ,   13.8790],\n      [1587497991000, 3489692692      ,        2.0,   0.159999     ,   21.6999],\n      [1587864812000, 3489692692      ,        2.0,   0.959999     ,   359.649],\n      [1587581329000, 3489692692      ,       1.0,  1.039999     ,   336.209],\n      [1587581339000, 3489692692      ,        3.0,   1.039999     ,   336.299],\n      [1587581329000, 3489692692      ,        1.0,   2.799999     ,   336.209],\n      [1588088096000, 3489692670      ,        3.0,   2.869564     ,   285.963],\n      [1588088099000, 3489692670      ,        2.0,   0.758753     ,   299.578],\n      [1588088199000, 3489692670      ,        1.0,   3.965424     ,   5.89677]]\n\n\ndf=spark.createDataFrame(list,['epoch_ms','ID','state','value 1','value 2'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+----------+-----+--------+-------+\n     epoch_ms|        ID|state| value 1|value 2|\n+-------------+----------+-----+--------+-------+\n1585766054000|3489692692|  3.0|0.159999|7.58996|\n1585766055000|3489692692|  3.0|0.239999|11.2699|\n1585766058000|3489692692|  3.0|0.135489| 13.879|\n1587497991000|3489692692|  2.0|0.159999|21.6999|\n1587864812000|3489692692|  2.0|0.959999|359.649|\n1587581329000|3489692692|  1.0|1.039999|336.209|\n1587581339000|3489692692|  3.0|1.039999|336.299|\n1587581329000|3489692692|  1.0|2.799999|336.209|\n1588088096000|3489692670|  3.0|2.869564|285.963|\n1588088099000|3489692670|  2.0|0.758753|299.578|\n1588088199000|3489692670|  1.0|3.965424|5.89677|\n+-------------+----------+-----+--------+-------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"ID\").orderBy(F.lit(1))\nw1=Window().partitionBy(\"ID\",\"state\").orderBy(\"rowNum\")\nw2=Window().partitionBy(\"ID\").orderBy(\"rowNum\")\n\ndf.withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"inc_sum\", F.sum(F.when((F.col(\"state\")==3)&\\\n                                      (F.row_number().over(w1)==1),F.lit(1)).otherwise(F.lit(0))).over(w2))\\\n  .groupBy(\"inc_sum\").agg(F.first(\"ID\").alias(\"ID\"),\\\n                          F.max(\"epoch_ms\").alias(\"start_epoch\"),\\\n                          F.min(\"epoch_ms\").alias(\"end_epoch\"),F.max(\"value 1\").alias(\"max_value1\"),\\\n                          F.max(\"value 2\").alias(\"max_value2\")).drop(\"inc_sum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------------+----------+----------+\n        ID|  start_epoch|    end_epoch|max_value1|max_value2|\n+----------+-------------+-------------+----------+----------+\n3489692670|1588088199000|1585766054000|  3.965424|   359.649|\n+----------+-------------+-------------+----------+----------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.show()#sampledata\n#+-------------+----------+-----+--------+-------+\n#|     epoch_ms|        ID|state| value 1|value 2|\n#+-------------+----------+-----+--------+-------+\n#|1585766054000|3489692692|  3.0|0.159999|7.58996|\n#|1585766055000|3489692692|  3.0|0.239999|11.2699|\n#|1585766058000|3489692692|  3.0|0.135489| 13.879|\n#|1587497991000|3489692692|  2.0|0.159999|21.6999|\n#|1587864812000|3489692692|  2.0|0.959999|359.649|\n#|1587581329000|3489692692|  1.0|1.039999|336.209|\n#|1587581339000|3489692692|  3.0|1.039999|336.299|\n#|1587581329000|3489692692|  1.0|2.799999|336.209|\n#|1588088096000|3489692670|  3.0|2.869564|285.963|\n#|1588088099000|3489692670|  2.0|0.758753|299.578|\n#|1588088199000|3489692670|  1.0|3.965424|5.89677|\n#+-------------+----------+-----+--------+-------+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"rowNum\")\n\ndf.withColumn(\"rowNum\", F.monotonically_increasing_id())\\\n  .withColumn(\"inc_sum\", F.sum(F.when((F.col(\"state\")==3) & (F.lag(\"state\").over(w)!=3)\\\n                                      ,F.lit(1)).otherwise(F.lit(0)))\\\n                                       .over(w))\\\n    .groupBy(\"inc_sum\").agg(F.first(\"ID\").alias(\"ID\"),\\\n                          F.first(\"epoch_ms\").alias(\"start_epoch\"),\\\n                          F.last(\"epoch_ms\").alias(\"end_epoch\"),F.max(\"value 1\").alias(\"max_value1\"),\\\n                          F.max(\"value 2\").alias(\"max_value2\")).drop(\"inc_sum\").show()\n\n#+----------+-------------+-------------+----------+----------+\n#|        ID|  start_epoch|    end_epoch|max_value1|max_value2|\n#+----------+-------------+-------------+----------+----------+\n#|3489692692|1585766054000|1587581329000|  1.039999|   359.649|\n#|3489692692|1587581339000|1587581329000|  2.799999|   336.299|\n#|3489692670|1588088096000|1588088199000|  3.965424|   299.578|\n#+----------+-------------+-------------+----------+----------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------------+-------------+----------+----------+\n        ID|  start_epoch|    end_epoch|max_value1|max_value2|\n+----------+-------------+-------------+----------+----------+\n3489692692|1585766054000|1587581329000|  1.039999|   359.649|\n3489692692|1587581339000|1587581329000|  2.799999|   336.299|\n3489692670|1588088096000|1588088199000|  3.965424|   299.578|\n+----------+-------------+-------------+----------+----------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"ID\").orderBy(\"rowNum\")\n\ndf.withColumn(\"rowNum\", F.monotonically_increasing_id())\\\n  .withColumn(\"inc_sum\", F.sum(F.when((F.col(\"state\")==3) & (F.lag(\"state\").over(w)!=3)\\\n                                      ,F.lit(1)).otherwise(F.lit(0)))\\\n                                       .over(w)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+----------+-----+--------+-------+-----------+-------+\n     epoch_ms|        ID|state| value 1|value 2|     rowNum|inc_sum|\n+-------------+----------+-----+--------+-------+-----------+-------+\n1588088096000|3489692670|  3.0|2.869564|285.963|51539607552|      0|\n1588088099000|3489692670|  2.0|0.758753|299.578|60129542144|      0|\n1588088199000|3489692670|  1.0|3.965424|5.89677|60129542145|      0|\n1585766054000|3489692692|  3.0|0.159999|7.58996|          0|      0|\n1585766055000|3489692692|  3.0|0.239999|11.2699| 8589934592|      0|\n1585766058000|3489692692|  3.0|0.135489| 13.879|17179869184|      0|\n1587497991000|3489692692|  2.0|0.159999|21.6999|17179869185|      0|\n1587864812000|3489692692|  2.0|0.959999|359.649|25769803776|      0|\n1587581329000|3489692692|  1.0|1.039999|336.209|34359738368|      0|\n1587581339000|3489692692|  3.0|1.039999|336.299|42949672960|      1|\n1587581329000|3489692692|  1.0|2.799999|336.209|42949672961|      1|\n+-------------+----------+-----+--------+-------+-----------+-------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["w2=Window().partitionBy().orderBy(\"rowNum\")\n\ndf.withColumn(\"rowNum\", F.monotonically_increasing_id())\\\n  .withColumn(\"inc_sum\", F.sum(F.when((F.col(\"state\")==3) & (F.lag(\"state\").over(w2)!=3),F.lit(1)).otherwise(F.lit(0)))\\\n              .over(w2))\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+----------+-----+--------+-------+-----------+-------+\n     epoch_ms|        ID|state| value 1|value 2|     rowNum|inc_sum|\n+-------------+----------+-----+--------+-------+-----------+-------+\n1585766054000|3489692692|  3.0|0.159999|7.58996|          0|      0|\n1585766055000|3489692692|  3.0|0.239999|11.2699| 8589934592|      0|\n1585766058000|3489692692|  3.0|0.135489| 13.879|17179869184|      0|\n1587497991000|3489692692|  2.0|0.159999|21.6999|17179869185|      0|\n1587864812000|3489692692|  2.0|0.959999|359.649|25769803776|      0|\n1587581329000|3489692692|  1.0|1.039999|336.209|34359738368|      0|\n1587581339000|3489692692|  3.0|1.039999|336.299|42949672960|      1|\n1587581329000|3489692692|  1.0|2.799999|336.209|42949672961|      1|\n1588088096000|3489692670|  3.0|2.869564|285.963|51539607552|      2|\n1588088099000|3489692670|  2.0|0.758753|299.578|60129542144|      2|\n1588088199000|3489692670|  1.0|3.965424|5.89677|60129542145|      2|\n+-------------+----------+-----+--------+-------+-----------+-------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["list=[[1588119659000,3489692692,3.0,0.239999,11.2699], \n      [1587497991000,3489692692,2.0,0.159999,21.6999],\n      [1587864812000,3489692692,2.0,0.959999,359.649], \n      [1587581329000,3489692692,1.0,1.039999,336.209], \n      [1587581329000,3489692692,3.0,1.039999,336.299],  \n      [1587581329000,3489692692,1.0,2.799999,336.209]]\n\ndf=spark.createDataFrame(list,['epoch_ms','ID','state','value 1','value 2'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+----------+-----+--------+-------+\n     epoch_ms|        ID|state| value 1|value 2|\n+-------------+----------+-----+--------+-------+\n1588119659000|3489692692|  3.0|0.239999|11.2699|\n1587497991000|3489692692|  2.0|0.159999|21.6999|\n1587864812000|3489692692|  2.0|0.959999|359.649|\n1587581329000|3489692692|  1.0|1.039999|336.209|\n1587581329000|3489692692|  3.0|1.039999|336.299|\n1587581329000|3489692692|  1.0|2.799999|336.209|\n+-------------+----------+-----+--------+-------+\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["df.show() #sampledata\n#+-------------+----------+-----+--------+-------+\n#|     epoch_ms|        ID|state| value 1|value 2|\n#+-------------+----------+-----+--------+-------+\n#|1588119659000|3489692692|  3.0|0.239999|11.2699|\n#|1587497991000|3489692692|  2.0|0.159999|21.6999|\n#|1587864812000|3489692692|  2.0|0.959999|359.649|\n#|1587581329000|3489692692|  1.0|1.039999|336.209|\n#|1587581329000|3489692692|  3.0|1.039999|336.299|\n#|1587581329000|3489692692|  1.0|2.799999|336.209|\n#+-------------+----------+-----+--------+-------+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"ID\").orderBy(F.lit(1))\nw2=Window().partitionBy(\"ID\").orderBy(\"rowNum\")\n\ndf.withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"inc_sum\", F.sum(F.when(F.col(\"state\")==3,F.lit(1)).otherwise(F.lit(0))).over(w2))\\\n  .groupBy(\"inc_sum\").agg(F.first(\"ID\").alias(\"ID\"),\\\n                          F.max(\"epoch_ms\").alias(\"start_epoch\"),\\\n                          F.min(\"epoch_ms\").alias(\"end_epoch\"),F.max(\"value 1\").alias(\"max_value1\"),\\\n                          F.max(\"value 2\").alias(\"max_value2\")).drop(\"inc_sum\").show()\n\n#+-------+----------+-------------+-------------+----------+----------+\n#|inc_sum|        ID|  start_epoch|    end_epoch|max_value1|max_value2|\n#+-------+----------+-------------+-------------+----------+----------+\n#|      1|3489692692|1588119659000|1587497991000|  1.039999|   359.649|\n#|      2|3489692692|1587581329000|1587581329000|  2.799999|   336.299|\n#+-------+----------+-------------+-------------+----------+----------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+----------+-------------+-------------+----------+----------+\ninc_sum|        ID|  start_epoch|    end_epoch|max_value1|max_value2|\n+-------+----------+-------------+-------------+----------+----------+\n      1|3489692692|1588119659000|1587497991000|  1.039999|   359.649|\n      2|3489692692|1587581329000|1587581329000|  2.799999|   336.299|\n+-------+----------+-------------+-------------+----------+----------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["df.orderBy(\"epoch_ms\",\"state\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+----------+-----+--------+-------+\n     epoch_ms|        ID|state| value 1|value 2|\n+-------------+----------+-----+--------+-------+\n1587497991000|3489692692|  2.0|0.159999|21.6999|\n1587581329000|3489692692|  1.0|1.039999|336.209|\n1587581329000|3489692692|  1.0|2.799999|336.209|\n1587581329000|3489692692|  3.0|1.039999|336.299|\n1587864812000|3489692692|  2.0|0.959999|359.649|\n1588119659000|3489692692|  3.0|0.239999|11.2699|\n+-------------+----------+-----+--------+-------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["l = [{'A': 'val1', 'B': 5}, {'A': 'val4', 'B': 2}]\n\nlist=[['val1',5],\n     ['val1',1],\n     ['val1',3],\n     ['val4',2],\n     ['val1',4],\n     ['val1',1]]\n\ndf=spark.createDataFrame(list,['A','B'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+\n   A|  B|\n+----+---+\nval1|  5|\nval1|  1|\nval1|  3|\nval4|  2|\nval1|  4|\nval1|  1|\n+----+---+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["' and '.join([\"A\"+\"=\"+\"'\"+d['A']+\"'\"+\" and \"+\"B\"+\"=\"+str(d['B']) for d in l])\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: &#34;A=&#39;val1&#39; and B=5 and A=&#39;val4&#39; and B=2&#34;</div>"]}}],"execution_count":25},{"cell_type":"code","source":["l = [{'A': 'val1', 'B': 5}, {'A': 'val4', 'B': 2}]\ndf.show()\n#+----+---+\n#|   A|  B|\n#+----+---+\n#|val1|  5|\n#|val1|  1|\n#|val1|  3|\n#|val4|  2|\n#|val1|  4|\n#|val1|  1|\n#+----+---+\n\ndf.filter(' or '.join([\"A\"+\"=\"+\"'\"+d['A']+\"'\"+\" and \"+\"B\"+\"=\"+str(d['B']) for d in l])).show()\n\n#+----+---+\n#|   A|  B|\n#+----+---+\n#|val1|  5|\n#|val4|  2|\n#+----+---+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+\n   A|  B|\n+----+---+\nval1|  5|\nval4|  2|\n+----+---+\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"stackhelp52","notebookId":9759589195107},"nbformat":4,"nbformat_minor":0}
