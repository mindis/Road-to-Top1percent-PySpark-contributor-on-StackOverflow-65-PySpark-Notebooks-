{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["df = spark.createDataFrame(\n[('11','1','1152397078','VVVVM',1,'3/5/2020',1,5),\n('11','1','1152944770','VVVVV',1,'3/6/2020',2,5),\n('11','1','1153856408','VVVVV',1,'3/15/2020',3,5),\n('11','2','1155884040','MVVVV',1,'4/2/2020',4,5),\n('11','2','1156854301','MMVVV',0,'4/17/2020',5,5),\n('12','1','1156854302','VVVVM',1,'3/6/2020',1,3),\n('12','1','1156854303','VVVVV',1,'3/7/2020',2,3),\n('12','2','1156854304','MVVVV',1,'3/16/2020',3,3)\n]\n,[\"consumer_id\",\"product_id\",\"TRX_ID\",\"pattern\",\"loyal\",\"trx_date\",\"row_num\",\"mx\"])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+----------+-------+-----+---------+-------+---+\nconsumer_id|product_id|    TRX_ID|pattern|loyal| trx_date|row_num| mx|\n+-----------+----------+----------+-------+-----+---------+-------+---+\n         11|         1|1152397078|  VVVVM|    1| 3/5/2020|      1|  5|\n         11|         1|1152944770|  VVVVV|    1| 3/6/2020|      2|  5|\n         11|         1|1153856408|  VVVVV|    1|3/15/2020|      3|  5|\n         11|         2|1155884040|  MVVVV|    1| 4/2/2020|      4|  5|\n         11|         2|1156854301|  MMVVV|    0|4/17/2020|      5|  5|\n         12|         1|1156854302|  VVVVM|    1| 3/6/2020|      1|  3|\n         12|         1|1156854303|  VVVVV|    1| 3/7/2020|      2|  3|\n         12|         2|1156854304|  MVVVV|    1|3/16/2020|      3|  3|\n+-----------+----------+----------+-------+-----+---------+-------+---+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["from pypsark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw = Window().partitionBy(\"consumer_id\").orderBy('row_num')\nlead=F.lead(\"loyal\").over(w)\ndf.withColumn(\"Flag\", F.when(((F.col(\"loyal\")==1)\\\n                             &((lead==0)|(lead.isNull()))),F.lit(1))\\\n                       .otherwise(F.lit(0))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------+----------+-------+-----+---------+-------+---+----+\nconsumer_id|product_id|    TRX_ID|pattern|loyal| trx_date|row_num| mx|Flag|\n+-----------+----------+----------+-------+-----+---------+-------+---+----+\n         11|         1|1152397078|  VVVVM|    1| 3/5/2020|      1|  5|   0|\n         11|         1|1152944770|  VVVVV|    1| 3/6/2020|      2|  5|   0|\n         11|         1|1153856408|  VVVVV|    1|3/15/2020|      3|  5|   0|\n         11|         2|1155884040|  MVVVV|    1| 4/2/2020|      4|  5|   1|\n         11|         2|1156854301|  MMVVV|    0|4/17/2020|      5|  5|   0|\n         12|         1|1156854302|  VVVVM|    1| 3/6/2020|      1|  3|   0|\n         12|         1|1156854303|  VVVVV|    1| 3/7/2020|      2|  3|   0|\n         12|         2|1156854304|  MVVVV|    1|3/16/2020|      3|  3|   1|\n+-----------+----------+----------+-------+-----+---------+-------+---+----+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["list=[[1,23,90],\n     [2,12,45],\n     [2,38,80],\n     [2,91,62],\n     [1,11,21],\n     [1,29,57],\n     [1,13,68],\n     [2,14,19]]\n\ndf=spark.createDataFrame(list,['id','column_b','column_a'])\n\n\ndf.select(\"id\",\"column_a\",\"column_b\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+--------+\n id|column_a|column_b|\n+---+--------+--------+\n  1|      90|      23|\n  2|      45|      12|\n  2|      80|      38|\n  2|      62|      91|\n  1|      21|      11|\n  1|      57|      29|\n  1|      68|      13|\n  2|      19|      14|\n+---+--------+--------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["You could only use **`collect_list`** over **`only one`** **`window`** and then use *higher order function* **`aggregate`** to get your desired result **`(sum/sum).`**\n\n    df.show() #sample dataframe\n    #+---+--------+--------+\n    #| id|column_a|column_b|\n    #+---+--------+--------+\n    #|  1|      90|      23|\n    #|  2|      45|      12|\n    #|  2|      80|      38|\n    #|  2|      62|      91|\n    #|  1|      21|      11|\n    #|  1|      57|      29|\n    #|  1|      68|      13|\n    #|  2|      19|      14|\n    #+---+--------+--------+\n    \n    from pyspark.sql import functions as F\n    from pyspark.sql.window import Window\n    \n    \n    window=Window().partitionBy(\"id\")\n    df.withColumn(\"column_c\",F.collect_list(F.array(\"column_a\",\"column_b\")).over(window))\\\n      .withColumn(\"column_c\", F.expr(\"\"\"aggregate(column_c,0,(acc,x)-> int(x[0])+acc)/\\\n                                   aggregate(column_c,0,(acc,x)-> int(x[1])+acc)\"\"\")).show()\n\n    #+---+--------+--------+------------------+\n    #| id|column_b|column_a|          column_c|\n    #+---+--------+--------+------------------+\n    #|  1|      23|      90|3.1052631578947367|\n    #|  1|      11|      21|3.1052631578947367|\n    #|  1|      29|      57|3.1052631578947367|\n    #|  1|      13|      68|3.1052631578947367|\n    #|  2|      12|      45|1.3290322580645162|\n    #|  2|      38|      80|1.3290322580645162|\n    #|  2|      91|      62|1.3290322580645162|\n    #|  2|      14|      19|1.3290322580645162|\n    #+---+--------+--------+------------------+\n\n**`Instead of:`**(2 windows)\n\n    window=Window().partitionBy(\"id\")\n    df.withColumn(\"colum_c\",F.sum(F.col(\"column_a\")).over(window)\\\n                                  /F.sum(F.col(\"column_b\")).over(window)).show()\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import functions as f\n\n  \nlist=[[1,23,90,'2019-2-23'],\n     [1,12,45,'2019-2-28'],\n     [1,38,80,'2019-3-21'],\n     [1,91,62,'2019-3-24'],\n     [2,11,21,'2019-3-29'],\n     [2,29,57,'2019-1-08'],\n     [2,13,68,'2019-1-12'],\n     [2,14,19,'2019-1-14']]\n\ndf=spark.createDataFrame(list,['account_id','column_b','column_a','event_date'])\n\n\ndf=df.select(\"account_id\",\"column_a\",\"column_b\",\"event_date\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["df.show() #sample data\n\n#+----------+--------+--------+----------+\n#|account_id|column_a|column_b|event_date|\n#+----------+--------+--------+----------+\n#|         1|      90|      23| 2019-2-23|\n#|         1|      45|      12| 2019-2-28|\n#|         1|      80|      38| 2019-3-21|\n#|         1|      62|      91| 2019-3-24|\n#|         2|      21|      11| 2019-3-29|\n#|         2|      57|      29| 2019-1-08|\n#|         2|      68|      13| 2019-1-12|\n#|         2|      19|      14| 2019-1-14|\n#+----------+--------+--------+----------+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\ndays = lambda i: (i-1) * 86400\n\nwindow =\\\n    Window()\\\n    .partitionBy(f.col(\"account_id\"))\\\n    .orderBy(f.col(\"event_date\").cast(\"timestamp\").cast(\"long\"))\\\n    .rangeBetween(-days(7), 0)\n\ndf.withColumn(\"column_c\",F.collect_list(F.array(\"column_a\",\"column_b\")).over(window))\\\n  .withColumn(\"column_c\", F.expr(\"\"\"aggregate(column_c,0,(acc,x)-> int(x[0])+acc)/\\\n                               aggregate(column_c,0,(acc,x)-> int(x[1])+acc)\"\"\")).show()\n\n#+----------+--------+--------+----------+------------------+\n#|account_id|column_b|column_a|event_date|          column_c|\n#+----------+--------+--------+----------+------------------+\n#|         1|      23|      90| 2019-2-23|3.9130434782608696|\n#|         1|      12|      45| 2019-2-28| 3.857142857142857|\n#|         1|      38|      80| 2019-3-21|2.1052631578947367|\n#|         1|      91|      62| 2019-3-24|1.1007751937984496|\n#|         2|      29|      57| 2019-1-08|1.9655172413793103|\n#|         2|      13|      68| 2019-1-12|2.9761904761904763|\n#|         2|      14|      19| 2019-1-14|2.5714285714285716|\n#|         2|      11|      21| 2019-3-29|1.9090909090909092|\n#+----------+--------+--------+----------+------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------+--------+----------+\naccount_id|column_a|column_b|event_date|\n+----------+--------+--------+----------+\n         1|      90|      23| 2019-2-23|\n         1|      45|      12| 2019-2-28|\n         1|      80|      38| 2019-3-21|\n         1|      62|      91| 2019-3-24|\n         2|      21|      11| 2019-3-29|\n         2|      57|      29| 2019-1-08|\n         2|      68|      13| 2019-1-12|\n         2|      19|      14| 2019-1-14|\n+----------+--------+--------+----------+\n\n+----------+--------+--------+----------+------------------+\naccount_id|column_a|column_b|event_date|          column_c|\n+----------+--------+--------+----------+------------------+\n         1|      90|      23| 2019-2-23|3.9130434782608696|\n         1|      45|      12| 2019-2-28| 3.857142857142857|\n         1|      80|      38| 2019-3-21|2.1052631578947367|\n         1|      62|      91| 2019-3-24|1.1007751937984496|\n         2|      57|      29| 2019-1-08|1.9655172413793103|\n         2|      68|      13| 2019-1-12|2.9761904761904763|\n         2|      19|      14| 2019-1-14|2.5714285714285716|\n         2|      21|      11| 2019-3-29|1.9090909090909092|\n+----------+--------+--------+----------+------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.window import Window\n\ndays = lambda i: i * 86400\n\nwindow =\\\n    Window()\\\n    .partitionBy(f.col(\"account_id\"))\\\n    .orderBy(f.col(\"event_date\").cast(\"timestamp\").cast(\"long\"))\\\n    .rangeBetween(-days(7), 0)\n\ndf.withColumn(\"column_c\",f.collect_list(f.array(\"column_a\",\"column_b\")).over(window))\\\n  .withColumn(\"column_c\", f.expr(\"\"\"aggregate(column_c,0,(acc,x)-> int(x[0])+acc)/\\\n                               aggregate(column_c,0,(acc,x)-> int(x[1])+acc)\"\"\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------+--------+----------+------------------+\naccount_id|column_a|column_b|event_date|          column_c|\n+----------+--------+--------+----------+------------------+\n         1|      90|      23| 2019-2-23|3.9130434782608696|\n         1|      45|      12| 2019-2-28| 3.857142857142857|\n         1|      80|      38| 2019-3-21|2.1052631578947367|\n         1|      62|      91| 2019-3-24|1.1007751937984496|\n         2|      57|      29| 2019-1-08|1.9655172413793103|\n         2|      68|      13| 2019-1-12|2.9761904761904763|\n         2|      19|      14| 2019-1-14|2.5714285714285716|\n         2|      21|      11| 2019-3-29|1.9090909090909092|\n+----------+--------+--------+----------+------------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.window import Window\n\ndays = lambda i: i * 86400\n\nwindow =\\\n    Window()\\\n    .partitionBy(f.col(\"account_id\"))\\\n    .orderBy(f.col(\"event_date\").cast(\"timestamp\").cast(\"long\"))\\\n    .rangeBetween(-days(7), 0)\n\ndf.withColumn(\"column_c\",f.collect_list(f.struct\\\n                                        (f.col(\"column_a\").alias(\"a\"),\\\n                                         f.col(\"column_b\").alias(\"b\")))\\\n                                                          .over(window))\\\n  .withColumn(\"column_c\", f.expr(\"\"\"aggregate(column_c,0,(acc,x)-> int(x.a)+acc)/\\\n                               aggregate(column_c,0,(acc,x)-> int(x.b)+acc)\"\"\")).explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Project [account_id#7210L, column_a#7212L, column_b#7211L, event_date#7213, (cast(aggregate(column_c#8464, 0, lambdafunction((cast(lambda x#8472.a as int) + lambda acc#8471), lambda acc#8471, lambda x#8472, false), lambdafunction(lambda id#8473, lambda id#8473, false)) as double) / cast(aggregate(column_c#8464, 0, lambdafunction((cast(lambda x#8475.b as int) + lambda acc#8474), lambda acc#8474, lambda x#8475, false), lambdafunction(lambda id#8476, lambda id#8476, false)) as double)) AS column_c#8470]\n+- Window [account_id#7210L, column_a#7212L, column_b#7211L, event_date#7213, collect_list(_w1#8483, 0, 0) windowspecdefinition(account_id#7210L, _w0#8482L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) AS column_c#8464], [account_id#7210L], [_w0#8482L ASC NULLS FIRST]\n   +- Sort [account_id#7210L ASC NULLS FIRST, _w0#8482L ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(account_id#7210L, 200), [id=#2052]\n         +- *(1) Project [account_id#7210L, column_a#7212L, column_b#7211L, event_date#7213, cast(cast(event_date#7213 as timestamp) as bigint) AS _w0#8482L, named_struct(a, column_a#7212L, b, column_b#7211L) AS _w1#8483]\n            +- *(1) Scan ExistingRDD[account_id#7210L,column_b#7211L,column_a#7212L,event_date#7213]\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["days = lambda i: i * 86400\n\nwindow =\\\n    Window()\\\n    .partitionBy(f.col(\"account_id\"))\\\n    .orderBy(f.col(\"event_date\").cast(\"timestamp\").cast(\"long\"))\\\n    .rangeBetween(-days(7), 0)\n\ndf.withColumn(\"colum_c\",F.sum(F.col(\"column_a\")).over(window)\\\n                              /F.sum(F.col(\"column_b\")).over(window)).explain()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nWindow [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, (cast(sum(column_a#4850L) windowspecdefinition(account_id#4848L, _w0#6804L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double) / cast(sum(column_b#4849L) windowspecdefinition(account_id#4848L, _w0#6804L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double)) AS colum_c#6798], [account_id#4848L], [_w0#6804L ASC NULLS FIRST]\n+- Sort [account_id#4848L ASC NULLS FIRST, _w0#6804L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(account_id#4848L, 200), [id=#1453]\n      +- *(1) Project [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, cast(cast(event_date#4851 as timestamp) as bigint) AS _w0#6804L]\n         +- *(1) Scan ExistingRDD[account_id#4848L,column_b#4849L,column_a#4850L,event_date#4851]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["== Physical Plan ==\nWindow [id#16L, column_b#17L, column_a#18L, (cast(sum(column_a#18L) windowspecdefinition(id#16L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) as double) / cast(sum(column_b#17L) windowspecdefinition(id#16L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) as double)) AS colum_c#2635], [id#16L]\n+- Sort [id#16L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(id#16L, 200), [id=#722]\n      +- *(1) Scan ExistingRDD[id#16L,column_b#17L,column_a#18L]"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+--------+------------------+\n id|column_b|column_a|          column_c|\n+---+--------+--------+------------------+\n  1|      23|      90|3.1052631578947367|\n  1|      11|      21|3.1052631578947367|\n  1|      29|      57|3.1052631578947367|\n  1|      13|      68|3.1052631578947367|\n  2|      12|      45|1.3290322580645162|\n  2|      38|      80|1.3290322580645162|\n  2|      91|      62|1.3290322580645162|\n  2|      14|      19|1.3290322580645162|\n+---+--------+--------+------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["df.withColumn(\"colum_c\",F.sum(F.col(\"column_a\")).over(window)\\\n                              /F.sum(F.col(\"column_b\")).over(window)).explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nWindow [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, (cast(sum(column_a#4850L) windowspecdefinition(account_id#4848L, _w0#6818L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double) / cast(sum(column_b#4849L) windowspecdefinition(account_id#4848L, _w0#6818L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double)) AS colum_c#6812], [account_id#4848L], [_w0#6818L ASC NULLS FIRST]\n+- Sort [account_id#4848L ASC NULLS FIRST, _w0#6818L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(account_id#4848L, 200), [id=#1492]\n      +- *(1) Project [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, cast(cast(event_date#4851 as timestamp) as bigint) AS _w0#6818L]\n         +- *(1) Scan ExistingRDD[account_id#4848L,column_b#4849L,column_a#4850L,event_date#4851]\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["== Physical Plan ==\nWindow [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, (cast(sum(column_a#4850L) windowspecdefinition(account_id#4848L, _w0#6818L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double) / cast(sum(column_b#4849L) windowspecdefinition(account_id#4848L, _w0#6818L ASC NULLS FIRST, specifiedwindowframe(RangeFrame, -604800, currentrow$())) as double)) AS colum_c#6812], [account_id#4848L], [_w0#6818L ASC NULLS FIRST]\n+- Sort [account_id#4848L ASC NULLS FIRST, _w0#6818L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(account_id#4848L, 200), [id=#1492]\n      +- *(1) Project [account_id#4848L, column_b#4849L, column_a#4850L, event_date#4851, cast(cast(event_date#4851 as timestamp) as bigint) AS _w0#6818L]\n         +- *(1) Scan ExistingRDD[account_id#4848L,column_b#4849L,column_a#4850L,event_date#4851]"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["window=Window().partitionBy(\"id\")\ndf.withColumn(\"colum_c\",F.sum(F.col(\"column_a\")/F.col(\"column_b\")).over(window)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+--------+--------+-----------------+\n id|column_b|column_a|          colum_c|\n+---+--------+--------+-----------------+\n  1|      23|      90|13.01842085950032|\n  1|      11|      21|13.01842085950032|\n  1|      29|      57|13.01842085950032|\n  1|      13|      68|13.01842085950032|\n  2|      12|      45|7.893724696356275|\n  2|      38|      80|7.893724696356275|\n  2|      91|      62|7.893724696356275|\n  2|      14|      19|7.893724696356275|\n+---+--------+--------+-----------------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["window=Window().partitionBy(\"id\")\ndf.withColumn(\"colum_c\",F.sum(F.col(\"column_a\")).over(window)/F.sum(F.col(\"column_b\")).over(window)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nWindow [id#16L, column_b#17L, column_a#18L, (cast(sum(column_a#18L) windowspecdefinition(id#16L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) as double) / cast(sum(column_b#17L) windowspecdefinition(id#16L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) as double)) AS colum_c#2635], [id#16L]\n+- Sort [id#16L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(id#16L, 200), [id=#722]\n      +- *(1) Scan ExistingRDD[id#16L,column_b#17L,column_a#18L]\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql import functions as F"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["list1=[['abcde00000qMQ00001',['casa', 'alejado', 'buen', 'gusto']],\n       ['abcde00000qMq00002',['clientes', 'contentos', 'servi']],\n       ['abcde00000qMQ00003',['resto', 'bien']               ]]\n\ndf1=spark.createDataFrame(list1,['ID','MeaningfulWords'])\n\ndf1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+--------------------+\n                ID|     MeaningfulWords|\n+------------------+--------------------+\nabcde00000qMQ00001|[casa, alejado, b...|\nabcde00000qMq00002|[clientes, conten...|\nabcde00000qMQ00003|       [resto, bien]|\n+------------------+--------------------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["\n\nlist2=[[ 1.68,     'casa'],\n       [  2.8,  'alejado'],\n       [ 1.03,     'buen'],\n       [ 3.68,    'gusto'],\n       [ 0.68, 'clientes'],\n       [  2.1,'contentos'],\n       [ 2.68,    'servi'],\n       [ 1.18,    'resto'],\n       [ 1.98,    'bien']]\n\ndf2=spark.createDataFrame(list2,['score','word'])\n\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---------+\nscore|     word|\n+-----+---------+\n 1.68|     casa|\n  2.8|  alejado|\n 1.03|     buen|\n 3.68|    gusto|\n 0.68| clientes|\n  2.1|contentos|\n 2.68|    servi|\n 1.18|    resto|\n 1.98|     bien|\n+-----+---------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf1.join(df2, F.expr(\"\"\"array_contains(MeaningfulWords,word)\"\"\"))\\\n   .groupBy(\"ID\").agg(F.first(\"MeaningfulWords\").alias(\"MeaningfullWords\")\\\n                      ,F.collect_list(\"score\").alias(\"ScoreList\")\\\n                      ,F.mean(\"score\").alias(\"MeanScore\"))\\\n                      .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+----------------------------+-----------------------+------------------+\nID                |MeaningfullWords            |ScoreList              |MeanScore         |\n+------------------+----------------------------+-----------------------+------------------+\nabcde00000qMQ00003|[resto, bien]               |[1.18, 1.98]           |1.58              |\nabcde00000qMq00002|[clientes, contentos, servi]|[0.68, 2.1, 2.68]      |1.8200000000000003|\nabcde00000qMQ00001|[casa, alejado, buen, gusto]|[1.68, 2.8, 1.03, 3.68]|2.2975            |\n+------------------+----------------------------+-----------------------+------------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf1.join(df2, F.expr(\"\"\"array_contains(MeaningfulWords,word)\"\"\"))\\\n   .groupBy(\"ID\").agg(F.first(\"MeaningfulWords\").alias(\"MeaningfullWords\")\\\n                      ,F.collect_list(\"score\").alias(\"ScoreList\"))\\\n   .withColumn(\"MeanScore\", F.expr(\"\"\"aggregate((transform(ScoreList,x->double(x)))\\\n                                      ,0,(x,acc)-> acc+x,acc->(acc/ size(Scorelist)))\"\"\")).show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["df1.show()\n\n#+------------------+----------------------------+\n#|ID                |MeaningfulWords             |\n#+------------------+----------------------------+\n#|abcde00000qMQ00001|[casa, alejado, buen, gusto]|\n#|abcde00000qMq00002|[clientes, contentos, servi]|\n#|abcde00000qMQ00003|[resto, bien]               |\n#+------------------+----------------------------+\n\ndf2.show()\n\n#+-----+---------+\n#|score|     word|\n#+-----+---------+\n#| 1.68|     casa|\n#|  2.8|  alejado|\n#| 1.03|     buen|\n#| 3.68|    gusto|\n#| 0.68| clientes|\n#|  2.1|contentos|\n#| 2.68|    servi|\n#| 1.18|    resto|\n#| 1.98|     bien|\n#+-----+---------+\n\n\nfrom pyspark.sql import functions as F\ndf1.join(df2, F.expr(\"\"\"array_contains(MeaningfulWords,word)\"\"\"))\\\n   .groupBy(\"ID\").agg(F.first(\"MeaningfulWords\").alias(\"MeaningfullWords\")\\\n                      ,F.collect_list(\"score\").alias(\"ScoreList\")\\\n                      ,F.mean(\"score\").alias(\"MeanScore\"))\\\n                      .show(truncate=False)\n\n#+------------------+----------------------------+-----------------------+------------------+\n#|ID                |MeaningfullWords            |ScoreList              |MeanScore         |\n#+------------------+----------------------------+-----------------------+------------------+\n#|abcde00000qMQ00003|[resto, bien]               |[1.18, 1.98]           |1.58              |\n#|abcde00000qMq00002|[clientes, contentos, servi]|[0.68, 2.1, 2.68]      |1.8200000000000003|\n#|abcde00000qMQ00001|[casa, alejado, buen, gusto]|[1.68, 2.8, 1.03, 3.68]|2.2975            |\n#+------------------+----------------------------+-----------------------+------------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+----------------------------+-----------------------+---------+\nID                |MeaningfullWords            |ScoreList              |MeanScore|\n+------------------+----------------------------+-----------------------+---------+\nabcde00000qMQ00003|[resto, bien]               |[1.18, 1.98]           |3.16     |\nabcde00000qMq00002|[clientes, contentos, servi]|[0.68, 2.1, 2.68]      |5.46     |\nabcde00000qMQ00001|[casa, alejado, buen, gusto]|[1.68, 2.8, 1.03, 3.68]|9.19     |\n+------------------+----------------------------+-----------------------+---------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf1.join(df2, F.expr(\"\"\"array_contains(MeaningfulWords,word)\"\"\"))\\\n   .groupBy(\"ID\").agg(F.first(\"MeaningfulWords\").alias(\"MeaningfullWords\")\\\n                      ,F.collect_list(\"score\").alias(\"ScoreList\"))\\\n   .withColumn(\"MeanScore\", F.expr(\"\"\"aggregate((transform(ScoreList,x->int(x*100)))\\\n                                      ,0,(x,acc)-> acc+x,acc->(acc/100)/ size(Scorelist))\"\"\")).show(truncate=False)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+----------------------------+-----------------------+---------+\nID                |MeaningfullWords            |ScoreList              |MeanScore|\n+------------------+----------------------------+-----------------------+---------+\nabcde00000qMQ00003|[resto, bien]               |[1.18, 1.98]           |1.58     |\nabcde00000qMq00002|[clientes, contentos, servi]|[0.68, 2.1, 2.68]      |1.82     |\nabcde00000qMQ00001|[casa, alejado, buen, gusto]|[1.68, 2.8, 1.03, 3.68]|2.2975   |\n+------------------+----------------------------+-----------------------+---------+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["list=[['A',          'yes'        ,1      ,1000],\n      ['A',           'no'         ,2      ,100],\n      ['A' ,          'no'         ,3      ,100],\n      ['A'  ,         'no'         ,4      ,100],\n      ['A'   ,        'no'         ,5      ,100],\n      ['B'    ,       'yes'        ,1      ,2000],\n      ['B'     ,     'no'         ,2      ,200],\n      ['B'       ,    'no'         ,3      ,100],\n      ['B'      ,     'no'         ,4      ,100],\n      ['B'       ,    'no'         ,5      ,200],\n      ['C'         ,  'yes'        ,1      ,3000],\n      ['C'        ,   'no'         ,2      ,100],\n      ['C'          , 'no'         ,3      ,100],\n      ['C'           ,'no'         ,4      ,200],\n      ['C'           ,'no'         ,5      ,200]]\n\ndf=spark.createDataFrame(list,['Group','Risk Group','State','Value'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+-----+-----+\nGroup|Risk Group|State|Value|\n+-----+----------+-----+-----+\n    A|       yes|    1| 1000|\n    A|        no|    2|  100|\n    A|        no|    3|  100|\n    A|        no|    4|  100|\n    A|        no|    5|  100|\n    B|       yes|    1| 2000|\n    B|        no|    2|  200|\n    B|        no|    3|  100|\n    B|        no|    4|  100|\n    B|        no|    5|  200|\n    C|       yes|    1| 3000|\n    C|        no|    2|  100|\n    C|        no|    3|  100|\n    C|        no|    4|  200|\n    C|        no|    5|  200|\n+-----+----------+-----+-----+\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"Group\")\ndf.withColumn(\"Risk Group Value\", F.sum(F.when(F.col(\"Risk Group\")=='yes',F.col(\"Value\"))).over(w))\\\n  .filter(F.col(\"Risk Group\")!='yes')\\\n  .orderBy(\"Group\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----------+-----+-----+----------------+\nGroup|Risk Group|State|Value|Risk Group Value|\n+-----+----------+-----+-----+----------------+\n    A|        no|    2|  100|            1000|\n    A|        no|    3|  100|            1000|\n    A|        no|    4|  100|            1000|\n    A|        no|    5|  100|            1000|\n    B|        no|    2|  200|            2000|\n    B|        no|    3|  100|            2000|\n    B|        no|    4|  100|            2000|\n    B|        no|    5|  200|            2000|\n    C|        no|    2|  100|            3000|\n    C|        no|    5|  200|            3000|\n    C|        no|    3|  100|            3000|\n    C|        no|    4|  200|            3000|\n+-----+----------+-----+-----+----------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["Group - Risk Group - State - Value - Risk Group Value\nA           no         2      100         1000\nA           no         3      100         1000\nA           no         4      100         1000\nA           no         5      100         1000 \nB           no         2      200         2000\nB           no         3      100         2000\nB           no         4      100         2000\nB           no         5      200         2000\nC           no         2      100         3000\nC           no         3      100         3000\nC           no         4      200         3000\nC           no         5      200         3000"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nlist=[[['a', 'b','b','c']],\n      [['b', 'c', 'd']]]\n\n\ndf=spark.createDataFrame(list,['atr_list'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\n    atr_list|\n+------------+\n[a, b, b, c]|\n   [b, c, d]|\n+------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["df.show()\n\n#+------------+\n#|    atr_list|\n#+------------+\n#|[a, b, b, c]|\n#|   [b, c, d]|\n#+------------+\n\nfrom pyspark.sql import functions as F\n\nelements=['a','b','c','d']\n\ncollected=df.withColumn(\"a\", F.when(F.array_contains(\"atr_list\",\"a\"),F.expr(\"\"\"size(filter(atr_list,x->x='a'))\"\"\")).otherwise(F.lit(0)))\\\n  .withColumn(\"b\", F.when(F.array_contains(\"atr_list\",\"b\"),F.expr(\"\"\"size(filter(atr_list,x->x='b'))\"\"\")))\\\n  .withColumn(\"c\", F.when(F.array_contains(\"atr_list\",\"c\"),F.expr(\"\"\"size(filter(atr_list,x->x='c'))\"\"\")))\\\n  .withColumn(\"d\", F.when(F.array_contains(\"atr_list\",\"d\"),F.expr(\"\"\"size(filter(atr_list,x->x='d'))\"\"\")))\\\n  .agg(*[F.sum(x).alias(x) for x in elements])\\\n  .collect()[0]\n\n{elements[i]: [x for x in collected][i] for i in range(len(elements))} \n\nOut[31]: {'a': 1, 'b': 3, 'c': 2, 'd': 1}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\n    atr_list|\n+------------+\n[a, b, b, c]|\n   [b, c, d]|\n+------------+\n\nOut[31]: {&#39;a&#39;: 1, &#39;b&#39;: 3, &#39;c&#39;: 2, &#39;d&#39;: 1}</div>"]}}],"execution_count":28},{"cell_type":"code","source":["df.show()\n\n#+------------+\n#|    atr_list|\n#+------------+\n#|[a, b, b, c]|\n#|   [b, c, d]|\n#+------------+\n\nelements=['a','b','c','d']\ndf.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(atr_list,x->x={}))\"\\\n                                                    .format(\"'\"+y+\"'\"))).alias(y)) for y in elements]))\\\n  .select(*[F.sum(F.col(\"struct.{}.col1\".format(x))).alias(x) for x in elements])\\\n  .collect()[0]\n\n{elements[i]: [x for x in collected][i] for i in range(len(elements))} \n    \n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\n    atr_list|\n+------------+\n[a, b, b, c]|\n   [b, c, d]|\n+------------+\n\nOut[59]: {&#39;a&#39;: 1, &#39;b&#39;: 3, &#39;c&#39;: 2, &#39;d&#39;: 1}</div>"]}}],"execution_count":29},{"cell_type":"code","source":["elements=['a','b','c','d']\ndf.withColumn(\"struct\", F.struct(*[(F.struct(F.expr(\"size(filter(atr_list,x->x={}))\"\\\n                                                    .format(\"'\"+y+\"'\"))).alias(y)) for y in elements]))\\\n  .select(*[F.sum(F.col(\"struct.{}.col1\".format(x))).alias(x) for x in elements])\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+\n  a|  b|  c|  d|\n+---+---+---+---+\n  1|  3|  2|  1|\n+---+---+---+---+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["[\"size(filter(atr_list,x->x={}))\".format(\"'\"+y+\"'\") for y in ['a','b','c','d']]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: [&#34;size(filter(atr_list,x-&gt;x=&#39;a&#39;))&#34;,\n &#34;size(filter(atr_list,x-&gt;x=&#39;b&#39;))&#34;,\n &#34;size(filter(atr_list,x-&gt;x=&#39;c&#39;))&#34;,\n &#34;size(filter(atr_list,x-&gt;x=&#39;d&#39;))&#34;]</div>"]}}],"execution_count":31},{"cell_type":"code","source":["counter"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: {&#39;a&#39;: 1, &#39;b&#39;: 3, &#39;c&#39;: 2, &#39;d&#39;: 1}</div>"]}}],"execution_count":32},{"cell_type":"code","source":["['a','b','c','d'][1]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: &#39;b&#39;</div>"]}}],"execution_count":33},{"cell_type":"code","source":["df.withColumn(\"atr_list\",F.expr(\"\"\"transform(array_distinct(atr_list), x-> struct(x,\\\n                                  size(filter(atr_list,y->y=x))))\"\"\"))\\\n   .show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["df.withColumn(\"atr_list\",F.expr(\"\"\"transform(array_distinct(atr_list), x-> map(x,\\\n                                  size(filter(atr_list,y->y=x))))\"\"\"))\\\n   .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- atr_list: array (nullable = true)\n    |-- element: map (containsNull = false)\n    |    |-- key: string\n    |    |-- value: integer (valueContainsNull = false)\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["df.agg(F.flatten(F.collect_list(\"atr_list\")).alias(\"atr_list\"))\\\n   .withColumn(\"atr_list\",F.expr(\"\"\"transform(array_distinct(atr_list), x-> map(x,\\\n                                  size(filter(atr_list,y->y=x))))\"\"\")).collect()[0][0]\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: [{&#39;a&#39;: 1}, {&#39;b&#39;: 3}, {&#39;c&#39;: 2}, {&#39;d&#39;: 1}]</div>"]}}],"execution_count":36},{"cell_type":"code","source":["df.withColumn(\"atr_list\", F.explode(\"atr_list\"))\\\n  .groupBy(\"atr_list\").count().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+-----+\natr_list|count|\n+--------+-----+\n       d|    1|\n       c|    2|\n       b|    3|\n       a|    1|\n+--------+-----+\n\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["df.withColumn(\"atr_list\", F.expr(\"\"\"transform(array_distinct(atr_list), x-> struct(x as item,\\\n                                  size(filter(atr_list,y->y=x)) as sum)))\"\"\"))\\\n  .select(\"atr_list.item\",\"atr_list.sum\")\\\n  .groupBy(\"item\").agg(F.sum(\"sum\").alias(\"SUM\"))\\\n  .show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["df.withColumn(\"atr_list2\", F.expr(\"\"\"transform(atr_list, x-> map(x,\\\n                               (aggregate(atr_list,0,(y,acc)->IF(x=y,int(acc)+int(1),int(acc))\\\n                               ,acc->acc))))\"\"\"))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["df.withColumn(\"atr_list2\", F.expr(\"\"\"aggregate()\"\"\"))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["df1 = spark.createDataFrame(\n     [\n     ('ll',5),\n     ('yy',6),\n     ],\n     ('x','days')\n    )\n\ndf = spark.createDataFrame(\n[\n    ('ll','2020-01-05','1','10'),\n    ('ll','2020-01-06','1','10'),\n    ('ll','2020-01-07','1','10'),\n    ('ll','2020-01-08','1','10'),\n    ('ll','2020-01-09','1','10'),\n    ('ll','2020-01-10','1','10'),\n    ('ll','2020-01-11','1','10'),\n    ('ll','2020-01-12','1','10'),\n    ('ll','2020-01-05','2','30'),\n    ('ll','2020-01-06','2','30'),\n    ('ll','2020-01-07','2','30'),\n    ('ll','2020-01-08','2','30'),\n    ('ll','2020-01-09','2','30'),\n    ('ll','2020-01-10','2','10'),\n    ('ll','2020-01-11','2','10'),\n    ('ll','2020-01-12','2','10'),\n    ('yy','2020-01-05','1','20'),\n    ('yy','2020-01-06','1','20'),\n    ('yy','2020-01-07','1','20'),\n    ('yy','2020-01-08','1','20'),\n    ('yy','2020-01-09','1','20'),\n    ('yy','2020-01-10','1','20'),\n    ('yy','2020-01-11','1','20'),\n    ('yy','2020-01-12','1','20'),\n    ('yy','2020-01-05','2','40'),\n    ('yy','2020-01-06','2','40'),\n    ('yy','2020-01-07','2','40'),\n    ('yy','2020-01-08','2','40'),\n    ('yy','2020-01-09','2','40'),\n    ('yy','2020-01-10','2','40'),\n    ('yy','2020-01-11','2','40'),\n    ('yy','2020-01-12','2','40')\n     ],\n    ('x','date','flag','value')\n    )\ndf1.show()\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----+\n  x|days|\n+---+----+\n ll|   5|\n yy|   6|\n+---+----+\n\n+---+----------+----+-----+\n  x|      date|flag|value|\n+---+----------+----+-----+\n ll|2020-01-05|   1|   10|\n ll|2020-01-06|   1|   10|\n ll|2020-01-07|   1|   10|\n ll|2020-01-08|   1|   10|\n ll|2020-01-09|   1|   10|\n ll|2020-01-10|   1|   10|\n ll|2020-01-11|   1|   10|\n ll|2020-01-12|   1|   10|\n ll|2020-01-05|   2|   30|\n ll|2020-01-06|   2|   30|\n ll|2020-01-07|   2|   30|\n ll|2020-01-08|   2|   30|\n ll|2020-01-09|   2|   30|\n ll|2020-01-10|   2|   10|\n ll|2020-01-11|   2|   10|\n ll|2020-01-12|   2|   10|\n yy|2020-01-05|   1|   20|\n yy|2020-01-06|   1|   20|\n yy|2020-01-07|   1|   20|\n yy|2020-01-08|   1|   20|\n+---+----------+----+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"x\",\"flag\").orderBy(F.to_date(\"date\",\"yyyy-dd-MM\"))\nw1=Window().partitionBy(\"x\",\"flag\")\ndf.join(df1, ['x'])\\\n  .withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"expected_result\", F.sum(F.when(F.col(\"rowNum\")>F.col(\"days\")\\\n                                     ,F.lit(None)).otherwise(F.col(\"value\")))\\\n                                      .over(w1)).drop(\"days\",\"rowNum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+----+-----+---------------+\n  x|      date|flag|value|expected_result|\n+---+----------+----+-----+---------------+\n ll|2020-01-05|   1|   10|           50.0|\n ll|2020-01-06|   1|   10|           50.0|\n ll|2020-01-07|   1|   10|           50.0|\n ll|2020-01-08|   1|   10|           50.0|\n ll|2020-01-09|   1|   10|           50.0|\n ll|2020-01-10|   1|   10|           50.0|\n ll|2020-01-11|   1|   10|           50.0|\n ll|2020-01-12|   1|   10|           50.0|\n ll|2020-01-05|   2|   30|          150.0|\n ll|2020-01-06|   2|   30|          150.0|\n ll|2020-01-07|   2|   30|          150.0|\n ll|2020-01-08|   2|   30|          150.0|\n ll|2020-01-09|   2|   30|          150.0|\n ll|2020-01-10|   2|   10|          150.0|\n ll|2020-01-11|   2|   10|          150.0|\n ll|2020-01-12|   2|   10|          150.0|\n yy|2020-01-05|   1|   20|          120.0|\n yy|2020-01-06|   1|   20|          120.0|\n yy|2020-01-07|   1|   20|          120.0|\n yy|2020-01-08|   1|   20|          120.0|\n+---+----------+----+-----+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["+---+----------+----+-----+---------------+\n    |  x|      date|flag|value|expected_result|\n    +---+----------+----+-----+---------------+\n    | ll|2020-01-05|   1|   10|             50|\n    | ll|2020-01-06|   1|   10|             50|\n    | ll|2020-01-07|   1|   10|             50|\n    | ll|2020-01-08|   1|   10|             50|\n    | ll|2020-01-09|   1|   10|             50|\n    | ll|2020-01-10|   1|   10|             50|\n    | ll|2020-01-11|   1|   10|             50|\n    | ll|2020-01-12|   1|   10|             50|\n    | ll|2020-01-05|   2|   30|            150|\n    | ll|2020-01-06|   2|   30|            150|\n    | ll|2020-01-07|   2|   30|            150|\n    | ll|2020-01-08|   2|   30|            150|\n    | ll|2020-01-09|   2|   30|            150|\n    | ll|2020-01-10|   2|   10|            150|\n    | ll|2020-01-11|   2|   10|            150|\n    | ll|2020-01-12|   2|   10|            150|\n    | yy|2020-01-05|   1|   20|            120|\n    | yy|2020-01-06|   1|   20|            120|\n    | yy|2020-01-07|   1|   20|            120|\n    | yy|2020-01-08|   1|   20|            120|\n    | yy|2020-01-09|   1|   20|            120|\n    | yy|2020-01-10|   1|   20|            120|\n    | yy|2020-01-11|   1|   20|            120|\n    | yy|2020-01-12|   1|   20|            120|\n    | yy|2020-01-05|   2|   40|            240|\n    | yy|2020-01-06|   2|   40|            240|\n    | yy|2020-01-07|   2|   40|            240|\n    | yy|2020-01-08|   2|   40|            240|\n    | yy|2020-01-09|   2|   40|            240|\n    | yy|2020-01-10|   2|   40|            240|\n    | yy|2020-01-11|   2|   40|            240|\n    | yy|2020-01-12|   2|   40|            240|\n    +---+----------+----+-----+---------------+"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["list=[[1, 1, '12/3/2020'],\n     [1,  0, '12/4/2020'],\n     [1,   1, '12/5/2020'],\n     [1,   1, '12/6/2020'],\n     [1,   0, '12/7/2020'],\n     [1,   1, '12/8/2020'],\n          [1,   0, '12/9/2020'],\n          [1,   0,'12/10/2020'],\n          [1,   0,'12/11/2020'],\n          [1,   1,'12/12/2020'],\n          [2,   1, '12/1/2020'],\n          [2,   0, '12/2/2020'],\n          [2,   0, '12/3/2020'],\n         [2,  1, '12/4/2020']]\n\n\ndf=spark.createDataFrame(list,['customer_id','Flag','trx_date'])\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----+----------+\ncustomer_id|Flag|  trx_date|\n+-----------+----+----------+\n          1|   1| 12/3/2020|\n          1|   0| 12/4/2020|\n          1|   1| 12/5/2020|\n          1|   1| 12/6/2020|\n          1|   0| 12/7/2020|\n          1|   1| 12/8/2020|\n          1|   0| 12/9/2020|\n          1|   0|12/10/2020|\n          1|   0|12/11/2020|\n          1|   1|12/12/2020|\n          2|   1| 12/1/2020|\n          2|   0| 12/2/2020|\n          2|   0| 12/3/2020|\n          2|   1| 12/4/2020|\n+-----------+----+----------+\n\n</div>"]}}],"execution_count":44},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().orderBy(\"customer_id\",\"trx_date\")\nw1=Window().partitionBy(\"Flag2\").orderBy(\"trx_date\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\ndf.withColumn(\"trx_date\", F.to_date(\"trx_date\", \"MM/dd/yyyy\"))\\\n  .withColumn(\"Flag2\", F.sum(\"Flag\").over(w))\\\n  .withColumn(\"destination\", F.when(F.col(\"Flag\")==0, F.first(\"trx_date\").over(w1)).otherwise(F.col(\"trx_date\")))\\\n  .withColumn(\"trx_date\", F.date_format(\"trx_date\",\"MM/dd/yyyy\"))\\\n  .withColumn(\"destination\", F.date_format(\"destination\", \"MM/dd/yyyy\"))\\\n  .orderBy(\"customer_id\",\"trx_date\").drop(\"Flag2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----+----------+-----------+\ncustomer_id|Flag|  trx_date|destination|\n+-----------+----+----------+-----------+\n          1|   1|12/03/2020| 12/03/2020|\n          1|   0|12/04/2020| 12/03/2020|\n          1|   1|12/05/2020| 12/05/2020|\n          1|   1|12/06/2020| 12/06/2020|\n          1|   0|12/07/2020| 12/06/2020|\n          1|   1|12/08/2020| 12/08/2020|\n          1|   0|12/09/2020| 12/08/2020|\n          1|   0|12/10/2020| 12/08/2020|\n          1|   0|12/11/2020| 12/08/2020|\n          1|   1|12/12/2020| 12/12/2020|\n          2|   1|12/01/2020| 12/01/2020|\n          2|   0|12/02/2020| 12/01/2020|\n          2|   0|12/03/2020| 12/01/2020|\n          2|   1|12/04/2020| 12/04/2020|\n+-----------+----+----------+-----------+\n\n</div>"]}}],"execution_count":45},{"cell_type":"code","source":["+-----------+----+----------+-----------+\n|customer_id|Flag|  trx_date|destination|\n+-----------+----+----------+-----------+\n|          1|   1| 12/3/2020|  12/3/2020|\n|          1|   0| 12/4/2020|  12/3/2020|\n|          1|   1| 12/5/2020|  12/5/2020|\n|          1|   1| 12/6/2020|  12/6/2020|\n|          1|   0| 12/7/2020|  12/6/2020|\n|          1|   1| 12/8/2020|  12/8/2020|\n|          1|   0| 12/9/2020|  12/8/2020|\n|          1|   0|12/10/2020|  12/8/2020|\n|          1|   0|12/11/2020|  12/8/2020|\n|          1|   1|12/12/2020| 12/12/2020|\n|          2|   1| 12/1/2020|  12/1/2020|\n|          2|   0| 12/2/2020|  12/1/2020|\n|          2|   0| 12/3/2020|  12/1/2020|\n|          2|   1| 12/4/2020|  12/4/2020|\n+-----------+----+----------+-----------+"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["list=[['2020-01-13T22:22:10.000+0000',1,173736,3043,2996],\n     ['2020-01-13T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-14T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-15T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-13T22:22:10.000+0000',2,257624,1500,53],\n     ['2020-01-13T22:43:19.000+0000',2,257625,1500,65],\n     ['2020-01-13T22:22:10.000+0000',1,173736,3043,2996],\n     ['2020-01-13T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-14T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-15T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-13T22:22:10.000+0000',2,257624,1500,53],\n     ['2020-01-13T22:43:19.000+0000',2,257625,1500,65],\n     ['2020-01-13T22:22:10.000+0000',1,173736,3043,2996],\n     ['2020-01-13T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-14T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-15T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-13T22:22:10.000+0000',2,257624,1500,53],\n     ['2020-01-13T22:43:19.000+0000',2,257625,1500,65],\n     ['2020-01-13T22:22:10.000+0000',1,173736,3043,2996],\n     ['2020-01-13T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-14T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-15T22:43:19.000+0000',1,173775,3042,2996],\n     ['2020-01-13T22:22:10.000+0000',2,257624,1500,53],\n     ['2020-01-13T22:43:19.000+0000',2,257625,1500,65]]\n\ndf=spark.createDataFrame(list,['date','id','stat1','stat2','stat3'])\n\ndf.show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------+---+------+-----+-----+\ndate                        |id |stat1 |stat2|stat3|\n+----------------------------+---+------+-----+-----+\n2020-01-13T22:22:10.000+0000|1  |173736|3043 |2996 |\n2020-01-13T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-14T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-15T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-13T22:22:10.000+0000|2  |257624|1500 |53   |\n2020-01-13T22:43:19.000+0000|2  |257625|1500 |65   |\n2020-01-13T22:22:10.000+0000|1  |173736|3043 |2996 |\n2020-01-13T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-14T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-15T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-13T22:22:10.000+0000|2  |257624|1500 |53   |\n2020-01-13T22:43:19.000+0000|2  |257625|1500 |65   |\n2020-01-13T22:22:10.000+0000|1  |173736|3043 |2996 |\n2020-01-13T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-14T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-15T22:43:19.000+0000|1  |173775|3042 |2996 |\n2020-01-13T22:22:10.000+0000|2  |257624|1500 |53   |\n2020-01-13T22:43:19.000+0000|2  |257625|1500 |65   |\n2020-01-13T22:22:10.000+0000|1  |173736|3043 |2996 |\n2020-01-13T22:43:19.000+0000|1  |173775|3042 |2996 |\n+----------------------------+---+------+-----+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["df1,df2,df3=df.randomSplit([0.3,0.3,0.4])\n\nprint(df1.count(),df2.count(),df3.count())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">6 10 8\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["def split(df):\n    if df.count()>2:\n        df1,df2,df3=df.randomSplit([0.2,0.2,0.2],24)\n        return df1.show(),df2.show(),df3.show()\n    else:\n        return df.show()\n      \nsplit(df)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---+------+-----+-----+\n                date| id| stat1|stat2|stat3|\n+--------------------+---+------+-----+-----+\n2020-01-13T22:22:...|  1|173736| 3043| 2996|\n2020-01-13T22:22:...|  2|257624| 1500|   53|\n2020-01-13T22:43:...|  2|257625| 1500|   65|\n+--------------------+---+------+-----+-----+\n\n+----+---+-----+-----+-----+\ndate| id|stat1|stat2|stat3|\n+----+---+-----+-----+-----+\n+----+---+-----+-----+-----+\n\n+--------------------+---+------+-----+-----+\n                date| id| stat1|stat2|stat3|\n+--------------------+---+------+-----+-----+\n2020-01-13T22:43:...|  1|173775| 3042| 2996|\n2020-01-14T22:43:...|  1|173775| 3042| 2996|\n2020-01-15T22:43:...|  1|173775| 3042| 2996|\n+--------------------+---+------+-----+-----+\n\nOut[3]: (None, None, None)</div>"]}}],"execution_count":49},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3071577957973779&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> df1<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> df2<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df3<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;df3&#39; is not defined</div>"]}}],"execution_count":50},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\",\"date1\").orderBy(\"date\")\nw2=Window().partitionBy(\"id\",\"date1\")\ndf.withColumn(\"date\", F.to_timestamp(\"date\",\"yyyy-MM-dd'T'HH:mm:ss\"))\\\n  .withColumn(\"date1\", F.to_date(\"date\"))\\\n  .withColumn(\"rownum\", F.row_number().over(w))\\\n  .withColumn(\"max\", F.max(\"rownum\").over(w2))\\\n  .filter('rownum=max').drop(\"date1\",\"rownum\",\"max\")\\\n  .orderBy(\"id\",\"date\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---+------+-----+-----+\ndate               |id |stat1 |stat2|stat3|\n+-------------------+---+------+-----+-----+\n2020-01-13 22:43:19|1  |173775|3042 |2996 |\n2020-01-14 22:43:19|1  |173775|3042 |2996 |\n2020-01-15 22:43:19|1  |173775|3042 |2996 |\n2020-01-13 22:43:19|2  |257625|1500 |65   |\n+-------------------+---+------+-----+-----+\n\n</div>"]}}],"execution_count":51},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\",\"date1\").orderBy(\"date\").rangeBetween(Window.unboundedPreceding,Window.unboundedFollowing)\ndf.withColumn(\"date\", F.to_timestamp(\"date\",\"yyyy-MM-dd'T'HH:mm:ss\"))\\\n  .withColumn(\"date1\", F.to_date(\"date\"))\\\n  .withColumn(\"rownum\", F.last(\"date\").over(w))\\\n  .filter('rownum=date').drop(\"date1\",\"rownum\").orderBy(\"id\",\"date\").show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+---+------+-----+-----+\ndate               |id |stat1 |stat2|stat3|\n+-------------------+---+------+-----+-----+\n2020-01-13 22:43:19|1  |173775|3042 |2996 |\n2020-01-14 22:43:19|1  |173775|3042 |2996 |\n2020-01-15 22:43:19|1  |173775|3042 |2996 |\n2020-01-13 22:43:19|2  |257625|1500 |65   |\n+-------------------+---+------+-----+-----+\n\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["df = spark.createDataFrame([['x',1, \"9999\"], ['x',2, \"120\"], ['x',3, \"102\"], ['x',4, \"3000\"],['x',5, \"299\"],['x',6, \"100\"]],['id',\"row_number\", \"time_diff\"])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+---------+\n id|row_number|time_diff|\n+---+----------+---------+\n  x|         1|     9999|\n  x|         2|      120|\n  x|         3|      102|\n  x|         4|     3000|\n  x|         5|      299|\n  x|         6|      100|\n+---+----------+---------+\n\n</div>"]}}],"execution_count":53},{"cell_type":"code","source":["w=Window().partitionBy(\"id\").orderBy(\"row_number\")\ndf.withColumn(\"new_row_number\", F.when(F.col(\"time_diff\")>=160,F.col(\"row_number\")))\\\n  .withColumn(\"new_row_number\", F.last(\"new_row_number\",True).over(w)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+---------+--------------+\n id|row_number|time_diff|new_row_number|\n+---+----------+---------+--------------+\n  x|         1|     9999|             1|\n  x|         2|      120|             1|\n  x|         3|      102|             1|\n  x|         4|     3000|             4|\n  x|         5|      299|             5|\n  x|         6|      100|             5|\n+---+----------+---------+--------------+\n\n</div>"]}}],"execution_count":54},{"cell_type":"code","source":["list=[[ 5,     'a'],\n      [  2,     'b'],\n      [  3,     'c'],\n      [ 6,    'a'],\n      [ 7,     'b']]\n\ndfviol=spark.createDataFrame(list,['Number','Letter'])\n\n\nlist1=[[ 1,     'a',                1,                 5,                 6,                10],\n       [ 2,     'a',                7,                9,                 0,                  4],\n       [ 3,     'a',               11,                15,                10,                 14],\n       [ 4,     'b',                1,                 5,                 0,                  4],\n       [ 5,     'b',                7,                 9,                 6,                 10],\n       [ 6,     'c',                1,                 5,                 0,                  4],\n       [ 7,     'c',                7,                 9,                 6,                 10],\n       [ 8,     'c',               11,                15,                10,                 14]]\n\n\ndfcent=spark.createDataFrame(list1,['ID','Letter','Num_range_low_odd','Num_range_high_odd','Num_range_low_even','Num_range_high_even'])\n\ndfviol.show()\ndfcent.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+\nNumber|Letter|\n+------+------+\n     5|     a|\n     2|     b|\n     3|     c|\n     6|     a|\n     7|     b|\n+------+------+\n\n+---+------+-----------------+------------------+------------------+-------------------+\n ID|Letter|Num_range_low_odd|Num_range_high_odd|Num_range_low_even|Num_range_high_even|\n+---+------+-----------------+------------------+------------------+-------------------+\n  1|     a|                1|                 5|                 6|                 10|\n  2|     a|                7|                 9|                 0|                  4|\n  3|     a|               11|                15|                10|                 14|\n  4|     b|                1|                 5|                 0|                  4|\n  5|     b|                7|                 9|                 6|                 10|\n  6|     c|                1|                 5|                 0|                  4|\n  7|     c|                7|                 9|                 6|                 10|\n  8|     c|               11|                15|                10|                 14|\n+---+------+-----------------+------------------+------------------+-------------------+\n\n</div>"]}}],"execution_count":55},{"cell_type":"code","source":["joinCondition = F.when(dfviol.Number%2== 0, [(dfcent.Num_range_low_even <= dfviol.Number) & (dfcent.Num_range_high_even >= dfviol.Number)]).otherwise([(dfcent.Num_range_low_odd <= dfviol.Number) & (dfcent.Num_range_high_odd >= dfviol.Number)])\n\ndf_full = dfviol.join(dfcent,[dfviol.Letter == dfcent.Letter, joinCondition], how='inner')\ndf_full.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2734917699186196&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>joinCondition <span class=\"ansi-blue-fg\">=</span> F<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>dfviol<span class=\"ansi-blue-fg\">.</span>Number<span class=\"ansi-blue-fg\">%</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">.</span>Num_range_low_even <span class=\"ansi-blue-fg\">&lt;=</span> dfviol<span class=\"ansi-blue-fg\">.</span>Number<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&amp;</span> <span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">.</span>Num_range_high_even <span class=\"ansi-blue-fg\">&gt;=</span> dfviol<span class=\"ansi-blue-fg\">.</span>Number<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>otherwise<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">.</span>Num_range_low_odd <span class=\"ansi-blue-fg\">&lt;=</span> dfviol<span class=\"ansi-blue-fg\">.</span>Number<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&amp;</span> <span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">.</span>Num_range_high_odd <span class=\"ansi-blue-fg\">&gt;=</span> dfviol<span class=\"ansi-blue-fg\">.</span>Number<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> df_full <span class=\"ansi-blue-fg\">=</span> dfviol<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span>dfviol<span class=\"ansi-blue-fg\">.</span>Letter <span class=\"ansi-blue-fg\">==</span> dfcent<span class=\"ansi-blue-fg\">.</span>Letter<span class=\"ansi-blue-fg\">,</span> joinCondition<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;inner&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> df_full<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/functions.py</span> in <span class=\"ansi-cyan-fg\">when</span><span class=\"ansi-blue-fg\">(condition, value)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    745</span>         <span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;condition should be a Column&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    746</span>     v <span class=\"ansi-blue-fg\">=</span> value<span class=\"ansi-blue-fg\">.</span>_jc <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>value<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">else</span> value\n<span class=\"ansi-green-fg\">--&gt; 747</span><span class=\"ansi-red-fg\">     </span>jc <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>functions<span class=\"ansi-blue-fg\">.</span>when<span class=\"ansi-blue-fg\">(</span>condition<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">,</span> v<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    748</span>     <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    749</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1246</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1247</span>     <span class=\"ansi-green-fg\">def</span> __call__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1248</span><span class=\"ansi-red-fg\">         </span>args_command<span class=\"ansi-blue-fg\">,</span> temp_args <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_build_args<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1249</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1250</span>         command <span class=\"ansi-blue-fg\">=</span> proto<span class=\"ansi-blue-fg\">.</span>CALL_COMMAND_NAME <span class=\"ansi-blue-fg\">+</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_build_args</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1210</span>     <span class=\"ansi-green-fg\">def</span> _build_args<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1211</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>converters <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> len<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1212</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-blue-fg\">(</span>new_args<span class=\"ansi-blue-fg\">,</span> temp_args<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_get_args<span class=\"ansi-blue-fg\">(</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1213</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1214</span>             new_args <span class=\"ansi-blue-fg\">=</span> args\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_get_args</span><span class=\"ansi-blue-fg\">(self, args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1197</span>                 <span class=\"ansi-green-fg\">for</span> converter <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>                     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">.</span>can_convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1199</span><span class=\"ansi-red-fg\">                         </span>temp_arg <span class=\"ansi-blue-fg\">=</span> converter<span class=\"ansi-blue-fg\">.</span>convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1200</span>                         temp_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>                         new_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</span> in <span class=\"ansi-cyan-fg\">convert</span><span class=\"ansi-blue-fg\">(self, object, gateway_client)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    499</span>         java_list <span class=\"ansi-blue-fg\">=</span> ArrayList<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    500</span>         <span class=\"ansi-green-fg\">for</span> element <span class=\"ansi-green-fg\">in</span> object<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 501</span><span class=\"ansi-red-fg\">             </span>java_list<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">(</span>element<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    502</span>         <span class=\"ansi-green-fg\">return</span> java_list\n<span class=\"ansi-green-intense-fg ansi-bold\">    503</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1246</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1247</span>     <span class=\"ansi-green-fg\">def</span> __call__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1248</span><span class=\"ansi-red-fg\">         </span>args_command<span class=\"ansi-blue-fg\">,</span> temp_args <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_build_args<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1249</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1250</span>         command <span class=\"ansi-blue-fg\">=</span> proto<span class=\"ansi-blue-fg\">.</span>CALL_COMMAND_NAME <span class=\"ansi-blue-fg\">+</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_build_args</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1210</span>     <span class=\"ansi-green-fg\">def</span> _build_args<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1211</span>         <span class=\"ansi-green-fg\">if</span> self<span class=\"ansi-blue-fg\">.</span>converters <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span> <span class=\"ansi-green-fg\">and</span> len<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">&gt;</span> <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1212</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-blue-fg\">(</span>new_args<span class=\"ansi-blue-fg\">,</span> temp_args<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_get_args<span class=\"ansi-blue-fg\">(</span>args<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1213</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1214</span>             new_args <span class=\"ansi-blue-fg\">=</span> args\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">_get_args</span><span class=\"ansi-blue-fg\">(self, args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1197</span>                 <span class=\"ansi-green-fg\">for</span> converter <span class=\"ansi-green-fg\">in</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>converters<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1198</span>                     <span class=\"ansi-green-fg\">if</span> converter<span class=\"ansi-blue-fg\">.</span>can_convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">-&gt; 1199</span><span class=\"ansi-red-fg\">                         </span>temp_arg <span class=\"ansi-blue-fg\">=</span> converter<span class=\"ansi-blue-fg\">.</span>convert<span class=\"ansi-blue-fg\">(</span>arg<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1200</span>                         temp_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1201</span>                         new_args<span class=\"ansi-blue-fg\">.</span>append<span class=\"ansi-blue-fg\">(</span>temp_arg<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py</span> in <span class=\"ansi-cyan-fg\">convert</span><span class=\"ansi-blue-fg\">(self, object, gateway_client)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    498</span>         ArrayList <span class=\"ansi-blue-fg\">=</span> JavaClass<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;java.util.ArrayList&#34;</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    499</span>         java_list <span class=\"ansi-blue-fg\">=</span> ArrayList<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 500</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">for</span> element <span class=\"ansi-green-fg\">in</span> object<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    501</span>             java_list<span class=\"ansi-blue-fg\">.</span>add<span class=\"ansi-blue-fg\">(</span>element<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    502</span>         <span class=\"ansi-green-fg\">return</span> java_list\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">__iter__</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    342</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    343</span>     <span class=\"ansi-green-fg\">def</span> __iter__<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 344</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> TypeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Column is not iterable&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    345</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    346</span>     <span class=\"ansi-red-fg\"># string methods</span>\n\n<span class=\"ansi-red-fg\">TypeError</span>: Column is not iterable</div>"]}}],"execution_count":56},{"cell_type":"code","source":["#joinCondition = when(dfviol.Number%2== 0, [dfcent.Num_range_low_even <= dfviol.Number,dfcent.Num_range_high_even >= #dfviol.Number]).otherwise([dfcent.Num_range_low_odd <= dfviol.Number,dfcent.Num_range_high_odd >= dfviol.Number])\n\ndf1 = dfviol.filter(F.col(\"Number\")%2==0).join(dfcent,[dfviol.Letter == dfcent.Letter, dfcent.Num_range_low_even <= dfviol.Number,dfcent.Num_range_high_even >= dfviol.Number], how='inner')\ndf2 = dfviol.filter(F.col(\"Number\")%2!=0).join(dfcent,[dfviol.Letter == dfcent.Letter,dfcent.Num_range_low_odd <= dfviol.Number,dfcent.Num_range_high_odd >= dfviol.Number], how='inner')\n\ndf1.union(df2).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+---+------+-----------------+------------------+------------------+-------------------+\nNumber|Letter| ID|Letter|Num_range_low_odd|Num_range_high_odd|Num_range_low_even|Num_range_high_even|\n+------+------+---+------+-----------------+------------------+------------------+-------------------+\n     2|     b|  4|     b|                1|                 5|                 0|                  4|\n     6|     a|  1|     a|                1|                 5|                 6|                 10|\n     3|     c|  6|     c|                1|                 5|                 0|                  4|\n     7|     b|  5|     b|                7|                 9|                 6|                 10|\n     5|     a|  1|     a|                1|                 5|                 6|                 10|\n+------+------+---+------+-----------------+------------------+------------------+-------------------+\n\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["#joinCondition = when(dfviol.Number%2== 0, [dfcent.Num_range_low_even <= dfviol.Number,dfcent.Num_range_high_even >= #dfviol.Number]).otherwise([dfcent.Num_range_low_odd <= dfviol.Number,dfcent.Num_range_high_odd >= dfviol.Number])\n\ndf1 = dfviol.filter(F.col(\"Number\")%2==0).join(dfcent,[dfviol.Letter == dfcent.Letter, dfcent.Num_range_low_even <= dfviol.Number,dfcent.Num_range_high_even >= dfviol.Number], how='inner')\ndf2 = dfviol.filter(F.col(\"Number\")%2!=0).join(dfcent,[dfviol.Letter == dfcent.Letter,dfcent.Num_range_low_odd <= dfviol.Number,dfcent.Num_range_high_odd >= dfviol.Number], how='inner')\n\ndf1.union(df2).show()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["joinCondition = F.expr(\"\"\"IF(Number % 2==0, array((Num_range_low_even <= Number),(Num_range_high_even >= Number),(Letter)),array(((Num_range_low_odd <= Number) or (Num_range_high_odd >= Number) or (Letter1 == Letter))))\"\"\")\n\ndf_full = dfviol.withColumnRenamed(\"Letter\",\"Letter1\").join(dfcent,joinCondition, how='inner')\ndf_full.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1822.join.\n: org.apache.spark.sql.AnalysisException: cannot resolve &#39;array((`Num_range_low_even` &lt;= `Number`), (`Num_range_high_even` &gt;= `Number`), `Letter`)&#39; due to data type mismatch: input to function array should all be the same type, but it&#39;s [boolean, boolean, string]; line 1 pos 18;\n&#39;Join Inner, &#39;IF(((Number#775L % cast(2 as bigint)) = cast(0 as bigint)), array((Num_range_low_even#783L &lt;= Number#775L), (Num_range_high_even#784L &gt;= Number#775L), Letter#780), array((((Num_range_low_odd#781L &lt;= Number#775L) || (Num_range_high_odd#782L &gt;= Number#775L)) || (Letter1#829 = Letter#780))))\n:- Project [Number#775L, Letter#776 AS Letter1#829]\n:  +- LogicalRDD [Number#775L, Letter#776], false\n+- LogicalRDD [ID#779L, Letter#780, Num_range_low_odd#781L, Num_range_high_odd#782L, Num_range_low_even#783L, Num_range_high_even#784L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:322)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8$$anonfun$apply$13.apply(TreeNode.scala:381)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:300)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:147)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:89)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:117)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:83)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:81)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3534)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:1023)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2734917699186199&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> joinCondition <span class=\"ansi-blue-fg\">=</span> F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;IF(Number % 2==0, array((Num_range_low_even &lt;= Number),(Num_range_high_even &gt;= Number),(Letter)),array(((Num_range_low_odd &lt;= Number) or (Num_range_high_odd &gt;= Number) or (Letter1 == Letter))))&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df_full <span class=\"ansi-blue-fg\">=</span> dfviol<span class=\"ansi-blue-fg\">.</span>withColumnRenamed<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Letter&#34;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;Letter1&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>dfcent<span class=\"ansi-blue-fg\">,</span>joinCondition<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;inner&#39;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> df_full<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1077</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1078</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> basestring<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be basestring&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 1079</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1080</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1081</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;cannot resolve &#39;array((`Num_range_low_even` &lt;= `Number`), (`Num_range_high_even` &gt;= `Number`), `Letter`)&#39; due to data type mismatch: input to function array should all be the same type, but it&#39;s [boolean, boolean, string]; line 1 pos 18;\\n&#39;Join Inner, &#39;IF(((Number#775L % cast(2 as bigint)) = cast(0 as bigint)), array((Num_range_low_even#783L &lt;= Number#775L), (Num_range_high_even#784L &gt;= Number#775L), Letter#780), array((((Num_range_low_odd#781L &lt;= Number#775L) || (Num_range_high_odd#782L &gt;= Number#775L)) || (Letter1#829 = Letter#780))))\\n:- Project [Number#775L, Letter#776 AS Letter1#829]\\n:  +- LogicalRDD [Number#775L, Letter#776], false\\n+- LogicalRDD [ID#779L, Letter#780, Num_range_low_odd#781L, Num_range_high_odd#782L, Num_range_low_even#783L, Num_range_high_even#784L], false\\n&#34;</div>"]}}],"execution_count":59},{"cell_type":"code","source":["from pyspark.sql.functions import when\n\njoinCondition = when(dfviol.Number%2== 0, (dfcent.Num_range_low_even <= dfviol.Number)&(dfcent.Num_range_high_even >= dfviol.Number)).otherwise((dfcent.Num_range_low_odd <= dfviol.Number) & (dfcent.Num_range_high_odd >= dfviol.Number))\n\ndf_full = dfviol.join(dfcent,[dfviol.Letter == dfcent.Letter, joinCondition], how='inner')\ndf_full.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+---+------+-----------------+------------------+------------------+-------------------+\nNumber|Letter| ID|Letter|Num_range_low_odd|Num_range_high_odd|Num_range_low_even|Num_range_high_even|\n+------+------+---+------+-----------------+------------------+------------------+-------------------+\n     3|     c|  6|     c|                1|                 5|                 0|                  4|\n     2|     b|  4|     b|                1|                 5|                 0|                  4|\n     7|     b|  5|     b|                7|                 9|                 6|                 10|\n     5|     a|  1|     a|                1|                 5|                 6|                 10|\n     6|     a|  1|     a|                1|                 5|                 6|                 10|\n+------+------+---+------+-----------------+------------------+------------------+-------------------+\n\n</div>"]}}],"execution_count":60},{"cell_type":"code","source":["https://medium.com/@murtazahash"],"metadata":{},"outputs":[],"execution_count":61}],"metadata":{"name":"stackhelp54","notebookId":2734917699186193},"nbformat":4,"nbformat_minor":0}
