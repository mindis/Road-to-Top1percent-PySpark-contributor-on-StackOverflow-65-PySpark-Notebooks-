{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[['Bob', 5.62,'juice'],\n      ['Sue',  0.85, 'milk'],\n      ['Joe',  1.04, 'eggs']]\n\ndf=spark.createDataFrame(list,['name','amount','item'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----+\nname|amount| item|\n+----+------+-----+\n Bob|  5.62|juice|\n Sue|  0.85| milk|\n Joe|  1.04| eggs|\n+----+------+-----+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["df.filter(~df.item.isin(['milk','eggs'])).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------+-----+\nname|amount| item|\n+----+------+-----+\n Bob|  5.62|juice|\n+----+------+-----+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["df = df.filter(~df.item.isin(['milk','eggs']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;command-2321691597675911&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">1</span>\n<span class=\"ansi-red-fg\">    df = df.filter(~df.item.isin([&#39;milk&#39;,&#39;eggs&#39;])</span>\n                                                 ^\n<span class=\"ansi-red-fg\">SyntaxError</span><span class=\"ansi-red-fg\">:</span> unexpected EOF while parsing\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["list=[['A' ,      '2019-12-15',      '2020-04-30'],\n     ['B' ,      '2020-03-03'  ,    '2020-04-30']]\n\ndf=spark.createDataFrame(list,['ID','Start_Date','End_Date'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+----------+\n ID|Start_Date|  End_Date|\n+---+----------+----------+\n  A|2019-12-15|2020-04-30|\n  B|2020-03-03|2020-04-30|\n+---+----------+----------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf.withColumn(\"Id_period\", F.explode(F.expr(\"\"\"transform(sequence(to_date(start_date),to_date(end_date)\\\n                                                         ,interval 1 month),x-> date_format(x,'yyyyMM'))\"\"\"))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+----------+---------+\n ID|Start_Date|  End_Date|Id_period|\n+---+----------+----------+---------+\n  A|2019-12-15|2020-04-30|   201912|\n  A|2019-12-15|2020-04-30|   202001|\n  A|2019-12-15|2020-04-30|   202002|\n  A|2019-12-15|2020-04-30|   202003|\n  A|2019-12-15|2020-04-30|   202004|\n  B|2020-03-03|2020-04-30|   202003|\n  B|2020-03-03|2020-04-30|   202004|\n+---+----------+----------+---------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf.withColumn(\"monthsDiff\", F.months_between(\"End_Date\", \"Start_Date\"))\\\n    .withColumn(\"repeat\", F.expr(\"split(repeat(',', monthsDiff), ',')\"))\\\n     .select(\"*\", F.posexplode(\"repeat\").alias(\"date\", \"val\"))\\\n    .withColumn(\"Id_period\", F.expr(\"\"\"date_format(add_months(Start_Date, date),'yyyyMM')\"\"\"))\\\n    .drop(\"repeat\",\"val\",\"monthsDiff\",\"date\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+----------+---------+\n ID|Start_Date|  End_Date|Id_period|\n+---+----------+----------+---------+\n  A|2019-12-15|2020-04-30|   201912|\n  A|2019-12-15|2020-04-30|   202001|\n  A|2019-12-15|2020-04-30|   202002|\n  A|2019-12-15|2020-04-30|   202003|\n  A|2019-12-15|2020-04-30|   202004|\n  B|2020-03-03|2020-04-30|   202003|\n  B|2020-03-03|2020-04-30|   202004|\n+---+----------+----------+---------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["ID      Start_Date      End_Date       ID_period\nA       2019-12-15      2020-04-30     201912\nA       2019-12-15      2020-04-30     202001\nA       2019-12-15      2020-04-30     202002\nA       2019-12-15      2020-04-30     202003\nA       2019-12-15      2020-04-30     202004\nB       2020-03-03      2020-04-30     202003\nB       2020-03-03      2020-04-30     202004"],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"stackhelp61","notebookId":2321691597675905},"nbformat":4,"nbformat_minor":0}
