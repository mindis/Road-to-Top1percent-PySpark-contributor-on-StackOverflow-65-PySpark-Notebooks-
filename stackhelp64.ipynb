{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["list=[[['B', 'A', 'A', 'C', 'B', 'A']],\n       [['C', 'D', 'E']],\n       [['B']],\n       [['C', 'C', 'C']]]\n\ndf=spark.createDataFrame(list,['mylist'])\n\ndf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+\n            mylist|\n+------------------+\n[B, A, A, C, B, A]|\n         [C, D, E]|\n               [B]|\n         [C, C, C]|\n+------------------+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql import functions as F\ndf\\\n  .withColumn(\"histo\",\\\n          F.expr(\"\"\"map_from_arrays(array_distinct(mylist),transform(array_distinct(mylist),\\\n              x-> size(filter(mylist,y-> y=x))))\"\"\"))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------------+\nmylist            |histo                   |\n+------------------+------------------------+\n[B, A, A, C, B, A]|[B -&gt; 2, A -&gt; 3, C -&gt; 1]|\n[C, D, E]         |[C -&gt; 1, D -&gt; 1, E -&gt; 1]|\n[B]               |[B -&gt; 1]                |\n[C, C, C]         |[C -&gt; 3]                |\n+------------------+------------------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["list=[[9347774,'2019-01-01 00:00:00','2019-01-08 02:10:00', 7.090277777777778,    0.0],\n      [9347774,'2019-01-08 02:10:00','2019-02-04 05:28:00',           27.1375,    0.0],\n      [9347774,'2019-02-04 05:28:00','2019-03-05 19:29:00',29.584027777777777,    0.0],\n      [9347774,'2019-03-05 19:29:00','2019-04-22 18:45:00', 47.96944444444444,    0.0],\n      [9347774,'2019-04-22 18:45:00','2019-05-03 05:05:00',10.430555555555555,    0.0],\n      [9347774,'2019-05-03 05:05:00','2019-05-17 16:25:00',14.472222222222221,    0.0],\n      [9347774,'2019-05-17 16:25:00','2019-06-05 22:18:00', 19.24513888888889,    0.0],\n      [9347774,'2019-01-01 00:00:00','2020-01-01 00:00:00',             365.0,    365.0],\n      [9347774,'2019-06-05 22:18:00','2020-01-01 00:00:00',209.07083333333333,    0.0]]\n\ndf=spark.createDataFrame(list,['id','begin','end','days','overlap'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------------------+-------------------+------------------+-------+\n     id|              begin|                end|              days|overlap|\n+-------+-------------------+-------------------+------------------+-------+\n9347774|2019-01-01 00:00:00|2019-01-08 02:10:00| 7.090277777777778|    0.0|\n9347774|2019-01-08 02:10:00|2019-02-04 05:28:00|           27.1375|    0.0|\n9347774|2019-02-04 05:28:00|2019-03-05 19:29:00|29.584027777777777|    0.0|\n9347774|2019-03-05 19:29:00|2019-04-22 18:45:00| 47.96944444444444|    0.0|\n9347774|2019-04-22 18:45:00|2019-05-03 05:05:00|10.430555555555555|    0.0|\n9347774|2019-05-03 05:05:00|2019-05-17 16:25:00|14.472222222222221|    0.0|\n9347774|2019-05-17 16:25:00|2019-06-05 22:18:00| 19.24513888888889|    0.0|\n9347774|2019-01-01 00:00:00|2020-01-01 00:00:00|             365.0|  365.0|\n9347774|2019-06-05 22:18:00|2020-01-01 00:00:00|209.07083333333333|    0.0|\n+-------+-------------------+-------------------+------------------+-------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .when(F.col(\"maxrownum\")==1,F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n              .replace(1,0)\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"rowNum\",\"maxrownum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------------------+-------------------+------------------+------------------+\n     id|              begin|                end|              days|           overlap|\n+-------+-------------------+-------------------+------------------+------------------+\n9347774|2019-01-01 00:00:00|2019-01-08 02:10:00| 7.090277777777778|               7.0|\n9347774|2019-01-08 02:10:00|2019-02-04 05:28:00|           27.1375|           27.1375|\n9347774|2019-02-04 05:28:00|2019-03-05 19:29:00|29.584027777777777|29.584027777777777|\n9347774|2019-03-05 19:29:00|2019-04-22 18:45:00| 47.96944444444444| 47.96944444444444|\n9347774|2019-04-22 18:45:00|2019-05-03 05:05:00|10.430555555555555|10.430555555555555|\n9347774|2019-05-03 05:05:00|2019-05-17 16:25:00|14.472222222222221|14.472222222222221|\n9347774|2019-05-17 16:25:00|2019-06-05 22:18:00| 19.24513888888889| 19.24513888888889|\n9347774|2019-01-01 00:00:00|2020-01-01 00:00:00|             365.0|               0.0|\n9347774|2019-06-05 22:18:00|2020-01-01 00:00:00|209.07083333333333|209.07083333333333|\n+-------+-------------------+-------------------+------------------+------------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["list=[[7777777,'2019-01-05 01:00:00','2019-04-04 00:00:00',88.95833333333333],\n[7777777,'2019-04-04 00:00:00','2019-07-11 00:00:00',             98.0],\n[7777777,'2019-07-11 00:00:00','2019-09-17 00:00:00',             68.0],\n[7777777,'2019-09-17 00:00:00','2019-09-19 22:01:00',2.917361111111111],\n[7777777,'2019-09-19 22:01:00','2020-01-01 00:00:00',103.0826388888889],\n[7777777,'2019-09-19 22:01:00','2020-01-01 00:00:00',103.0826388888889]]\n\ndf=spark.createDataFrame(list,['id','begin','end','days'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------------------+-------------------+-----------------+\n     id|              begin|                end|             days|\n+-------+-------------------+-------------------+-----------------+\n7777777|2019-01-05 01:00:00|2019-04-04 00:00:00|88.95833333333333|\n7777777|2019-04-04 00:00:00|2019-07-11 00:00:00|             98.0|\n7777777|2019-07-11 00:00:00|2019-09-17 00:00:00|             68.0|\n7777777|2019-09-17 00:00:00|2019-09-19 22:01:00|2.917361111111111|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|\n+-------+-------------------+-------------------+-----------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .when(F.col(\"maxrownum\")==1,F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n              .replace(1,0)\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"rowNum\",\"maxrownum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------------------+-------------------+-----------------+-----------------+\n     id|              begin|                end|             days|          overlap|\n+-------+-------------------+-------------------+-----------------+-----------------+\n7777777|2019-01-05 01:00:00|2019-04-04 00:00:00|88.95833333333333|              0.0|\n7777777|2019-04-04 00:00:00|2019-07-11 00:00:00|             98.0|              0.0|\n7777777|2019-07-11 00:00:00|2019-09-17 00:00:00|             68.0|              0.0|\n7777777|2019-09-17 00:00:00|2019-09-19 22:01:00|2.917361111111111|              0.0|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|103.0826388888889|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|              0.0|\n+-------+-------------------+-------------------+-----------------+-----------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id order by begin),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"maxrownum\",\"rowNum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------------------+-------------------+-----------------+-----------------+\n     id|              begin|                end|             days|          overlap|\n+-------+-------------------+-------------------+-----------------+-----------------+\n7777777|2019-01-05 01:00:00|2019-04-04 00:00:00|88.95833333333333|              0.0|\n7777777|2019-04-04 00:00:00|2019-07-11 00:00:00|             98.0|              0.0|\n7777777|2019-07-11 00:00:00|2019-09-17 00:00:00|             68.0|              0.0|\n7777777|2019-09-17 00:00:00|2019-09-19 22:01:00|2.917361111111111|              0.0|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|103.0826388888889|\n7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|              0.0|\n+-------+-------------------+-------------------+-----------------+-----------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["+-------+-------------------+-------------------+-----------------+-------+\n|     id|              begin|                end|             days|overlap|\n+-------+-------------------+-------------------+-----------------+-------+\n|7777777|2019-01-05 01:00:00|2019-04-04 00:00:00|88.95833333333333|      0|\n|7777777|2019-04-04 00:00:00|2019-07-11 00:00:00|             98.0|      0|\n|7777777|2019-07-11 00:00:00|2019-09-17 00:00:00|             68.0|      0|\n|7777777|2019-09-17 00:00:00|2019-09-19 22:01:00|2.917361111111111|      0|\n|7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|103.082|\n|7777777|2019-09-19 22:01:00|2020-01-01 00:00:00|103.0826388888889|      0|\n+-------+-------------------+-------------------+-----------------+-------+"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["list=[[2      ,'2019-01-01 00:00:00','2019-12-25 00:00:00',358.0],   \n[2      ,'2019-12-25 00:00:00','2020-01-01 00:00:00',  7.0],   \n[3      ,'2019-01-01 00:00:00','2019-12-25 00:00:00',358.0],  \n[3      ,'2019-12-20 00:00:00','2020-01-01 00:00:00', 12.0],    \n[4      ,'2019-01-01 00:00:00','2019-12-25 00:00:00',358.0],  \n[4      ,'2019-01-01 00:00:00','2019-11-25 00:00:00',328.0],\n[4      ,'2019-12-20 00:00:00','2020-01-01 00:00:00', 12.0]]\n\ndf=spark.createDataFrame(list,['id','begin','end','days'])\n\ndf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+\n id|              begin|                end| days|\n+---+-------------------+-------------------+-----+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n+---+-------------------+-------------------+-----+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .when(F.col(\"maxrownum\")==1,F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n              .replace(1,0)\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"rowNum\",\"maxrownum\").show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .when(F.col(\"maxrownum\")==1,F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n              .replace(1,0)\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"rowNum\",\"maxrownum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+-------+\n id|              begin|                end| days|overlap|\n+---+-------------------+-------------------+-----+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|    0.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|  328.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5.0|\n+---+-------------------+-------------------+-----+-------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .when(F.col(\"maxrownum\")==1,F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n              .replace(1,0)\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"rowNum\",\"overlap\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+---------+------+-------+\n id|              begin|                end| days|maxrownum|rowNum|overlap|\n+---+-------------------+-------------------+-----+---------+------+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|        0|     0|    0.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|        0|     0|    0.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|        0|     0|    0.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|        0|     0|    5.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|        0|     0|  328.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|        0|     0|    0.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|        0|     0|    5.0|\n+---+-------------------+-------------------+-----+---------+------+-------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\nw2=Window().partitionBy(\"id\",\"begin\",\"end\").orderBy(\"begin\")\nw3=Window().partitionBy(\"id\",\"begin\",\"end\")\nw4=Window().partitionBy(\"id\",\"begin\",\"end\",\"maxrownum\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn('maxrownum', F.max(F.row_number().over(w2)).over(w3))\\\n  .withColumn('rowNum', F.row_number().over(w4))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id order by begin),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .when(F.size(F.array_intersect(\"seq\",\"seq1\"))!=0,F.size(F.array_intersect(\"seq\",\"seq1\"))-1)\n              .when((F.col(\"maxrownum\")!=1)&(F.col(\"rowNum\")<F.col(\"maxrownum\")),F.col(\"days\"))\\\n              .otherwise(F.lit(0)))\\\n         .orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\",\"maxrownum\",\"rowNum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+-------+\n id|              begin|                end| days|overlap|\n+---+-------------------+-------------------+-----+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|    0.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|  328.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5.0|\n+---+-------------------+-------------------+-----+-------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .otherwise(F.size(F.array_intersect(\"seq\",\"seq1\"))-1)).orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+-------+\n id|              begin|                end| days|overlap|\n+---+-------------------+-------------------+-----+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|      0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|    328|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n+---+-------------------+-------------------+-----+-------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["df.show() #sample dataframe\n#+---+-------------------+-------------------+-----+\n#| id|              begin|                end| days|\n#+---+-------------------+-------------------+-----+\n#|  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|\n#|  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n#|  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|\n#|  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n#+---+-------------------+-------------------+-----+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.size(F.array_intersect(\"seq\",\"seq1\"))-1).orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\").show()\n\n#+---+-------------------+-------------------+-----+-------+\n#| id|              begin|                end| days|overlap|\n#+---+-------------------+-------------------+-----+-------+\n#|  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|      0|\n#|  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n#|  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|    328|\n#|  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n#+---+-------------------+-------------------+-----+-------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+\n id|              begin|                end| days|\n+---+-------------------+-------------------+-----+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n+---+-------------------+-------------------+-----+\n\n+---+-------------------+-------------------+-----+-------+\n id|              begin|                end| days|overlap|\n+---+-------------------+-------------------+-----+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|      0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      5|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|    328|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    334|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n+---+-------------------+-------------------+-----+-------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df.show() #sample dataframe\n#+---+-------------------+-------------------+-----+\n#| id|              begin|                end| days|\n#+---+-------------------+-------------------+-----+\n#|  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|\n#|  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n#|  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n#|  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|\n#|  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n#+---+-------------------+-------------------+-----+\n\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"overlap\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .otherwise(F.size(F.array_intersect(\"seq\",\"seq1\"))-1)).orderBy(\"id\",\"end\").drop(\"seq\",\"seq1\").show()\n\n#+---+-------------------+-------------------+-----+-------+\n#| id|              begin|                end| days|overlap|\n#+---+-------------------+-------------------+-----+-------+\n#|  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|      0|\n#|  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n#|  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|    328|\n#|  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n#|  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n#+---+-------------------+-------------------+-----+-------+\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+\n id|              begin|                end| days|\n+---+-------------------+-------------------+-----+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|\n+---+-------------------+-------------------+-----+\n\n+---+-------------------+-------------------+-----+-------+\n id|              begin|                end| days|overlap|\n+---+-------------------+-------------------+-----+-------+\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|      0|\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|    328|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|      0|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|      5|\n+---+-------------------+-------------------+-----+-------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["w=Window().partitionBy(\"id\")\nw1=Window().partitionBy(\"id\").orderBy(\"begin\")\ndf.withColumn(\"seq\", F.expr(\"\"\"sequence(to_timestamp(begin), to_timestamp(end),interval 1 day)\"\"\"))\\\n  .withColumn(\"seq1\", F.collect_list(\"seq\").over(w))\\\n  .withColumn(\"seq1\", F.expr(\"\"\"flatten(filter(collect_list(seq) over\\\n                                (partition by id order by begin),x-> arrays_overlap(x,seq)==True and seq!=x))\"\"\"))\\\n  .withColumn(\"intersect\", F.when(F.row_number().over(w1)==1, F.lit(0))\\\n              .otherwise(F.size(F.array_intersect(\"seq\",\"seq1\"))-1)).orderBy(\"id\",\"end\").show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["collect_list(wpi)\\\n                                    over (order by financial_year)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["+-------+-------------------+-------------------+-----+-----+\n|  id   |              begin|                end| days| over|\n+-------+-------------------+-------------------+-----+-----+\n|2      |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0|\n|2      |2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|    0|\n+-------+-------------------+-------------------+-----+-----+\n+-------+-------------------+-------------------+-----+-----+\n|     id|              begin|                end| days| over|\n+-------+-------------------+-------------------+-----+-----+\n|3      |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0|\n|3      |2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5|\n+-------+-------------------+-------------------+-----+-----+\n+-------+-------------------+-------------------+-----+-----+\n|     id|              begin|                end| days| over|\n+-------+-------------------+-------------------+-----+-----+\n|4      |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|    0|\n|4      |2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|328.0|\n|4      |2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|    5|\n+-------+-------------------+-------------------+-----+-----+"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["w=Window().partitionBy(\"id\").orderBy(\"begin\")\n\ndf.withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"dates\", F.when(F.col(\"rowNum\")%2==0, F.col(\"begin\")).otherwise(F.col(\"end\")))\\\n  .withColumn(\"overlap\", F.when(F.col(\"rowNum\")==1, F.lit(0))\\\n                          .otherwise(F.datediff(F.col(\"dates\"),F.lag(\"dates\").over(w)))).drop(\"rowNum\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+-------------------+-------+\n id|              begin|                end| days|              dates|overlap|\n+---+-------------------+-------------------+-----+-------------------+-------+\n  3|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|2019-12-25 00:00:00|      0|\n  3|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|2019-12-20 00:00:00|     -5|\n  2|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|2019-12-25 00:00:00|      0|\n  2|2019-12-25 00:00:00|2020-01-01 00:00:00|  7.0|2019-12-25 00:00:00|      0|\n  4|2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|2019-12-25 00:00:00|      0|\n  4|2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|2019-01-01 00:00:00|   -358|\n  4|2019-12-20 00:00:00|2020-01-01 00:00:00| 12.0|2020-01-01 00:00:00|    365|\n+---+-------------------+-------------------+-----+-------------------+-------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["w=Window().partitionBy(\"id\").orderBy(\"begin\")\n\ndf.withColumn(\"rowNum\", F.row_number().over(w))\\\n  .withColumn(\"dates\", F.when(F.col(\"rowNum\")==1, F.array(F.lit(1))).otherwise(F.array(F.col(\"begin\"),F.lag(\"end\").over(w))))\\\n  .withColumn(\"over\",F.when(F.size(\"dates\")==1,F.lit(0)).otherwise(F.datediff(F.col(\"dates\")[1],F.col(\"dates\")[0])))\\\n  .show(truncate=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------------------+-------------------+-----+------+------------------------------------------+----+\nid |begin              |end                |days |rowNum|dates                                     |over|\n+---+-------------------+-------------------+-----+------+------------------------------------------+----+\n3  |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|1     |[1]                                       |0   |\n3  |2019-12-20 00:00:00|2020-01-01 00:00:00|12.0 |2     |[2019-12-20 00:00:00, 2019-12-25 00:00:00]|5   |\n2  |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|1     |[1]                                       |0   |\n2  |2019-12-25 00:00:00|2020-01-01 00:00:00|7.0  |2     |[2019-12-25 00:00:00, 2019-12-25 00:00:00]|0   |\n4  |2019-01-01 00:00:00|2019-12-25 00:00:00|358.0|1     |[1]                                       |0   |\n4  |2019-01-01 00:00:00|2019-11-25 00:00:00|328.0|2     |[2019-01-01 00:00:00, 2019-12-25 00:00:00]|358 |\n4  |2019-12-20 00:00:00|2020-01-01 00:00:00|12.0 |3     |[2019-12-20 00:00:00, 2019-11-25 00:00:00]|-25 |\n+---+-------------------+-------------------+-----+------+------------------------------------------+----+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["import pandas as pd\n# Master DF\ndf = pd.DataFrame({\"Name\": [\"Mike\", \"Bob\", \"Steve\", \"Jim\", \"Dan\"], \"Age\": [22, 44, 66, 22, 66], \"Job\": [\"Doc\", \"Cashier\", \"Fireman\", \"Doc\", \"Fireman\"]})\ndf=spark.createDataFrame(df)\n#Secondary DF\ndf1 = pd.DataFrame({\"Age\": [22, 66], \"Job\": [\"Doc\", \"Fireman\"]})\ndf1=spark.createDataFrame(df1)\nnew_df1 = pd.DataFrame({\"Age\": [22, 66], \"Job\": [\"Doc\", \"Fireman\"], \"Returned_value\": ['Mike-Jim', 'Steve-Dan']})\nnew_df1=spark.createDataFrame(new_df1)\ndf.show()\ndf1.show()\nnew_df1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+---+-------+\n Name|Age|    Job|\n+-----+---+-------+\n Mike| 22|    Doc|\n  Bob| 44|Cashier|\nSteve| 66|Fireman|\n  Jim| 22|    Doc|\n  Dan| 66|Fireman|\n+-----+---+-------+\n\n+---+-------+\nAge|    Job|\n+---+-------+\n 22|    Doc|\n 66|Fireman|\n+---+-------+\n\n+---+-------+--------------+\nAge|    Job|Returned_value|\n+---+-------+--------------+\n 22|    Doc|      Mike-Jim|\n 66|Fireman|     Steve-Dan|\n+---+-------+--------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf.join(df1,['Age','Job'])\\\n  .groupBy(\"Age\",\"Job\").agg(F.concat_ws('-',F.collect_list(\"Name\")).alias(\"Returned_value\")).show()\n\n#+---+-------+--------------+\n#|Age|    Job|Returned_value|\n#+---+-------+--------------+\n#| 22|    Doc|      Mike-Jim|\n#| 66|Fireman|     Steve-Dan|\n#+---+-------+--------------+"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+--------------+\nAge|    Job|Returned_value|\n+---+-------+--------------+\n 22|    Doc|      Mike-Jim|\n 66|Fireman|     Steve-Dan|\n+---+-------+--------------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["list=[[ 1  , '12:00:00'    , 'a'],\n      [ 2  , '13:00:00'    , 'b'],\n      [ 3  , '11:00:00'    , 'c']]\n\ndf1=spark.createDataFrame(list,['id','SMS Created','Content'])\n\nlist1=[[ 'Created' , '11:30:00' , 1  ,[1,2]],\n    [ 'Updated' , '11:42:00' , 1  ,[1,2,3]],\n[ 'Updated' , '11:50:00' , 1  ,[1,2,4]],\n[ 'Updated' , '12:50:00' , 1  ,[1,2]],\n[ 'Created' , '12:30:00' , 2 ,[1,2]],\n[ 'Updated' , '12:42:00' , 2  ,[1,2,3]],\n[ 'Updated' , '12:50:00' , 2  ,[1,2,4]],\n[ 'Updated' , '13:10:00' , 2  ,[1,2]],\n[ 'Created' , '10:30:00' , 3  ,[1,2]],\n[ 'Updated' , '10:42:00' , 3  ,[1,2,3]],\n[ 'Updated' , '10:50:00' , 3  ,[1,2,4]],\n[ 'Updated' , '12:10:00' , 2  ,[1,2]]]\n\ndf2=spark.createDataFrame(list1,['Event','Time','id','Members'])\n\ndf1.show()\n\ndf2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+-------+\n id|SMS Created|Content|\n+---+-----------+-------+\n  1|   12:00:00|      a|\n  2|   13:00:00|      b|\n  3|   11:00:00|      c|\n+---+-----------+-------+\n\n+-------+--------+---+---------+\n  Event|    Time| id|  Members|\n+-------+--------+---+---------+\nCreated|11:30:00|  1|   [1, 2]|\nUpdated|11:42:00|  1|[1, 2, 3]|\nUpdated|11:50:00|  1|[1, 2, 4]|\nUpdated|12:50:00|  1|   [1, 2]|\nCreated|12:30:00|  2|   [1, 2]|\nUpdated|12:42:00|  2|[1, 2, 3]|\nUpdated|12:50:00|  2|[1, 2, 4]|\nUpdated|13:10:00|  2|   [1, 2]|\nCreated|10:30:00|  3|   [1, 2]|\nUpdated|10:42:00|  3|[1, 2, 3]|\nUpdated|10:50:00|  3|[1, 2, 4]|\nUpdated|12:10:00|  2|   [1, 2]|\n+-------+--------+---+---------+\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["w=Window().partitionBy(\"id\")\ncond=F.expr(\"\"\"`SMS Created`>Time\"\"\")\ndf1.join(df2,['id'])\\\n   .withColumn(\"max\", F.max(\"Time\").over(w)).filter('max=Time').drop(\"max\").orderBy(\"id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o285.join.\n: org.apache.spark.sql.AnalysisException: USING column ``SMS Created`&gt;Time` cannot be resolved on the left side of the join. The left-side columns: [id, SMS Created, Content];\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$100$$anonfun$apply$71.apply(Analyzer.scala:2364)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$100$$anonfun$apply$71.apply(Analyzer.scala:2364)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$100.apply(Analyzer.scala:2363)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$100.apply(Analyzer.scala:2362)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2362)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2312)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2309)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2309)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2308)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executePhase$1.apply(QueryExecution.scala:229)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:228)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:75)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:82)\n\tat org.apache.spark.sql.Dataset$$anonfun$ofRows$1.apply(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:835)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3543)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:981)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1584831649314252&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> w<span class=\"ansi-blue-fg\">=</span>Window<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;id&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> cond<span class=\"ansi-blue-fg\">=</span>F<span class=\"ansi-blue-fg\">.</span>expr<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;&#34;&#34;`SMS Created`&gt;Time&#34;&#34;&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>df2<span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;id&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#34;`SMS Created`&gt;Time&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>    <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;max&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>max<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Time&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>over<span class=\"ansi-blue-fg\">(</span>w<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;max=Time&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;max&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>orderBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;id&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">join</span><span class=\"ansi-blue-fg\">(self, other, on, how)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1077</span>                 on <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jseq<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1078</span>             <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>how<span class=\"ansi-blue-fg\">,</span> basestring<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;how should be basestring&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 1079</span><span class=\"ansi-red-fg\">             </span>jdf <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">,</span> on<span class=\"ansi-blue-fg\">,</span> how<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1080</span>         <span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>jdf<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1081</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#39;USING column ``SMS Created`&gt;Time` cannot be resolved on the left side of the join. The left-side columns: [id, SMS Created, Content];&#39;</div>"]}}],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\")\ndf1.join(df2.withColumnRenamed(\"id\",\"id2\"), (F.col(\"id\")==F.col(\"id2\"))&(F.col(\"SMS Created\")>F.col(\"Time\"))).drop(\"id2\")\\\n   .withColumn(\"max\", F.max(\"Time\").over(w))\\\n   .filter('max=Time').drop(\"max\").orderBy(\"id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----------+-------+-------+--------+---------+\n id|SMS Created|Content|  Event|    Time|  Members|\n+---+-----------+-------+-------+--------+---------+\n  1|   12:00:00|      a|Updated|11:50:00|[1, 2, 4]|\n  2|   13:00:00|      b|Updated|12:50:00|[1, 2, 4]|\n  3|   11:00:00|      c|Updated|10:50:00|[1, 2, 4]|\n+---+-----------+-------+-------+--------+---------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nw=Window().partitionBy(\"id\")\ndf1.join(df2, ['id']&(df1['SMS Created']>df2.Time))\\\n   .withColumn(\"max\", F.max(\"Time\").over(w))\\\n   .filter('max=Time').drop(\"max\").orderBy(\"id\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1584831649314246&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> w<span class=\"ansi-blue-fg\">=</span>Window<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;id&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\"> </span>df1<span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>df2<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;id&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">&amp;</span><span class=\"ansi-blue-fg\">(</span>df1<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;SMS Created&#39;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">&gt;</span>df2<span class=\"ansi-blue-fg\">.</span>Time<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span>    <span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;max&#34;</span><span class=\"ansi-blue-fg\">,</span> F<span class=\"ansi-blue-fg\">.</span>max<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Time&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>over<span class=\"ansi-blue-fg\">(</span>w<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>    <span class=\"ansi-blue-fg\">.</span>filter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;max=Time&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;max&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>orderBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;id&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/column.py</span> in <span class=\"ansi-cyan-fg\">_</span><span class=\"ansi-blue-fg\">(self, other)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    113</span>     <span class=\"ansi-green-fg\">def</span> _<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> other<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    114</span>         jc <span class=\"ansi-blue-fg\">=</span> other<span class=\"ansi-blue-fg\">.</span>_jc <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>other<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">else</span> other\n<span class=\"ansi-green-fg\">--&gt; 115</span><span class=\"ansi-red-fg\">         </span>njc <span class=\"ansi-blue-fg\">=</span> getattr<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">,</span> name<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">(</span>jc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">return</span> Column<span class=\"ansi-blue-fg\">(</span>njc<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    117</span>     _<span class=\"ansi-blue-fg\">.</span>__doc__ <span class=\"ansi-blue-fg\">=</span> doc\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    331</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 332</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name, value))\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    333</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    334</span>             raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JError</span>: An error occurred while calling o2367.and. Trace:\npy4j.Py4JException: Method and([class java.util.ArrayList]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:341)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:349)\n\tat py4j.Gateway.invoke(Gateway.java:286)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql import *\n\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\n\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n\n\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee1, employee2])\n\n\ndepartmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = spark.createDataFrame(departmentsWithEmployeesSeq1)\n\ndf1.show(truncate=False)\ndf1.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+-----------------------------------------------------------------------------------------------------+\ndepartment                      |employees                                                                                            |\n+--------------------------------+-----------------------------------------------------------------------------------------------------+\n[123456, Computer Science]      |[[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000]]|\n[789012, Mechanical Engineering]|[[michael, armbrust, no-reply@berkeley.edu, 100000], [xiangrui, meng, no-reply@stanford.edu, 120000]]|\n+--------------------------------+-----------------------------------------------------------------------------------------------------+\n\nroot\n-- department: struct (nullable = true)\n    |-- id: string (nullable = true)\n    |-- name: string (nullable = true)\n-- employees: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- firstName: string (nullable = true)\n    |    |-- lastName: string (nullable = true)\n    |    |-- email: string (nullable = true)\n    |    |-- salary: long (nullable = true)\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["df1.schema.fields[1]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: StructField(employees,ArrayType(StructType(List(StructField(firstName,StringType,true),StructField(lastName,StringType,true),StructField(email,StringType,true),StructField(salary,LongType,true))),true),true)</div>"]}}],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\ndf1.selectExpr(\"department\",\"\"\"inline_outer(employees)\"\"\")\\\n   .withColumn(\"FullName\", F.concat_ws(\" \",\"firstName\",\"lastName\"))\\\n   .select(\"department\", F.array(F.struct(*[F.col(x).alias(x) for x in\\\n                                     ['firstName','lastName','email','salary','FullName']]))\\\n           .alias(\"employees\")).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- department: struct (nullable = true)\n    |-- id: string (nullable = true)\n    |-- name: string (nullable = true)\n-- employees: array (nullable = false)\n    |-- element: struct (containsNull = false)\n    |    |-- firstName: string (nullable = true)\n    |    |-- lastName: string (nullable = true)\n    |    |-- email: string (nullable = true)\n    |    |-- salary: long (nullable = true)\n    |    |-- FullName: string (nullable = false)\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["df1.withColumn(\"ar\", F.expr(\"\"\"transform(employees, x-> struct(concat_ws(' ',x.firstName,x.lastName) as\\\n                               FullName))\"\"\"))\\\n   .withColumn(\"employees\", F.arrays_zip(\"employees\",\"ar\"))\\\n   .withColumn(\"employees\", F.expr(\"\"\"transform(employees,x->\\\n    struct(x.employees.firstName))\"\"\"))\\\n   .drop(\"ar\").printSchema()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["root\n |-- department: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |-- employees: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- firstName: string (nullable = true)\n |    |    |-- lastName: string (nullable = true)\n |    |    |-- email: string (nullable = true)\n |    |    |-- salary: long (nullable = true)\n |    |    |-- FullName: string (containsNull = true)l"],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"stackhelp64","notebookId":3808834502731010},"nbformat":4,"nbformat_minor":0}
